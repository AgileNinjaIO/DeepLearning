{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f14a0f0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return x / x.max(axis=0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    label_binarizer = preprocessing.LabelBinarizer()\n",
    "    encoded_labels = range(10)\n",
    "    label_binarizer.fit(encoded_labels)\n",
    "    return label_binarizer.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], image_shape[2]], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor.shape.as_list()[3], conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    # Apply convolution\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    \n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    # Apply max pooling\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    shape = x_tensor.shape.as_list()\n",
    "    return tf.reshape(x_tensor, [-1, np.prod(shape[1:])])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # Weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.shape.as_list()[1], num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    # Calculate tensor\n",
    "    tensor = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    \n",
    "    # Apply activation function\n",
    "    tensor = tf.nn.relu(tensor)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # Weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.shape.as_list()[1], num_outputs]))\n",
    "    bias = tf.Variable(tf.random_uniform([num_outputs], -0.1, 0.1))\n",
    "    \n",
    "    # Calculate tensor\n",
    "    tensor = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Apply 1, 2, or 3 Convolution and Max Pool layers   \n",
    "    \n",
    "    conv_ksize       = (3,3)\n",
    "    conv_strides     = (1,1)\n",
    "    pool_ksize       = (2,2)\n",
    "    pool_strides     = (2,2)\n",
    "    \n",
    "    conv_num_outputs_1 = 24\n",
    "    conv_num_outputs_2 = 48\n",
    "    \n",
    "    conv_layer = conv2d_maxpool(x, conv_num_outputs_1, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layer = conv2d_maxpool(x, conv_num_outputs_2, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # Apply a Flatten Layer\n",
    "    conv_layer = flatten(conv_layer)\n",
    "    \n",
    "    # Apply 1, 2, or 3 Fully Connected Layers\n",
    "    conv_layer = fully_conn(conv_layer, 512)\n",
    "    \n",
    "    # Apply dropout\n",
    "    conv_layer = tf.nn.dropout(conv_layer, keep_prob)\n",
    "    \n",
    "    # Apply Output Layer\n",
    "    conv_layer = output(conv_layer, 10)\n",
    "    \n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1})\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1})\n",
    "    print(\"Loss: {:.3f} Accuracy: {:.3f}\".format(loss, validation_accuracy), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tune Parameters\n",
    "epochs = 500\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 23.199 Accuracy: 0.213\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 9.607 Accuracy: 0.204\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 5.149 Accuracy: 0.187\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 3.039 Accuracy: 0.166\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.530 Accuracy: 0.179\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.382 Accuracy: 0.191\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 2.303 Accuracy: 0.204\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 2.255 Accuracy: 0.210\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 2.222 Accuracy: 0.209\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 2.196 Accuracy: 0.218\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 2.176 Accuracy: 0.216\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 2.160 Accuracy: 0.220\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 2.150 Accuracy: 0.229\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 2.144 Accuracy: 0.222\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 2.133 Accuracy: 0.229\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 2.123 Accuracy: 0.236\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 2.121 Accuracy: 0.233\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 2.114 Accuracy: 0.235\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 2.109 Accuracy: 0.235\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 2.105 Accuracy: 0.241\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 2.105 Accuracy: 0.241\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 2.093 Accuracy: 0.243\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 2.095 Accuracy: 0.242\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 2.089 Accuracy: 0.248\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 2.088 Accuracy: 0.241\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 2.083 Accuracy: 0.246\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 2.080 Accuracy: 0.247\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 2.078 Accuracy: 0.245\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 2.073 Accuracy: 0.253\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 2.071 Accuracy: 0.252\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 2.073 Accuracy: 0.255\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 2.071 Accuracy: 0.256\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 2.071 Accuracy: 0.252\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 2.077 Accuracy: 0.256\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 2.067 Accuracy: 0.262\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 2.060 Accuracy: 0.260\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 2.068 Accuracy: 0.259\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 2.062 Accuracy: 0.261\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 2.069 Accuracy: 0.262\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 2.061 Accuracy: 0.265\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 2.049 Accuracy: 0.265\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 2.051 Accuracy: 0.269\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 2.045 Accuracy: 0.271\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 2.053 Accuracy: 0.271\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 2.056 Accuracy: 0.274\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 2.041 Accuracy: 0.278\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 2.048 Accuracy: 0.274\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 2.043 Accuracy: 0.276\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 2.035 Accuracy: 0.279\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 2.035 Accuracy: 0.278\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 2.027 Accuracy: 0.283\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 2.038 Accuracy: 0.282\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 2.034 Accuracy: 0.282\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 2.042 Accuracy: 0.287\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 2.032 Accuracy: 0.284\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 2.031 Accuracy: 0.285\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 2.028 Accuracy: 0.287\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 2.025 Accuracy: 0.290\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 2.026 Accuracy: 0.292\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 2.028 Accuracy: 0.296\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 2.028 Accuracy: 0.298\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 2.024 Accuracy: 0.300\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 2.031 Accuracy: 0.298\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 2.030 Accuracy: 0.297\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 2.029 Accuracy: 0.303\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 2.031 Accuracy: 0.304\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 2.028 Accuracy: 0.300\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 2.027 Accuracy: 0.302\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 2.025 Accuracy: 0.305\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 2.026 Accuracy: 0.303\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 2.034 Accuracy: 0.308\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 2.054 Accuracy: 0.309\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 2.040 Accuracy: 0.309\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 2.044 Accuracy: 0.310\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 2.033 Accuracy: 0.310\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 2.052 Accuracy: 0.314\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 2.062 Accuracy: 0.310\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 2.061 Accuracy: 0.313\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 2.077 Accuracy: 0.320\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 2.060 Accuracy: 0.315\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 2.039 Accuracy: 0.316\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 2.061 Accuracy: 0.322\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 2.084 Accuracy: 0.318\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 2.080 Accuracy: 0.320\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 2.097 Accuracy: 0.322\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 2.089 Accuracy: 0.325\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 2.074 Accuracy: 0.325\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 2.093 Accuracy: 0.320\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 2.102 Accuracy: 0.331\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 2.128 Accuracy: 0.327\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 2.113 Accuracy: 0.327\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 2.109 Accuracy: 0.327\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 2.141 Accuracy: 0.329\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 2.162 Accuracy: 0.332\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 2.159 Accuracy: 0.333\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 2.208 Accuracy: 0.333\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 2.167 Accuracy: 0.326\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 2.207 Accuracy: 0.333\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 2.233 Accuracy: 0.332\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 2.198 Accuracy: 0.331\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 2.200 Accuracy: 0.329\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 2.196 Accuracy: 0.333\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 2.245 Accuracy: 0.332\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 2.226 Accuracy: 0.334\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 2.238 Accuracy: 0.333\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 2.204 Accuracy: 0.335\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 2.193 Accuracy: 0.335\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 2.168 Accuracy: 0.333\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 2.158 Accuracy: 0.330\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 2.128 Accuracy: 0.331\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 2.114 Accuracy: 0.330\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 2.116 Accuracy: 0.330\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 2.141 Accuracy: 0.331\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 2.131 Accuracy: 0.330\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 2.131 Accuracy: 0.331\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 2.134 Accuracy: 0.328\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 2.135 Accuracy: 0.329\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 2.164 Accuracy: 0.327\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 2.161 Accuracy: 0.329\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 2.166 Accuracy: 0.327\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 2.172 Accuracy: 0.324\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 2.172 Accuracy: 0.328\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 2.180 Accuracy: 0.327\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 2.225 Accuracy: 0.322\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 2.233 Accuracy: 0.323\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 2.228 Accuracy: 0.328\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 2.209 Accuracy: 0.328\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 2.219 Accuracy: 0.333\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 2.222 Accuracy: 0.335\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 2.243 Accuracy: 0.333\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 2.299 Accuracy: 0.336\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 2.290 Accuracy: 0.333\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 2.296 Accuracy: 0.340\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 2.268 Accuracy: 0.337\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 2.302 Accuracy: 0.340\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 2.303 Accuracy: 0.339\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 2.343 Accuracy: 0.341\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 2.308 Accuracy: 0.340\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 2.343 Accuracy: 0.339\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 2.368 Accuracy: 0.341\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 2.362 Accuracy: 0.343\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 2.348 Accuracy: 0.342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 2.362 Accuracy: 0.342\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 2.357 Accuracy: 0.348\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 2.348 Accuracy: 0.344\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 2.353 Accuracy: 0.344\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 2.385 Accuracy: 0.346\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 2.377 Accuracy: 0.345\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 2.385 Accuracy: 0.348\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 2.385 Accuracy: 0.348\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 2.382 Accuracy: 0.349\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 2.366 Accuracy: 0.346\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 2.383 Accuracy: 0.350\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 2.441 Accuracy: 0.352\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 2.421 Accuracy: 0.351\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 2.430 Accuracy: 0.351\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 2.427 Accuracy: 0.350\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 2.415 Accuracy: 0.352\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 2.444 Accuracy: 0.354\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 2.475 Accuracy: 0.355\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 2.517 Accuracy: 0.354\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 2.533 Accuracy: 0.355\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 2.513 Accuracy: 0.353\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 2.548 Accuracy: 0.355\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 2.519 Accuracy: 0.355\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 2.498 Accuracy: 0.350\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 2.517 Accuracy: 0.355\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 2.555 Accuracy: 0.359\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 2.565 Accuracy: 0.361\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 2.573 Accuracy: 0.356\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 2.573 Accuracy: 0.361\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 2.577 Accuracy: 0.360\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 2.546 Accuracy: 0.362\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 2.624 Accuracy: 0.360\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 2.646 Accuracy: 0.361\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 2.611 Accuracy: 0.361\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 2.653 Accuracy: 0.359\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 2.704 Accuracy: 0.361\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 2.679 Accuracy: 0.362\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 2.721 Accuracy: 0.361\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 2.713 Accuracy: 0.360\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 2.726 Accuracy: 0.363\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 2.696 Accuracy: 0.357\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 2.747 Accuracy: 0.367\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 2.776 Accuracy: 0.366\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 2.767 Accuracy: 0.362\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 2.802 Accuracy: 0.360\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 2.831 Accuracy: 0.366\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 2.844 Accuracy: 0.365\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 2.831 Accuracy: 0.368\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 2.884 Accuracy: 0.367\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 2.884 Accuracy: 0.365\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 2.854 Accuracy: 0.367\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 2.926 Accuracy: 0.366\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 2.927 Accuracy: 0.367\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 2.921 Accuracy: 0.362\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 2.898 Accuracy: 0.366\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 2.943 Accuracy: 0.366\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 3.046 Accuracy: 0.364\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 3.041 Accuracy: 0.366\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 39.071 Accuracy: 0.223\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 14.669 Accuracy: 0.243\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 5.711 Accuracy: 0.222\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 2.635 Accuracy: 0.141\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 2.419 Accuracy: 0.118\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.363 Accuracy: 0.112\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 2.344 Accuracy: 0.106\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 2.334 Accuracy: 0.103\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 2.328 Accuracy: 0.102\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 2.324 Accuracy: 0.101\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.321 Accuracy: 0.102\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 2.318 Accuracy: 0.101\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 2.318 Accuracy: 0.101\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 2.317 Accuracy: 0.102\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 2.316 Accuracy: 0.099\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.315 Accuracy: 0.100\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 2.315 Accuracy: 0.099\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 2.315 Accuracy: 0.104\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 2.313 Accuracy: 0.097\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 2.312 Accuracy: 0.096\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.312 Accuracy: 0.097\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 2.312 Accuracy: 0.099\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 2.311 Accuracy: 0.097\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 2.313 Accuracy: 0.095\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 2.312 Accuracy: 0.096\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.312 Accuracy: 0.096\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 2.312 Accuracy: 0.095\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 2.310 Accuracy: 0.095\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 2.310 Accuracy: 0.095\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 2.309 Accuracy: 0.097\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 2.310 Accuracy: 0.098\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 2.309 Accuracy: 0.099\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 2.309 Accuracy: 0.098\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 2.306 Accuracy: 0.100\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 2.306 Accuracy: 0.101\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 2.305 Accuracy: 0.101\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 2.310 Accuracy: 0.098\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 2.311 Accuracy: 0.099\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 2.305 Accuracy: 0.101\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 2.307 Accuracy: 0.098\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 2.304 Accuracy: 0.101\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 2.308 Accuracy: 0.098\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 2.307 Accuracy: 0.099\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 2.302 Accuracy: 0.104\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 2.305 Accuracy: 0.100\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 2.301 Accuracy: 0.103\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 2.299 Accuracy: 0.103\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 2.298 Accuracy: 0.103\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 2.295 Accuracy: 0.108\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 2.301 Accuracy: 0.103\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 2.296 Accuracy: 0.107\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 2.305 Accuracy: 0.100\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 2.296 Accuracy: 0.106\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 2.292 Accuracy: 0.108\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 2.299 Accuracy: 0.103\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 2.290 Accuracy: 0.109\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 2.300 Accuracy: 0.104\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 2.302 Accuracy: 0.110\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 2.292 Accuracy: 0.107\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 2.299 Accuracy: 0.105\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 2.291 Accuracy: 0.108\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 2.286 Accuracy: 0.114\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 2.272 Accuracy: 0.120\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 2.273 Accuracy: 0.119\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 2.279 Accuracy: 0.113\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 2.274 Accuracy: 0.117\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 2.280 Accuracy: 0.119\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 2.276 Accuracy: 0.119\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 2.285 Accuracy: 0.111\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 2.278 Accuracy: 0.114\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 2.268 Accuracy: 0.124\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 2.291 Accuracy: 0.111\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 2.256 Accuracy: 0.132\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 2.269 Accuracy: 0.121\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 2.259 Accuracy: 0.129\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 2.261 Accuracy: 0.135\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 2.254 Accuracy: 0.135\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 2.231 Accuracy: 0.161\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 2.232 Accuracy: 0.151\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 2.227 Accuracy: 0.156\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 2.230 Accuracy: 0.163\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 2.256 Accuracy: 0.131\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 2.183 Accuracy: 0.190\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 2.191 Accuracy: 0.191\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 2.216 Accuracy: 0.161\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 2.165 Accuracy: 0.205\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 2.130 Accuracy: 0.213\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 2.142 Accuracy: 0.202\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 2.124 Accuracy: 0.204\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 2.098 Accuracy: 0.222\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 2.116 Accuracy: 0.208\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 2.110 Accuracy: 0.219\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 2.101 Accuracy: 0.223\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 2.085 Accuracy: 0.238\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 2.056 Accuracy: 0.251\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 2.031 Accuracy: 0.245\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 2.058 Accuracy: 0.238\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 1.984 Accuracy: 0.269\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 2.022 Accuracy: 0.257\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 1.984 Accuracy: 0.275\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.968 Accuracy: 0.274\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 2.000 Accuracy: 0.265\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 1.940 Accuracy: 0.277\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 1.949 Accuracy: 0.282\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 1.950 Accuracy: 0.276\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.923 Accuracy: 0.286\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.974 Accuracy: 0.279\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 1.919 Accuracy: 0.288\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 1.910 Accuracy: 0.280\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 1.911 Accuracy: 0.289\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.899 Accuracy: 0.297\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 1.926 Accuracy: 0.278\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 1.884 Accuracy: 0.299\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 1.866 Accuracy: 0.294\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 1.875 Accuracy: 0.293\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.881 Accuracy: 0.293\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 1.891 Accuracy: 0.278\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 1.857 Accuracy: 0.304\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 1.879 Accuracy: 0.299\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 1.849 Accuracy: 0.300\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.869 Accuracy: 0.293\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 1.861 Accuracy: 0.281\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 1.838 Accuracy: 0.315\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 1.830 Accuracy: 0.315\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 1.868 Accuracy: 0.287\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.820 Accuracy: 0.313\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 1.844 Accuracy: 0.311\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 1.819 Accuracy: 0.306\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 1.830 Accuracy: 0.312\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 1.816 Accuracy: 0.312\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.826 Accuracy: 0.313\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 1.828 Accuracy: 0.309\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 1.810 Accuracy: 0.307\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 1.806 Accuracy: 0.332\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 1.793 Accuracy: 0.316\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.783 Accuracy: 0.324\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 1.872 Accuracy: 0.292\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 1.814 Accuracy: 0.305\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 1.806 Accuracy: 0.322\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 1.814 Accuracy: 0.317\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.797 Accuracy: 0.318\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 1.791 Accuracy: 0.307\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 1.777 Accuracy: 0.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 1.765 Accuracy: 0.338\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 1.771 Accuracy: 0.326\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.767 Accuracy: 0.338\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 1.775 Accuracy: 0.318\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 1.757 Accuracy: 0.344\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 1.743 Accuracy: 0.345\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 1.753 Accuracy: 0.339\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.770 Accuracy: 0.338\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 1.773 Accuracy: 0.313\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 1.791 Accuracy: 0.312\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 1.737 Accuracy: 0.346\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 1.727 Accuracy: 0.358\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.757 Accuracy: 0.345\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 1.737 Accuracy: 0.336\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 1.723 Accuracy: 0.352\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 1.708 Accuracy: 0.366\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 1.726 Accuracy: 0.352\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.728 Accuracy: 0.360\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 1.748 Accuracy: 0.345\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 1.710 Accuracy: 0.365\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 1.701 Accuracy: 0.380\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 1.738 Accuracy: 0.352\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.702 Accuracy: 0.368\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 1.706 Accuracy: 0.357\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 1.699 Accuracy: 0.367\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 1.696 Accuracy: 0.375\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 1.686 Accuracy: 0.371\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.686 Accuracy: 0.377\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 1.681 Accuracy: 0.373\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 1.676 Accuracy: 0.376\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 1.654 Accuracy: 0.387\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 1.676 Accuracy: 0.372\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.662 Accuracy: 0.384\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 1.687 Accuracy: 0.370\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 1.751 Accuracy: 0.351\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 1.640 Accuracy: 0.388\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 1.668 Accuracy: 0.388\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.648 Accuracy: 0.388\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 1.647 Accuracy: 0.388\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 1.678 Accuracy: 0.387\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 1.633 Accuracy: 0.400\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 1.639 Accuracy: 0.391\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.646 Accuracy: 0.388\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 1.641 Accuracy: 0.387\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 1.692 Accuracy: 0.375\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 1.676 Accuracy: 0.392\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 1.633 Accuracy: 0.392\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.639 Accuracy: 0.383\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 1.633 Accuracy: 0.402\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 1.669 Accuracy: 0.389\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 1.625 Accuracy: 0.395\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 1.646 Accuracy: 0.393\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.621 Accuracy: 0.393\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 1.648 Accuracy: 0.386\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 1.627 Accuracy: 0.399\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 1.622 Accuracy: 0.397\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 1.615 Accuracy: 0.402\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.599 Accuracy: 0.404\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 1.599 Accuracy: 0.408\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 1.613 Accuracy: 0.405\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 1.603 Accuracy: 0.409\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 1.615 Accuracy: 0.402\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.586 Accuracy: 0.413\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 1.625 Accuracy: 0.405\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 1.583 Accuracy: 0.420\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 1.599 Accuracy: 0.403\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 1.610 Accuracy: 0.412\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.590 Accuracy: 0.406\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 1.590 Accuracy: 0.418\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 1.627 Accuracy: 0.406\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 1.611 Accuracy: 0.407\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 1.576 Accuracy: 0.425\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.584 Accuracy: 0.419\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 1.575 Accuracy: 0.424\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 1.588 Accuracy: 0.422\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 1.597 Accuracy: 0.418\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 1.559 Accuracy: 0.420\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.597 Accuracy: 0.424\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 1.557 Accuracy: 0.428\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 1.568 Accuracy: 0.424\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 1.580 Accuracy: 0.426\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 1.584 Accuracy: 0.422\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.565 Accuracy: 0.420\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 1.593 Accuracy: 0.420\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 1.562 Accuracy: 0.429\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 1.581 Accuracy: 0.431\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 1.546 Accuracy: 0.433\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.563 Accuracy: 0.434\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 1.569 Accuracy: 0.427\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 1.544 Accuracy: 0.434\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 1.571 Accuracy: 0.423\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 1.539 Accuracy: 0.430\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.542 Accuracy: 0.434\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 1.536 Accuracy: 0.443\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 1.566 Accuracy: 0.437\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 1.548 Accuracy: 0.444\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 1.534 Accuracy: 0.434\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.539 Accuracy: 0.435\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 1.534 Accuracy: 0.442\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 1.551 Accuracy: 0.438\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 1.568 Accuracy: 0.425\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 1.555 Accuracy: 0.435\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.576 Accuracy: 0.432\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 1.531 Accuracy: 0.449\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 1.534 Accuracy: 0.443\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 1.532 Accuracy: 0.449\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 1.529 Accuracy: 0.444\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.515 Accuracy: 0.455\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 1.518 Accuracy: 0.459\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 1.506 Accuracy: 0.461\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 1.518 Accuracy: 0.449\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 1.529 Accuracy: 0.445\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.527 Accuracy: 0.456\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 1.535 Accuracy: 0.446\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 1.503 Accuracy: 0.455\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 1.513 Accuracy: 0.453\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 1.499 Accuracy: 0.461\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.504 Accuracy: 0.452\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 1.521 Accuracy: 0.460\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 1.527 Accuracy: 0.449\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 1.486 Accuracy: 0.465\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 1.512 Accuracy: 0.450\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.498 Accuracy: 0.464\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 1.489 Accuracy: 0.459\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 1.501 Accuracy: 0.467\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 1.484 Accuracy: 0.465\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 1.492 Accuracy: 0.464\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.481 Accuracy: 0.468\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 1.523 Accuracy: 0.451\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 1.484 Accuracy: 0.470\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 1.475 Accuracy: 0.473\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 1.514 Accuracy: 0.448\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.495 Accuracy: 0.472\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 1.496 Accuracy: 0.463\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 1.466 Accuracy: 0.481\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 1.475 Accuracy: 0.468\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 1.475 Accuracy: 0.469\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.459 Accuracy: 0.481\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 1.463 Accuracy: 0.484\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 1.478 Accuracy: 0.466\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 1.472 Accuracy: 0.470\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 1.465 Accuracy: 0.472\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.463 Accuracy: 0.481\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 1.457 Accuracy: 0.491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 1.450 Accuracy: 0.481\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 1.455 Accuracy: 0.480\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 1.461 Accuracy: 0.469\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.466 Accuracy: 0.491\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 1.444 Accuracy: 0.492\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 1.449 Accuracy: 0.493\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 1.445 Accuracy: 0.491\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 1.454 Accuracy: 0.488\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.432 Accuracy: 0.494\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 1.459 Accuracy: 0.485\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 1.423 Accuracy: 0.496\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 1.433 Accuracy: 0.497\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 1.436 Accuracy: 0.492\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.437 Accuracy: 0.504\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 1.424 Accuracy: 0.501\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 1.408 Accuracy: 0.505\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 1.443 Accuracy: 0.490\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 1.410 Accuracy: 0.504\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.438 Accuracy: 0.493\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 1.430 Accuracy: 0.497\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 1.416 Accuracy: 0.508\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 1.407 Accuracy: 0.504\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 1.394 Accuracy: 0.502\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 1.390 Accuracy: 0.516\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 1.442 Accuracy: 0.493\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 1.417 Accuracy: 0.506\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 1.407 Accuracy: 0.507\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 1.415 Accuracy: 0.506\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 1.395 Accuracy: 0.512\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 1.419 Accuracy: 0.503\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 1.408 Accuracy: 0.516\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 1.403 Accuracy: 0.509\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 1.398 Accuracy: 0.513\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 1.395 Accuracy: 0.523\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 1.386 Accuracy: 0.511\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 1.383 Accuracy: 0.523\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 1.387 Accuracy: 0.515\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 1.374 Accuracy: 0.522\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 1.421 Accuracy: 0.513\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 1.382 Accuracy: 0.519\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 1.412 Accuracy: 0.511\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 1.377 Accuracy: 0.519\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 1.375 Accuracy: 0.517\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 1.412 Accuracy: 0.512\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 1.395 Accuracy: 0.519\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 1.398 Accuracy: 0.523\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 1.401 Accuracy: 0.504\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 1.405 Accuracy: 0.509\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.370 Accuracy: 0.519\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 1.366 Accuracy: 0.520\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 1.369 Accuracy: 0.531\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 1.354 Accuracy: 0.527\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 1.364 Accuracy: 0.522\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 1.376 Accuracy: 0.520\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 1.376 Accuracy: 0.519\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 1.353 Accuracy: 0.523\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 1.366 Accuracy: 0.518\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 1.357 Accuracy: 0.527\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 1.382 Accuracy: 0.527\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 1.367 Accuracy: 0.526\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 1.351 Accuracy: 0.533\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 1.352 Accuracy: 0.523\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 1.329 Accuracy: 0.530\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 1.364 Accuracy: 0.529\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 1.362 Accuracy: 0.526\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 1.347 Accuracy: 0.540\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 1.345 Accuracy: 0.545\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 1.334 Accuracy: 0.536\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 1.373 Accuracy: 0.520\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 1.353 Accuracy: 0.526\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 1.343 Accuracy: 0.533\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 1.340 Accuracy: 0.538\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 1.331 Accuracy: 0.532\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 1.336 Accuracy: 0.543\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 1.361 Accuracy: 0.524\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 1.323 Accuracy: 0.549\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 1.338 Accuracy: 0.539\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 1.325 Accuracy: 0.530\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 1.321 Accuracy: 0.547\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 1.354 Accuracy: 0.535\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 1.320 Accuracy: 0.545\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 1.322 Accuracy: 0.543\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 1.324 Accuracy: 0.540\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 1.326 Accuracy: 0.545\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 1.317 Accuracy: 0.546\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 1.335 Accuracy: 0.554\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 1.356 Accuracy: 0.530\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 1.304 Accuracy: 0.547\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 1.313 Accuracy: 0.540\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 1.354 Accuracy: 0.531\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 1.318 Accuracy: 0.557\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 1.321 Accuracy: 0.543\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 1.312 Accuracy: 0.540\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 1.352 Accuracy: 0.534\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 1.348 Accuracy: 0.532\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 1.318 Accuracy: 0.549\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 1.319 Accuracy: 0.550\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 1.311 Accuracy: 0.545\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 1.306 Accuracy: 0.546\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 1.333 Accuracy: 0.541\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 1.295 Accuracy: 0.562\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 1.321 Accuracy: 0.544\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 1.305 Accuracy: 0.543\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 1.335 Accuracy: 0.541\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 1.317 Accuracy: 0.540\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 1.309 Accuracy: 0.557\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 1.331 Accuracy: 0.538\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 1.316 Accuracy: 0.547\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 1.314 Accuracy: 0.548\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 1.298 Accuracy: 0.552\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 1.291 Accuracy: 0.563\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 1.311 Accuracy: 0.552\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 1.304 Accuracy: 0.550\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 1.327 Accuracy: 0.542\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 1.311 Accuracy: 0.548\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 1.282 Accuracy: 0.564\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 1.284 Accuracy: 0.562\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 1.269 Accuracy: 0.562\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 1.300 Accuracy: 0.553\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 1.287 Accuracy: 0.561\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 1.277 Accuracy: 0.564\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 1.279 Accuracy: 0.559\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 1.272 Accuracy: 0.562\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 1.296 Accuracy: 0.557\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 1.308 Accuracy: 0.546\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 1.293 Accuracy: 0.551\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 1.276 Accuracy: 0.561\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 1.291 Accuracy: 0.549\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 1.314 Accuracy: 0.545\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 1.325 Accuracy: 0.550\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 1.280 Accuracy: 0.565\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 1.315 Accuracy: 0.556\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 1.291 Accuracy: 0.556\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 1.294 Accuracy: 0.566\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 1.295 Accuracy: 0.560\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 1.301 Accuracy: 0.566\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 1.266 Accuracy: 0.567\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 1.259 Accuracy: 0.563\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 1.259 Accuracy: 0.567\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 1.293 Accuracy: 0.556\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 1.286 Accuracy: 0.570\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 1.325 Accuracy: 0.563\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 1.264 Accuracy: 0.566\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 1.278 Accuracy: 0.567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 1.286 Accuracy: 0.562\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 1.288 Accuracy: 0.573\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 1.268 Accuracy: 0.566\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 1.270 Accuracy: 0.556\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 1.279 Accuracy: 0.565\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 1.294 Accuracy: 0.559\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 1.290 Accuracy: 0.568\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 1.251 Accuracy: 0.571\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 1.264 Accuracy: 0.560\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 1.300 Accuracy: 0.567\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 1.299 Accuracy: 0.559\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 1.288 Accuracy: 0.571\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 1.272 Accuracy: 0.565\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 1.285 Accuracy: 0.558\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 1.271 Accuracy: 0.569\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 1.287 Accuracy: 0.565\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 1.274 Accuracy: 0.578\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 1.290 Accuracy: 0.563\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 1.264 Accuracy: 0.563\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 1.271 Accuracy: 0.577\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 1.287 Accuracy: 0.566\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 1.255 Accuracy: 0.576\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 1.259 Accuracy: 0.572\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 1.245 Accuracy: 0.570\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 1.280 Accuracy: 0.568\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 1.274 Accuracy: 0.568\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 1.283 Accuracy: 0.573\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 1.270 Accuracy: 0.564\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 1.260 Accuracy: 0.570\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 1.258 Accuracy: 0.576\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 1.270 Accuracy: 0.572\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 1.260 Accuracy: 0.583\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 1.263 Accuracy: 0.573\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 1.244 Accuracy: 0.570\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 1.302 Accuracy: 0.566\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 1.286 Accuracy: 0.563\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 1.273 Accuracy: 0.573\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 1.273 Accuracy: 0.569\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 1.268 Accuracy: 0.572\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 1.275 Accuracy: 0.578\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 1.277 Accuracy: 0.572\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 1.257 Accuracy: 0.577\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 1.243 Accuracy: 0.582\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 1.264 Accuracy: 0.572\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 1.263 Accuracy: 0.584\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 1.248 Accuracy: 0.579\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 1.270 Accuracy: 0.582\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 1.249 Accuracy: 0.578\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 1.254 Accuracy: 0.569\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 1.303 Accuracy: 0.570\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 1.277 Accuracy: 0.566\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 1.292 Accuracy: 0.578\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 1.265 Accuracy: 0.579\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 1.259 Accuracy: 0.574\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 1.292 Accuracy: 0.572\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 1.263 Accuracy: 0.574\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 1.289 Accuracy: 0.577\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 1.278 Accuracy: 0.575\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 1.241 Accuracy: 0.577\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 1.262 Accuracy: 0.574\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 1.282 Accuracy: 0.566\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 1.295 Accuracy: 0.576\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 1.285 Accuracy: 0.567\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 1.241 Accuracy: 0.583\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 1.281 Accuracy: 0.580\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 1.265 Accuracy: 0.583\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 1.284 Accuracy: 0.584\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 1.286 Accuracy: 0.575\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 1.247 Accuracy: 0.576\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 1.287 Accuracy: 0.574\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss: 1.257 Accuracy: 0.574\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss: 1.278 Accuracy: 0.586\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss: 1.261 Accuracy: 0.579\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss: 1.241 Accuracy: 0.577\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 1.283 Accuracy: 0.577\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss: 1.242 Accuracy: 0.580\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss: 1.293 Accuracy: 0.580\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss: 1.263 Accuracy: 0.584\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss: 1.240 Accuracy: 0.582\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 1.263 Accuracy: 0.575\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss: 1.277 Accuracy: 0.581\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss: 1.264 Accuracy: 0.590\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss: 1.265 Accuracy: 0.582\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss: 1.248 Accuracy: 0.570\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 1.285 Accuracy: 0.579\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss: 1.259 Accuracy: 0.582\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss: 1.284 Accuracy: 0.581\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss: 1.248 Accuracy: 0.581\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss: 1.236 Accuracy: 0.585\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 1.291 Accuracy: 0.576\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss: 1.245 Accuracy: 0.588\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss: 1.345 Accuracy: 0.587\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss: 1.276 Accuracy: 0.582\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss: 1.266 Accuracy: 0.584\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 1.262 Accuracy: 0.583\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss: 1.246 Accuracy: 0.581\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss: 1.256 Accuracy: 0.586\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss: 1.260 Accuracy: 0.581\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss: 1.256 Accuracy: 0.580\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 1.258 Accuracy: 0.581\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss: 1.258 Accuracy: 0.581\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss: 1.311 Accuracy: 0.591\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss: 1.268 Accuracy: 0.583\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss: 1.238 Accuracy: 0.584\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 1.264 Accuracy: 0.587\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss: 1.244 Accuracy: 0.585\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss: 1.281 Accuracy: 0.591\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss: 1.277 Accuracy: 0.590\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss: 1.292 Accuracy: 0.572\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 1.268 Accuracy: 0.580\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss: 1.256 Accuracy: 0.579\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss: 1.282 Accuracy: 0.589\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss: 1.266 Accuracy: 0.584\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss: 1.244 Accuracy: 0.579\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 1.287 Accuracy: 0.583\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss: 1.280 Accuracy: 0.585\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss: 1.288 Accuracy: 0.594\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss: 1.248 Accuracy: 0.583\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss: 1.249 Accuracy: 0.588\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 1.274 Accuracy: 0.586\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss: 1.258 Accuracy: 0.584\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss: 1.288 Accuracy: 0.591\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss: 1.273 Accuracy: 0.578\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss: 1.236 Accuracy: 0.588\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 1.264 Accuracy: 0.580\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss: 1.237 Accuracy: 0.582\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss: 1.288 Accuracy: 0.592\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss: 1.289 Accuracy: 0.598\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss: 1.266 Accuracy: 0.591\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 1.285 Accuracy: 0.583\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss: 1.257 Accuracy: 0.589\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss: 1.280 Accuracy: 0.592\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss: 1.299 Accuracy: 0.582\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss: 1.266 Accuracy: 0.588\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 1.261 Accuracy: 0.584\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss: 1.272 Accuracy: 0.593\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss: 1.277 Accuracy: 0.594\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss: 1.295 Accuracy: 0.587\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss: 1.259 Accuracy: 0.589\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 1.275 Accuracy: 0.576\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss: 1.270 Accuracy: 0.582\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss: 1.292 Accuracy: 0.593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, CIFAR-10 Batch 4:  Loss: 1.258 Accuracy: 0.587\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss: 1.282 Accuracy: 0.590\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 1.266 Accuracy: 0.587\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss: 1.277 Accuracy: 0.577\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss: 1.274 Accuracy: 0.594\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss: 1.300 Accuracy: 0.582\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss: 1.255 Accuracy: 0.591\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 1.293 Accuracy: 0.581\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss: 1.275 Accuracy: 0.588\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss: 1.323 Accuracy: 0.598\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss: 1.325 Accuracy: 0.594\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss: 1.259 Accuracy: 0.587\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 1.252 Accuracy: 0.593\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss: 1.270 Accuracy: 0.589\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss: 1.331 Accuracy: 0.598\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss: 1.314 Accuracy: 0.584\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss: 1.262 Accuracy: 0.593\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 1.277 Accuracy: 0.587\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss: 1.273 Accuracy: 0.589\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss: 1.350 Accuracy: 0.593\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss: 1.276 Accuracy: 0.586\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss: 1.286 Accuracy: 0.577\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 1.282 Accuracy: 0.585\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss: 1.290 Accuracy: 0.586\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss: 1.324 Accuracy: 0.596\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss: 1.267 Accuracy: 0.593\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss: 1.300 Accuracy: 0.579\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 1.274 Accuracy: 0.587\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss: 1.279 Accuracy: 0.582\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss: 1.290 Accuracy: 0.594\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss: 1.314 Accuracy: 0.595\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss: 1.291 Accuracy: 0.585\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 1.297 Accuracy: 0.586\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss: 1.273 Accuracy: 0.589\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss: 1.280 Accuracy: 0.593\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss: 1.273 Accuracy: 0.590\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss: 1.274 Accuracy: 0.592\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 1.285 Accuracy: 0.583\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss: 1.278 Accuracy: 0.593\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss: 1.308 Accuracy: 0.595\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss: 1.309 Accuracy: 0.596\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss: 1.316 Accuracy: 0.588\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 1.301 Accuracy: 0.585\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss: 1.263 Accuracy: 0.594\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss: 1.336 Accuracy: 0.596\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss: 1.289 Accuracy: 0.588\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss: 1.307 Accuracy: 0.588\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 1.317 Accuracy: 0.584\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss: 1.287 Accuracy: 0.587\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss: 1.311 Accuracy: 0.597\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss: 1.293 Accuracy: 0.590\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss: 1.307 Accuracy: 0.593\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 1.291 Accuracy: 0.586\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss: 1.310 Accuracy: 0.586\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss: 1.324 Accuracy: 0.605\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss: 1.305 Accuracy: 0.591\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss: 1.318 Accuracy: 0.595\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 1.308 Accuracy: 0.594\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss: 1.273 Accuracy: 0.593\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss: 1.324 Accuracy: 0.599\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss: 1.340 Accuracy: 0.588\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss: 1.289 Accuracy: 0.585\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 1.311 Accuracy: 0.583\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss: 1.296 Accuracy: 0.593\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss: 1.332 Accuracy: 0.585\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss: 1.328 Accuracy: 0.591\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss: 1.318 Accuracy: 0.585\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 1.349 Accuracy: 0.585\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss: 1.283 Accuracy: 0.598\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss: 1.352 Accuracy: 0.594\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss: 1.303 Accuracy: 0.597\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss: 1.305 Accuracy: 0.592\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 1.348 Accuracy: 0.587\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss: 1.311 Accuracy: 0.590\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss: 1.328 Accuracy: 0.599\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss: 1.320 Accuracy: 0.589\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss: 1.333 Accuracy: 0.587\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 1.324 Accuracy: 0.573\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss: 1.328 Accuracy: 0.588\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss: 1.394 Accuracy: 0.594\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss: 1.309 Accuracy: 0.585\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss: 1.332 Accuracy: 0.586\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 1.354 Accuracy: 0.584\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss: 1.320 Accuracy: 0.593\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss: 1.399 Accuracy: 0.594\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss: 1.294 Accuracy: 0.592\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss: 1.321 Accuracy: 0.591\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 1.356 Accuracy: 0.582\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss: 1.283 Accuracy: 0.594\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss: 1.381 Accuracy: 0.598\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss: 1.311 Accuracy: 0.596\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss: 1.324 Accuracy: 0.592\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 1.345 Accuracy: 0.586\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss: 1.300 Accuracy: 0.599\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss: 1.428 Accuracy: 0.596\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss: 1.315 Accuracy: 0.599\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss: 1.348 Accuracy: 0.585\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 1.365 Accuracy: 0.582\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss: 1.372 Accuracy: 0.589\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss: 1.389 Accuracy: 0.598\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss: 1.358 Accuracy: 0.596\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss: 1.353 Accuracy: 0.584\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 1.349 Accuracy: 0.590\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss: 1.318 Accuracy: 0.597\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss: 1.324 Accuracy: 0.601\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss: 1.318 Accuracy: 0.595\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss: 1.359 Accuracy: 0.596\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 1.437 Accuracy: 0.580\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss: 1.348 Accuracy: 0.602\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss: 1.455 Accuracy: 0.601\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss: 1.333 Accuracy: 0.599\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss: 1.380 Accuracy: 0.584\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 1.380 Accuracy: 0.583\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss: 1.451 Accuracy: 0.578\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss: 1.476 Accuracy: 0.599\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss: 1.390 Accuracy: 0.591\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss: 1.329 Accuracy: 0.585\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 1.426 Accuracy: 0.581\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss: 1.388 Accuracy: 0.591\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss: 1.416 Accuracy: 0.596\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss: 1.366 Accuracy: 0.597\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss: 1.336 Accuracy: 0.593\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 1.411 Accuracy: 0.578\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss: 1.356 Accuracy: 0.589\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss: 1.407 Accuracy: 0.602\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss: 1.314 Accuracy: 0.601\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss: 1.325 Accuracy: 0.590\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 1.398 Accuracy: 0.596\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss: 1.358 Accuracy: 0.596\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss: 1.460 Accuracy: 0.599\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss: 1.362 Accuracy: 0.594\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss: 1.340 Accuracy: 0.582\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 1.420 Accuracy: 0.581\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss: 1.376 Accuracy: 0.588\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss: 1.452 Accuracy: 0.602\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss: 1.313 Accuracy: 0.599\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss: 1.362 Accuracy: 0.586\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 1.369 Accuracy: 0.591\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss: 1.326 Accuracy: 0.592\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss: 1.420 Accuracy: 0.597\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss: 1.321 Accuracy: 0.598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143, CIFAR-10 Batch 5:  Loss: 1.334 Accuracy: 0.593\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 1.411 Accuracy: 0.595\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss: 1.351 Accuracy: 0.597\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss: 1.389 Accuracy: 0.608\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss: 1.354 Accuracy: 0.591\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss: 1.344 Accuracy: 0.596\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 1.394 Accuracy: 0.590\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss: 1.355 Accuracy: 0.598\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss: 1.399 Accuracy: 0.605\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss: 1.325 Accuracy: 0.604\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss: 1.347 Accuracy: 0.600\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 1.429 Accuracy: 0.588\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss: 1.379 Accuracy: 0.596\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss: 1.464 Accuracy: 0.604\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss: 1.375 Accuracy: 0.597\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss: 1.363 Accuracy: 0.591\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 1.429 Accuracy: 0.593\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss: 1.382 Accuracy: 0.593\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss: 1.454 Accuracy: 0.600\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss: 1.353 Accuracy: 0.595\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss: 1.388 Accuracy: 0.594\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 1.448 Accuracy: 0.590\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss: 1.358 Accuracy: 0.602\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss: 1.515 Accuracy: 0.598\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss: 1.397 Accuracy: 0.587\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss: 1.387 Accuracy: 0.601\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 1.383 Accuracy: 0.596\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss: 1.362 Accuracy: 0.601\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss: 1.412 Accuracy: 0.602\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss: 1.348 Accuracy: 0.599\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss: 1.344 Accuracy: 0.593\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 1.414 Accuracy: 0.587\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss: 1.370 Accuracy: 0.593\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss: 1.436 Accuracy: 0.601\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss: 1.341 Accuracy: 0.599\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss: 1.384 Accuracy: 0.593\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 1.435 Accuracy: 0.584\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss: 1.335 Accuracy: 0.595\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss: 1.398 Accuracy: 0.600\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss: 1.401 Accuracy: 0.591\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss: 1.362 Accuracy: 0.599\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 1.436 Accuracy: 0.589\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss: 1.362 Accuracy: 0.596\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss: 1.472 Accuracy: 0.600\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss: 1.422 Accuracy: 0.595\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss: 1.421 Accuracy: 0.587\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 1.413 Accuracy: 0.593\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss: 1.373 Accuracy: 0.591\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss: 1.404 Accuracy: 0.602\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss: 1.369 Accuracy: 0.600\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss: 1.414 Accuracy: 0.593\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 1.444 Accuracy: 0.589\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss: 1.379 Accuracy: 0.599\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss: 1.467 Accuracy: 0.597\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss: 1.410 Accuracy: 0.593\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss: 1.393 Accuracy: 0.593\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 1.444 Accuracy: 0.589\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss: 1.383 Accuracy: 0.592\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss: 1.467 Accuracy: 0.602\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss: 1.393 Accuracy: 0.593\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss: 1.392 Accuracy: 0.596\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 1.449 Accuracy: 0.588\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss: 1.367 Accuracy: 0.597\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss: 1.463 Accuracy: 0.605\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss: 1.403 Accuracy: 0.595\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss: 1.417 Accuracy: 0.597\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 1.436 Accuracy: 0.596\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss: 1.427 Accuracy: 0.596\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss: 1.491 Accuracy: 0.596\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss: 1.382 Accuracy: 0.592\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss: 1.434 Accuracy: 0.593\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 1.465 Accuracy: 0.594\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss: 1.397 Accuracy: 0.593\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss: 1.517 Accuracy: 0.601\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss: 1.401 Accuracy: 0.596\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss: 1.474 Accuracy: 0.594\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 1.488 Accuracy: 0.589\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss: 1.422 Accuracy: 0.590\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss: 1.498 Accuracy: 0.600\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss: 1.390 Accuracy: 0.599\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss: 1.435 Accuracy: 0.601\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 1.447 Accuracy: 0.589\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss: 1.424 Accuracy: 0.593\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss: 1.486 Accuracy: 0.604\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss: 1.439 Accuracy: 0.592\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss: 1.443 Accuracy: 0.593\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 1.455 Accuracy: 0.593\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss: 1.437 Accuracy: 0.590\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss: 1.500 Accuracy: 0.600\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss: 1.402 Accuracy: 0.595\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss: 1.412 Accuracy: 0.597\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 1.566 Accuracy: 0.587\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss: 1.447 Accuracy: 0.591\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss: 1.551 Accuracy: 0.596\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss: 1.454 Accuracy: 0.597\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss: 1.426 Accuracy: 0.592\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 1.431 Accuracy: 0.596\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss: 1.470 Accuracy: 0.599\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss: 1.475 Accuracy: 0.600\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss: 1.468 Accuracy: 0.597\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss: 1.460 Accuracy: 0.605\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 1.500 Accuracy: 0.599\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss: 1.461 Accuracy: 0.596\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss: 1.558 Accuracy: 0.600\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss: 1.444 Accuracy: 0.596\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss: 1.458 Accuracy: 0.597\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 1.484 Accuracy: 0.601\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss: 1.488 Accuracy: 0.597\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss: 1.581 Accuracy: 0.595\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss: 1.436 Accuracy: 0.591\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss: 1.432 Accuracy: 0.597\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 1.490 Accuracy: 0.598\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss: 1.493 Accuracy: 0.595\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss: 1.628 Accuracy: 0.601\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss: 1.445 Accuracy: 0.601\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss: 1.483 Accuracy: 0.598\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 1.505 Accuracy: 0.595\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss: 1.465 Accuracy: 0.598\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss: 1.535 Accuracy: 0.595\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss: 1.443 Accuracy: 0.601\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss: 1.461 Accuracy: 0.593\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 1.471 Accuracy: 0.597\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss: 1.564 Accuracy: 0.591\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss: 1.529 Accuracy: 0.597\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss: 1.438 Accuracy: 0.599\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss: 1.503 Accuracy: 0.604\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 1.487 Accuracy: 0.601\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss: 1.523 Accuracy: 0.588\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss: 1.580 Accuracy: 0.589\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss: 1.442 Accuracy: 0.593\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss: 1.514 Accuracy: 0.600\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 1.484 Accuracy: 0.602\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss: 1.511 Accuracy: 0.595\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss: 1.542 Accuracy: 0.598\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss: 1.496 Accuracy: 0.594\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss: 1.493 Accuracy: 0.603\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 1.493 Accuracy: 0.600\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss: 1.548 Accuracy: 0.582\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss: 1.616 Accuracy: 0.593\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss: 1.491 Accuracy: 0.596\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss: 1.498 Accuracy: 0.603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 1.496 Accuracy: 0.602\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss: 1.647 Accuracy: 0.586\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss: 1.575 Accuracy: 0.599\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss: 1.485 Accuracy: 0.599\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss: 1.492 Accuracy: 0.604\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 1.512 Accuracy: 0.599\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss: 1.667 Accuracy: 0.578\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss: 1.618 Accuracy: 0.596\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss: 1.474 Accuracy: 0.584\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss: 1.500 Accuracy: 0.602\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 1.552 Accuracy: 0.600\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss: 1.628 Accuracy: 0.583\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss: 1.634 Accuracy: 0.599\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss: 1.506 Accuracy: 0.598\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss: 1.539 Accuracy: 0.600\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 1.496 Accuracy: 0.602\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss: 1.602 Accuracy: 0.586\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss: 1.618 Accuracy: 0.602\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss: 1.513 Accuracy: 0.597\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss: 1.545 Accuracy: 0.602\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 1.518 Accuracy: 0.596\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss: 1.628 Accuracy: 0.581\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss: 1.633 Accuracy: 0.596\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss: 1.476 Accuracy: 0.589\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss: 1.549 Accuracy: 0.600\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 1.516 Accuracy: 0.597\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss: 1.626 Accuracy: 0.587\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss: 1.599 Accuracy: 0.596\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss: 1.554 Accuracy: 0.591\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss: 1.541 Accuracy: 0.596\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 1.494 Accuracy: 0.594\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss: 1.602 Accuracy: 0.596\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss: 1.593 Accuracy: 0.598\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss: 1.507 Accuracy: 0.598\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss: 1.534 Accuracy: 0.605\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 1.552 Accuracy: 0.601\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss: 1.669 Accuracy: 0.584\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss: 1.626 Accuracy: 0.600\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss: 1.542 Accuracy: 0.595\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss: 1.520 Accuracy: 0.595\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 1.534 Accuracy: 0.600\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss: 1.624 Accuracy: 0.591\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss: 1.667 Accuracy: 0.599\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss: 1.596 Accuracy: 0.593\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss: 1.552 Accuracy: 0.598\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 1.563 Accuracy: 0.597\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss: 1.672 Accuracy: 0.585\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss: 1.630 Accuracy: 0.598\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss: 1.584 Accuracy: 0.591\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss: 1.560 Accuracy: 0.601\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 1.563 Accuracy: 0.604\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss: 1.637 Accuracy: 0.590\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss: 1.645 Accuracy: 0.603\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss: 1.610 Accuracy: 0.599\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss: 1.622 Accuracy: 0.592\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 1.515 Accuracy: 0.597\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss: 1.637 Accuracy: 0.597\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss: 1.613 Accuracy: 0.603\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss: 1.592 Accuracy: 0.604\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss: 1.630 Accuracy: 0.597\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 1.575 Accuracy: 0.601\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss: 1.655 Accuracy: 0.594\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss: 1.660 Accuracy: 0.600\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss: 1.596 Accuracy: 0.609\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss: 1.680 Accuracy: 0.608\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 1.550 Accuracy: 0.604\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss: 1.664 Accuracy: 0.587\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss: 1.612 Accuracy: 0.596\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss: 1.629 Accuracy: 0.596\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss: 1.602 Accuracy: 0.598\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 1.575 Accuracy: 0.597\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss: 1.598 Accuracy: 0.604\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss: 1.673 Accuracy: 0.602\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss: 1.659 Accuracy: 0.607\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss: 1.626 Accuracy: 0.601\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 1.572 Accuracy: 0.602\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss: 1.631 Accuracy: 0.601\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss: 1.668 Accuracy: 0.595\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss: 1.672 Accuracy: 0.601\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss: 1.572 Accuracy: 0.598\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 1.572 Accuracy: 0.604\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss: 1.634 Accuracy: 0.604\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss: 1.589 Accuracy: 0.604\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss: 1.636 Accuracy: 0.599\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss: 1.572 Accuracy: 0.604\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 1.570 Accuracy: 0.600\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss: 1.678 Accuracy: 0.592\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss: 1.690 Accuracy: 0.605\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss: 1.708 Accuracy: 0.600\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss: 1.617 Accuracy: 0.601\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 1.567 Accuracy: 0.598\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss: 1.671 Accuracy: 0.589\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss: 1.700 Accuracy: 0.597\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss: 1.611 Accuracy: 0.604\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss: 1.622 Accuracy: 0.599\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 1.579 Accuracy: 0.599\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss: 1.668 Accuracy: 0.593\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss: 1.720 Accuracy: 0.604\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss: 1.720 Accuracy: 0.598\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss: 1.633 Accuracy: 0.603\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 1.655 Accuracy: 0.605\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss: 1.684 Accuracy: 0.597\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss: 1.678 Accuracy: 0.601\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss: 1.681 Accuracy: 0.605\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss: 1.667 Accuracy: 0.604\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 1.639 Accuracy: 0.602\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss: 1.691 Accuracy: 0.602\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss: 1.735 Accuracy: 0.606\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss: 1.730 Accuracy: 0.609\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss: 1.629 Accuracy: 0.601\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 1.622 Accuracy: 0.605\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss: 1.709 Accuracy: 0.597\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss: 1.733 Accuracy: 0.599\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss: 1.764 Accuracy: 0.600\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss: 1.689 Accuracy: 0.606\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 1.674 Accuracy: 0.603\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss: 1.677 Accuracy: 0.604\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss: 1.724 Accuracy: 0.603\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss: 1.761 Accuracy: 0.596\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss: 1.660 Accuracy: 0.603\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 1.687 Accuracy: 0.601\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss: 1.710 Accuracy: 0.596\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss: 1.748 Accuracy: 0.596\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss: 1.731 Accuracy: 0.599\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss: 1.663 Accuracy: 0.604\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 1.632 Accuracy: 0.606\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss: 1.721 Accuracy: 0.597\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss: 1.709 Accuracy: 0.608\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss: 1.779 Accuracy: 0.606\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss: 1.666 Accuracy: 0.603\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 1.730 Accuracy: 0.606\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss: 1.716 Accuracy: 0.601\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss: 1.697 Accuracy: 0.603\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss: 1.722 Accuracy: 0.606\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss: 1.657 Accuracy: 0.608\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 1.686 Accuracy: 0.601\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss: 1.712 Accuracy: 0.602\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss: 1.756 Accuracy: 0.599\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss: 1.787 Accuracy: 0.606\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss: 1.660 Accuracy: 0.602\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 1.687 Accuracy: 0.600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, CIFAR-10 Batch 2:  Loss: 1.780 Accuracy: 0.595\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss: 1.697 Accuracy: 0.603\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss: 1.776 Accuracy: 0.609\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss: 1.723 Accuracy: 0.604\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss: 1.708 Accuracy: 0.595\n",
      "Epoch 201, CIFAR-10 Batch 2:  Loss: 1.727 Accuracy: 0.602\n",
      "Epoch 201, CIFAR-10 Batch 3:  Loss: 1.785 Accuracy: 0.601\n",
      "Epoch 201, CIFAR-10 Batch 4:  Loss: 1.765 Accuracy: 0.604\n",
      "Epoch 201, CIFAR-10 Batch 5:  Loss: 1.668 Accuracy: 0.609\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss: 1.727 Accuracy: 0.603\n",
      "Epoch 202, CIFAR-10 Batch 2:  Loss: 1.802 Accuracy: 0.590\n",
      "Epoch 202, CIFAR-10 Batch 3:  Loss: 1.755 Accuracy: 0.596\n",
      "Epoch 202, CIFAR-10 Batch 4:  Loss: 1.832 Accuracy: 0.608\n",
      "Epoch 202, CIFAR-10 Batch 5:  Loss: 1.705 Accuracy: 0.606\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss: 1.707 Accuracy: 0.604\n",
      "Epoch 203, CIFAR-10 Batch 2:  Loss: 1.797 Accuracy: 0.597\n",
      "Epoch 203, CIFAR-10 Batch 3:  Loss: 1.821 Accuracy: 0.589\n",
      "Epoch 203, CIFAR-10 Batch 4:  Loss: 1.778 Accuracy: 0.609\n",
      "Epoch 203, CIFAR-10 Batch 5:  Loss: 1.762 Accuracy: 0.600\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss: 1.756 Accuracy: 0.604\n",
      "Epoch 204, CIFAR-10 Batch 2:  Loss: 1.876 Accuracy: 0.593\n",
      "Epoch 204, CIFAR-10 Batch 3:  Loss: 1.789 Accuracy: 0.602\n",
      "Epoch 204, CIFAR-10 Batch 4:  Loss: 1.836 Accuracy: 0.607\n",
      "Epoch 204, CIFAR-10 Batch 5:  Loss: 1.744 Accuracy: 0.607\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss: 1.775 Accuracy: 0.601\n",
      "Epoch 205, CIFAR-10 Batch 2:  Loss: 1.858 Accuracy: 0.598\n",
      "Epoch 205, CIFAR-10 Batch 3:  Loss: 1.845 Accuracy: 0.594\n",
      "Epoch 205, CIFAR-10 Batch 4:  Loss: 1.766 Accuracy: 0.604\n",
      "Epoch 205, CIFAR-10 Batch 5:  Loss: 1.823 Accuracy: 0.604\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss: 1.776 Accuracy: 0.595\n",
      "Epoch 206, CIFAR-10 Batch 2:  Loss: 1.775 Accuracy: 0.597\n",
      "Epoch 206, CIFAR-10 Batch 3:  Loss: 1.880 Accuracy: 0.592\n",
      "Epoch 206, CIFAR-10 Batch 4:  Loss: 1.809 Accuracy: 0.605\n",
      "Epoch 206, CIFAR-10 Batch 5:  Loss: 1.774 Accuracy: 0.602\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss: 1.782 Accuracy: 0.598\n",
      "Epoch 207, CIFAR-10 Batch 2:  Loss: 1.761 Accuracy: 0.601\n",
      "Epoch 207, CIFAR-10 Batch 3:  Loss: 1.943 Accuracy: 0.585\n",
      "Epoch 207, CIFAR-10 Batch 4:  Loss: 1.809 Accuracy: 0.603\n",
      "Epoch 207, CIFAR-10 Batch 5:  Loss: 1.825 Accuracy: 0.605\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss: 1.800 Accuracy: 0.599\n",
      "Epoch 208, CIFAR-10 Batch 2:  Loss: 1.744 Accuracy: 0.592\n",
      "Epoch 208, CIFAR-10 Batch 3:  Loss: 1.885 Accuracy: 0.597\n",
      "Epoch 208, CIFAR-10 Batch 4:  Loss: 1.812 Accuracy: 0.606\n",
      "Epoch 208, CIFAR-10 Batch 5:  Loss: 1.759 Accuracy: 0.607\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss: 1.776 Accuracy: 0.590\n",
      "Epoch 209, CIFAR-10 Batch 2:  Loss: 1.768 Accuracy: 0.600\n",
      "Epoch 209, CIFAR-10 Batch 3:  Loss: 1.895 Accuracy: 0.594\n",
      "Epoch 209, CIFAR-10 Batch 4:  Loss: 1.883 Accuracy: 0.605\n",
      "Epoch 209, CIFAR-10 Batch 5:  Loss: 1.825 Accuracy: 0.603\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss: 1.818 Accuracy: 0.590\n",
      "Epoch 210, CIFAR-10 Batch 2:  Loss: 1.779 Accuracy: 0.597\n",
      "Epoch 210, CIFAR-10 Batch 3:  Loss: 1.917 Accuracy: 0.592\n",
      "Epoch 210, CIFAR-10 Batch 4:  Loss: 1.878 Accuracy: 0.601\n",
      "Epoch 210, CIFAR-10 Batch 5:  Loss: 1.842 Accuracy: 0.609\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss: 1.852 Accuracy: 0.593\n",
      "Epoch 211, CIFAR-10 Batch 2:  Loss: 1.819 Accuracy: 0.593\n",
      "Epoch 211, CIFAR-10 Batch 3:  Loss: 1.920 Accuracy: 0.595\n",
      "Epoch 211, CIFAR-10 Batch 4:  Loss: 1.858 Accuracy: 0.609\n",
      "Epoch 211, CIFAR-10 Batch 5:  Loss: 1.826 Accuracy: 0.605\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss: 1.836 Accuracy: 0.595\n",
      "Epoch 212, CIFAR-10 Batch 2:  Loss: 1.876 Accuracy: 0.600\n",
      "Epoch 212, CIFAR-10 Batch 3:  Loss: 1.896 Accuracy: 0.591\n",
      "Epoch 212, CIFAR-10 Batch 4:  Loss: 1.937 Accuracy: 0.604\n",
      "Epoch 212, CIFAR-10 Batch 5:  Loss: 1.789 Accuracy: 0.612\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss: 1.822 Accuracy: 0.592\n",
      "Epoch 213, CIFAR-10 Batch 2:  Loss: 1.824 Accuracy: 0.600\n",
      "Epoch 213, CIFAR-10 Batch 3:  Loss: 1.891 Accuracy: 0.603\n",
      "Epoch 213, CIFAR-10 Batch 4:  Loss: 1.949 Accuracy: 0.602\n",
      "Epoch 213, CIFAR-10 Batch 5:  Loss: 1.873 Accuracy: 0.608\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss: 1.864 Accuracy: 0.591\n",
      "Epoch 214, CIFAR-10 Batch 2:  Loss: 1.893 Accuracy: 0.601\n",
      "Epoch 214, CIFAR-10 Batch 3:  Loss: 1.874 Accuracy: 0.597\n",
      "Epoch 214, CIFAR-10 Batch 4:  Loss: 1.996 Accuracy: 0.600\n",
      "Epoch 214, CIFAR-10 Batch 5:  Loss: 1.890 Accuracy: 0.612\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss: 1.808 Accuracy: 0.598\n",
      "Epoch 215, CIFAR-10 Batch 2:  Loss: 1.861 Accuracy: 0.598\n",
      "Epoch 215, CIFAR-10 Batch 3:  Loss: 1.845 Accuracy: 0.596\n",
      "Epoch 215, CIFAR-10 Batch 4:  Loss: 1.916 Accuracy: 0.603\n",
      "Epoch 215, CIFAR-10 Batch 5:  Loss: 1.815 Accuracy: 0.600\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss: 1.908 Accuracy: 0.591\n",
      "Epoch 216, CIFAR-10 Batch 2:  Loss: 1.897 Accuracy: 0.602\n",
      "Epoch 216, CIFAR-10 Batch 3:  Loss: 1.947 Accuracy: 0.598\n",
      "Epoch 216, CIFAR-10 Batch 4:  Loss: 1.902 Accuracy: 0.603\n",
      "Epoch 216, CIFAR-10 Batch 5:  Loss: 1.842 Accuracy: 0.598\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss: 1.896 Accuracy: 0.598\n",
      "Epoch 217, CIFAR-10 Batch 2:  Loss: 1.851 Accuracy: 0.603\n",
      "Epoch 217, CIFAR-10 Batch 3:  Loss: 1.869 Accuracy: 0.603\n",
      "Epoch 217, CIFAR-10 Batch 4:  Loss: 1.988 Accuracy: 0.603\n",
      "Epoch 217, CIFAR-10 Batch 5:  Loss: 1.836 Accuracy: 0.596\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss: 1.851 Accuracy: 0.596\n",
      "Epoch 218, CIFAR-10 Batch 2:  Loss: 1.930 Accuracy: 0.600\n",
      "Epoch 218, CIFAR-10 Batch 3:  Loss: 1.923 Accuracy: 0.598\n",
      "Epoch 218, CIFAR-10 Batch 4:  Loss: 1.934 Accuracy: 0.597\n",
      "Epoch 218, CIFAR-10 Batch 5:  Loss: 1.825 Accuracy: 0.605\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss: 1.841 Accuracy: 0.592\n",
      "Epoch 219, CIFAR-10 Batch 2:  Loss: 1.999 Accuracy: 0.601\n",
      "Epoch 219, CIFAR-10 Batch 3:  Loss: 1.975 Accuracy: 0.591\n",
      "Epoch 219, CIFAR-10 Batch 4:  Loss: 1.951 Accuracy: 0.595\n",
      "Epoch 219, CIFAR-10 Batch 5:  Loss: 1.839 Accuracy: 0.610\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss: 1.884 Accuracy: 0.594\n",
      "Epoch 220, CIFAR-10 Batch 2:  Loss: 2.003 Accuracy: 0.597\n",
      "Epoch 220, CIFAR-10 Batch 3:  Loss: 1.898 Accuracy: 0.601\n",
      "Epoch 220, CIFAR-10 Batch 4:  Loss: 1.929 Accuracy: 0.601\n",
      "Epoch 220, CIFAR-10 Batch 5:  Loss: 1.890 Accuracy: 0.596\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss: 1.917 Accuracy: 0.594\n",
      "Epoch 221, CIFAR-10 Batch 2:  Loss: 1.920 Accuracy: 0.599\n",
      "Epoch 221, CIFAR-10 Batch 3:  Loss: 1.882 Accuracy: 0.596\n",
      "Epoch 221, CIFAR-10 Batch 4:  Loss: 1.977 Accuracy: 0.594\n",
      "Epoch 221, CIFAR-10 Batch 5:  Loss: 1.937 Accuracy: 0.603\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss: 1.860 Accuracy: 0.596\n",
      "Epoch 222, CIFAR-10 Batch 2:  Loss: 2.019 Accuracy: 0.596\n",
      "Epoch 222, CIFAR-10 Batch 3:  Loss: 1.980 Accuracy: 0.599\n",
      "Epoch 222, CIFAR-10 Batch 4:  Loss: 1.855 Accuracy: 0.603\n",
      "Epoch 222, CIFAR-10 Batch 5:  Loss: 1.849 Accuracy: 0.606\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss: 2.051 Accuracy: 0.599\n",
      "Epoch 223, CIFAR-10 Batch 2:  Loss: 2.021 Accuracy: 0.598\n",
      "Epoch 223, CIFAR-10 Batch 3:  Loss: 1.920 Accuracy: 0.598\n",
      "Epoch 223, CIFAR-10 Batch 4:  Loss: 1.885 Accuracy: 0.601\n",
      "Epoch 223, CIFAR-10 Batch 5:  Loss: 1.862 Accuracy: 0.608\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss: 1.909 Accuracy: 0.587\n",
      "Epoch 224, CIFAR-10 Batch 2:  Loss: 1.961 Accuracy: 0.601\n",
      "Epoch 224, CIFAR-10 Batch 3:  Loss: 1.964 Accuracy: 0.590\n",
      "Epoch 224, CIFAR-10 Batch 4:  Loss: 1.939 Accuracy: 0.599\n",
      "Epoch 224, CIFAR-10 Batch 5:  Loss: 1.873 Accuracy: 0.602\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss: 1.900 Accuracy: 0.590\n",
      "Epoch 225, CIFAR-10 Batch 2:  Loss: 1.968 Accuracy: 0.602\n",
      "Epoch 225, CIFAR-10 Batch 3:  Loss: 2.025 Accuracy: 0.591\n",
      "Epoch 225, CIFAR-10 Batch 4:  Loss: 1.964 Accuracy: 0.596\n",
      "Epoch 225, CIFAR-10 Batch 5:  Loss: 1.896 Accuracy: 0.604\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss: 2.011 Accuracy: 0.594\n",
      "Epoch 226, CIFAR-10 Batch 2:  Loss: 1.934 Accuracy: 0.594\n",
      "Epoch 226, CIFAR-10 Batch 3:  Loss: 1.966 Accuracy: 0.605\n",
      "Epoch 226, CIFAR-10 Batch 4:  Loss: 1.943 Accuracy: 0.596\n",
      "Epoch 226, CIFAR-10 Batch 5:  Loss: 1.859 Accuracy: 0.609\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss: 1.947 Accuracy: 0.592\n",
      "Epoch 227, CIFAR-10 Batch 2:  Loss: 1.878 Accuracy: 0.603\n",
      "Epoch 227, CIFAR-10 Batch 3:  Loss: 2.039 Accuracy: 0.594\n",
      "Epoch 227, CIFAR-10 Batch 4:  Loss: 1.987 Accuracy: 0.598\n",
      "Epoch 227, CIFAR-10 Batch 5:  Loss: 1.975 Accuracy: 0.609\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss: 1.947 Accuracy: 0.598\n",
      "Epoch 228, CIFAR-10 Batch 2:  Loss: 2.033 Accuracy: 0.598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228, CIFAR-10 Batch 3:  Loss: 2.003 Accuracy: 0.597\n",
      "Epoch 228, CIFAR-10 Batch 4:  Loss: 2.001 Accuracy: 0.596\n",
      "Epoch 228, CIFAR-10 Batch 5:  Loss: 1.850 Accuracy: 0.611\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss: 2.002 Accuracy: 0.594\n",
      "Epoch 229, CIFAR-10 Batch 2:  Loss: 2.089 Accuracy: 0.595\n",
      "Epoch 229, CIFAR-10 Batch 3:  Loss: 2.107 Accuracy: 0.596\n",
      "Epoch 229, CIFAR-10 Batch 4:  Loss: 1.916 Accuracy: 0.600\n",
      "Epoch 229, CIFAR-10 Batch 5:  Loss: 1.966 Accuracy: 0.604\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss: 2.045 Accuracy: 0.591\n",
      "Epoch 230, CIFAR-10 Batch 2:  Loss: 2.124 Accuracy: 0.598\n",
      "Epoch 230, CIFAR-10 Batch 3:  Loss: 2.124 Accuracy: 0.598\n",
      "Epoch 230, CIFAR-10 Batch 4:  Loss: 2.084 Accuracy: 0.596\n",
      "Epoch 230, CIFAR-10 Batch 5:  Loss: 2.021 Accuracy: 0.610\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss: 2.015 Accuracy: 0.592\n",
      "Epoch 231, CIFAR-10 Batch 2:  Loss: 2.023 Accuracy: 0.604\n",
      "Epoch 231, CIFAR-10 Batch 3:  Loss: 2.114 Accuracy: 0.595\n",
      "Epoch 231, CIFAR-10 Batch 4:  Loss: 2.043 Accuracy: 0.591\n",
      "Epoch 231, CIFAR-10 Batch 5:  Loss: 2.003 Accuracy: 0.605\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss: 2.028 Accuracy: 0.596\n",
      "Epoch 232, CIFAR-10 Batch 2:  Loss: 2.103 Accuracy: 0.601\n",
      "Epoch 232, CIFAR-10 Batch 3:  Loss: 2.168 Accuracy: 0.590\n",
      "Epoch 232, CIFAR-10 Batch 4:  Loss: 2.021 Accuracy: 0.598\n",
      "Epoch 232, CIFAR-10 Batch 5:  Loss: 2.029 Accuracy: 0.609\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss: 2.126 Accuracy: 0.594\n",
      "Epoch 233, CIFAR-10 Batch 2:  Loss: 2.087 Accuracy: 0.599\n",
      "Epoch 233, CIFAR-10 Batch 3:  Loss: 2.158 Accuracy: 0.591\n",
      "Epoch 233, CIFAR-10 Batch 4:  Loss: 2.094 Accuracy: 0.593\n",
      "Epoch 233, CIFAR-10 Batch 5:  Loss: 2.016 Accuracy: 0.601\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss: 2.015 Accuracy: 0.599\n",
      "Epoch 234, CIFAR-10 Batch 2:  Loss: 2.099 Accuracy: 0.600\n",
      "Epoch 234, CIFAR-10 Batch 3:  Loss: 2.127 Accuracy: 0.596\n",
      "Epoch 234, CIFAR-10 Batch 4:  Loss: 2.013 Accuracy: 0.598\n",
      "Epoch 234, CIFAR-10 Batch 5:  Loss: 2.062 Accuracy: 0.603\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss: 2.021 Accuracy: 0.593\n",
      "Epoch 235, CIFAR-10 Batch 2:  Loss: 2.101 Accuracy: 0.597\n",
      "Epoch 235, CIFAR-10 Batch 3:  Loss: 2.139 Accuracy: 0.599\n",
      "Epoch 235, CIFAR-10 Batch 4:  Loss: 2.108 Accuracy: 0.595\n",
      "Epoch 235, CIFAR-10 Batch 5:  Loss: 2.098 Accuracy: 0.608\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss: 2.020 Accuracy: 0.594\n",
      "Epoch 236, CIFAR-10 Batch 2:  Loss: 2.071 Accuracy: 0.599\n",
      "Epoch 236, CIFAR-10 Batch 3:  Loss: 2.235 Accuracy: 0.592\n",
      "Epoch 236, CIFAR-10 Batch 4:  Loss: 2.035 Accuracy: 0.600\n",
      "Epoch 236, CIFAR-10 Batch 5:  Loss: 1.990 Accuracy: 0.605\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss: 2.121 Accuracy: 0.594\n",
      "Epoch 237, CIFAR-10 Batch 2:  Loss: 2.152 Accuracy: 0.595\n",
      "Epoch 237, CIFAR-10 Batch 3:  Loss: 2.252 Accuracy: 0.593\n",
      "Epoch 237, CIFAR-10 Batch 4:  Loss: 2.118 Accuracy: 0.595\n",
      "Epoch 237, CIFAR-10 Batch 5:  Loss: 2.013 Accuracy: 0.607\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss: 2.039 Accuracy: 0.596\n",
      "Epoch 238, CIFAR-10 Batch 2:  Loss: 2.106 Accuracy: 0.598\n",
      "Epoch 238, CIFAR-10 Batch 3:  Loss: 2.189 Accuracy: 0.595\n",
      "Epoch 238, CIFAR-10 Batch 4:  Loss: 2.103 Accuracy: 0.595\n",
      "Epoch 238, CIFAR-10 Batch 5:  Loss: 2.026 Accuracy: 0.604\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss: 2.139 Accuracy: 0.598\n",
      "Epoch 239, CIFAR-10 Batch 2:  Loss: 2.214 Accuracy: 0.604\n",
      "Epoch 239, CIFAR-10 Batch 3:  Loss: 2.136 Accuracy: 0.599\n",
      "Epoch 239, CIFAR-10 Batch 4:  Loss: 2.142 Accuracy: 0.597\n",
      "Epoch 239, CIFAR-10 Batch 5:  Loss: 2.059 Accuracy: 0.603\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss: 2.184 Accuracy: 0.592\n",
      "Epoch 240, CIFAR-10 Batch 2:  Loss: 2.238 Accuracy: 0.596\n",
      "Epoch 240, CIFAR-10 Batch 3:  Loss: 2.250 Accuracy: 0.597\n",
      "Epoch 240, CIFAR-10 Batch 4:  Loss: 2.039 Accuracy: 0.598\n",
      "Epoch 240, CIFAR-10 Batch 5:  Loss: 2.120 Accuracy: 0.600\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss: 2.085 Accuracy: 0.592\n",
      "Epoch 241, CIFAR-10 Batch 2:  Loss: 2.145 Accuracy: 0.600\n",
      "Epoch 241, CIFAR-10 Batch 3:  Loss: 2.246 Accuracy: 0.602\n",
      "Epoch 241, CIFAR-10 Batch 4:  Loss: 2.165 Accuracy: 0.594\n",
      "Epoch 241, CIFAR-10 Batch 5:  Loss: 2.111 Accuracy: 0.603\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss: 2.179 Accuracy: 0.597\n",
      "Epoch 242, CIFAR-10 Batch 2:  Loss: 2.208 Accuracy: 0.593\n",
      "Epoch 242, CIFAR-10 Batch 3:  Loss: 2.250 Accuracy: 0.597\n",
      "Epoch 242, CIFAR-10 Batch 4:  Loss: 2.165 Accuracy: 0.592\n",
      "Epoch 242, CIFAR-10 Batch 5:  Loss: 2.135 Accuracy: 0.604\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss: 2.228 Accuracy: 0.594\n",
      "Epoch 243, CIFAR-10 Batch 2:  Loss: 2.226 Accuracy: 0.596\n",
      "Epoch 243, CIFAR-10 Batch 3:  Loss: 2.116 Accuracy: 0.604\n",
      "Epoch 243, CIFAR-10 Batch 4:  Loss: 2.124 Accuracy: 0.597\n",
      "Epoch 243, CIFAR-10 Batch 5:  Loss: 2.091 Accuracy: 0.602\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss: 2.192 Accuracy: 0.596\n",
      "Epoch 244, CIFAR-10 Batch 2:  Loss: 2.238 Accuracy: 0.599\n",
      "Epoch 244, CIFAR-10 Batch 3:  Loss: 2.283 Accuracy: 0.601\n",
      "Epoch 244, CIFAR-10 Batch 4:  Loss: 2.191 Accuracy: 0.599\n",
      "Epoch 244, CIFAR-10 Batch 5:  Loss: 2.141 Accuracy: 0.601\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss: 2.264 Accuracy: 0.599\n",
      "Epoch 245, CIFAR-10 Batch 2:  Loss: 2.208 Accuracy: 0.596\n",
      "Epoch 245, CIFAR-10 Batch 3:  Loss: 2.306 Accuracy: 0.598\n",
      "Epoch 245, CIFAR-10 Batch 4:  Loss: 2.175 Accuracy: 0.598\n",
      "Epoch 245, CIFAR-10 Batch 5:  Loss: 2.106 Accuracy: 0.608\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss: 2.276 Accuracy: 0.589\n",
      "Epoch 246, CIFAR-10 Batch 2:  Loss: 2.316 Accuracy: 0.591\n",
      "Epoch 246, CIFAR-10 Batch 3:  Loss: 2.328 Accuracy: 0.594\n",
      "Epoch 246, CIFAR-10 Batch 4:  Loss: 2.195 Accuracy: 0.594\n",
      "Epoch 246, CIFAR-10 Batch 5:  Loss: 2.135 Accuracy: 0.605\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss: 2.293 Accuracy: 0.586\n",
      "Epoch 247, CIFAR-10 Batch 2:  Loss: 2.330 Accuracy: 0.590\n",
      "Epoch 247, CIFAR-10 Batch 3:  Loss: 2.385 Accuracy: 0.593\n",
      "Epoch 247, CIFAR-10 Batch 4:  Loss: 2.150 Accuracy: 0.593\n",
      "Epoch 247, CIFAR-10 Batch 5:  Loss: 2.210 Accuracy: 0.608\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss: 2.325 Accuracy: 0.592\n",
      "Epoch 248, CIFAR-10 Batch 2:  Loss: 2.247 Accuracy: 0.595\n",
      "Epoch 248, CIFAR-10 Batch 3:  Loss: 2.329 Accuracy: 0.596\n",
      "Epoch 248, CIFAR-10 Batch 4:  Loss: 2.130 Accuracy: 0.600\n",
      "Epoch 248, CIFAR-10 Batch 5:  Loss: 2.173 Accuracy: 0.603\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss: 2.430 Accuracy: 0.588\n",
      "Epoch 249, CIFAR-10 Batch 2:  Loss: 2.413 Accuracy: 0.594\n",
      "Epoch 249, CIFAR-10 Batch 3:  Loss: 2.413 Accuracy: 0.592\n",
      "Epoch 249, CIFAR-10 Batch 4:  Loss: 2.224 Accuracy: 0.600\n",
      "Epoch 249, CIFAR-10 Batch 5:  Loss: 2.175 Accuracy: 0.605\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss: 2.376 Accuracy: 0.592\n",
      "Epoch 250, CIFAR-10 Batch 2:  Loss: 2.294 Accuracy: 0.591\n",
      "Epoch 250, CIFAR-10 Batch 3:  Loss: 2.424 Accuracy: 0.592\n",
      "Epoch 250, CIFAR-10 Batch 4:  Loss: 2.258 Accuracy: 0.602\n",
      "Epoch 250, CIFAR-10 Batch 5:  Loss: 2.265 Accuracy: 0.603\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss: 2.414 Accuracy: 0.594\n",
      "Epoch 251, CIFAR-10 Batch 2:  Loss: 2.405 Accuracy: 0.594\n",
      "Epoch 251, CIFAR-10 Batch 3:  Loss: 2.381 Accuracy: 0.597\n",
      "Epoch 251, CIFAR-10 Batch 4:  Loss: 2.203 Accuracy: 0.597\n",
      "Epoch 251, CIFAR-10 Batch 5:  Loss: 2.221 Accuracy: 0.603\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss: 2.444 Accuracy: 0.590\n",
      "Epoch 252, CIFAR-10 Batch 2:  Loss: 2.346 Accuracy: 0.594\n",
      "Epoch 252, CIFAR-10 Batch 3:  Loss: 2.399 Accuracy: 0.595\n",
      "Epoch 252, CIFAR-10 Batch 4:  Loss: 2.202 Accuracy: 0.603\n",
      "Epoch 252, CIFAR-10 Batch 5:  Loss: 2.225 Accuracy: 0.603\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss: 2.408 Accuracy: 0.594\n",
      "Epoch 253, CIFAR-10 Batch 2:  Loss: 2.365 Accuracy: 0.589\n",
      "Epoch 253, CIFAR-10 Batch 3:  Loss: 2.492 Accuracy: 0.591\n",
      "Epoch 253, CIFAR-10 Batch 4:  Loss: 2.223 Accuracy: 0.597\n",
      "Epoch 253, CIFAR-10 Batch 5:  Loss: 2.263 Accuracy: 0.601\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss: 2.404 Accuracy: 0.592\n",
      "Epoch 254, CIFAR-10 Batch 2:  Loss: 2.364 Accuracy: 0.588\n",
      "Epoch 254, CIFAR-10 Batch 3:  Loss: 2.517 Accuracy: 0.593\n",
      "Epoch 254, CIFAR-10 Batch 4:  Loss: 2.246 Accuracy: 0.607\n",
      "Epoch 254, CIFAR-10 Batch 5:  Loss: 2.328 Accuracy: 0.599\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss: 2.523 Accuracy: 0.590\n",
      "Epoch 255, CIFAR-10 Batch 2:  Loss: 2.445 Accuracy: 0.587\n",
      "Epoch 255, CIFAR-10 Batch 3:  Loss: 2.526 Accuracy: 0.591\n",
      "Epoch 255, CIFAR-10 Batch 4:  Loss: 2.276 Accuracy: 0.600\n",
      "Epoch 255, CIFAR-10 Batch 5:  Loss: 2.315 Accuracy: 0.598\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss: 2.434 Accuracy: 0.596\n",
      "Epoch 256, CIFAR-10 Batch 2:  Loss: 2.595 Accuracy: 0.583\n",
      "Epoch 256, CIFAR-10 Batch 3:  Loss: 2.572 Accuracy: 0.589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256, CIFAR-10 Batch 4:  Loss: 2.286 Accuracy: 0.602\n",
      "Epoch 256, CIFAR-10 Batch 5:  Loss: 2.250 Accuracy: 0.604\n",
      "Epoch 257, CIFAR-10 Batch 1:  Loss: 2.406 Accuracy: 0.596\n",
      "Epoch 257, CIFAR-10 Batch 2:  Loss: 2.484 Accuracy: 0.589\n",
      "Epoch 257, CIFAR-10 Batch 3:  Loss: 2.403 Accuracy: 0.597\n",
      "Epoch 257, CIFAR-10 Batch 4:  Loss: 2.311 Accuracy: 0.603\n",
      "Epoch 257, CIFAR-10 Batch 5:  Loss: 2.265 Accuracy: 0.608\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss: 2.519 Accuracy: 0.592\n",
      "Epoch 258, CIFAR-10 Batch 2:  Loss: 2.584 Accuracy: 0.583\n",
      "Epoch 258, CIFAR-10 Batch 3:  Loss: 2.476 Accuracy: 0.601\n",
      "Epoch 258, CIFAR-10 Batch 4:  Loss: 2.293 Accuracy: 0.596\n",
      "Epoch 258, CIFAR-10 Batch 5:  Loss: 2.303 Accuracy: 0.601\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss: 2.502 Accuracy: 0.589\n",
      "Epoch 259, CIFAR-10 Batch 2:  Loss: 2.458 Accuracy: 0.583\n",
      "Epoch 259, CIFAR-10 Batch 3:  Loss: 2.581 Accuracy: 0.596\n",
      "Epoch 259, CIFAR-10 Batch 4:  Loss: 2.277 Accuracy: 0.604\n",
      "Epoch 259, CIFAR-10 Batch 5:  Loss: 2.285 Accuracy: 0.605\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss: 2.564 Accuracy: 0.591\n",
      "Epoch 260, CIFAR-10 Batch 2:  Loss: 2.559 Accuracy: 0.586\n",
      "Epoch 260, CIFAR-10 Batch 3:  Loss: 2.533 Accuracy: 0.595\n",
      "Epoch 260, CIFAR-10 Batch 4:  Loss: 2.280 Accuracy: 0.601\n",
      "Epoch 260, CIFAR-10 Batch 5:  Loss: 2.365 Accuracy: 0.602\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss: 2.436 Accuracy: 0.595\n",
      "Epoch 261, CIFAR-10 Batch 2:  Loss: 2.486 Accuracy: 0.595\n",
      "Epoch 261, CIFAR-10 Batch 3:  Loss: 2.605 Accuracy: 0.588\n",
      "Epoch 261, CIFAR-10 Batch 4:  Loss: 2.342 Accuracy: 0.598\n",
      "Epoch 261, CIFAR-10 Batch 5:  Loss: 2.459 Accuracy: 0.600\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss: 2.501 Accuracy: 0.585\n",
      "Epoch 262, CIFAR-10 Batch 2:  Loss: 2.523 Accuracy: 0.585\n",
      "Epoch 262, CIFAR-10 Batch 3:  Loss: 2.631 Accuracy: 0.589\n",
      "Epoch 262, CIFAR-10 Batch 4:  Loss: 2.436 Accuracy: 0.594\n",
      "Epoch 262, CIFAR-10 Batch 5:  Loss: 2.364 Accuracy: 0.602\n",
      "Epoch 263, CIFAR-10 Batch 1:  Loss: 2.460 Accuracy: 0.596\n",
      "Epoch 263, CIFAR-10 Batch 2:  Loss: 2.605 Accuracy: 0.592\n",
      "Epoch 263, CIFAR-10 Batch 3:  Loss: 2.706 Accuracy: 0.594\n",
      "Epoch 263, CIFAR-10 Batch 4:  Loss: 2.324 Accuracy: 0.602\n",
      "Epoch 263, CIFAR-10 Batch 5:  Loss: 2.404 Accuracy: 0.601\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss: 2.680 Accuracy: 0.588\n",
      "Epoch 264, CIFAR-10 Batch 2:  Loss: 2.563 Accuracy: 0.590\n",
      "Epoch 264, CIFAR-10 Batch 3:  Loss: 2.741 Accuracy: 0.591\n",
      "Epoch 264, CIFAR-10 Batch 4:  Loss: 2.373 Accuracy: 0.597\n",
      "Epoch 264, CIFAR-10 Batch 5:  Loss: 2.315 Accuracy: 0.598\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss: 2.577 Accuracy: 0.594\n",
      "Epoch 265, CIFAR-10 Batch 2:  Loss: 2.667 Accuracy: 0.588\n",
      "Epoch 265, CIFAR-10 Batch 3:  Loss: 2.541 Accuracy: 0.594\n",
      "Epoch 265, CIFAR-10 Batch 4:  Loss: 2.434 Accuracy: 0.595\n",
      "Epoch 265, CIFAR-10 Batch 5:  Loss: 2.436 Accuracy: 0.599\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss: 2.696 Accuracy: 0.592\n",
      "Epoch 266, CIFAR-10 Batch 2:  Loss: 2.561 Accuracy: 0.583\n",
      "Epoch 266, CIFAR-10 Batch 3:  Loss: 2.517 Accuracy: 0.598\n",
      "Epoch 266, CIFAR-10 Batch 4:  Loss: 2.400 Accuracy: 0.597\n",
      "Epoch 266, CIFAR-10 Batch 5:  Loss: 2.490 Accuracy: 0.602\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss: 2.718 Accuracy: 0.584\n",
      "Epoch 267, CIFAR-10 Batch 2:  Loss: 2.670 Accuracy: 0.590\n",
      "Epoch 267, CIFAR-10 Batch 3:  Loss: 2.604 Accuracy: 0.594\n",
      "Epoch 267, CIFAR-10 Batch 4:  Loss: 2.453 Accuracy: 0.592\n",
      "Epoch 267, CIFAR-10 Batch 5:  Loss: 2.431 Accuracy: 0.607\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss: 2.713 Accuracy: 0.585\n",
      "Epoch 268, CIFAR-10 Batch 2:  Loss: 2.741 Accuracy: 0.585\n",
      "Epoch 268, CIFAR-10 Batch 3:  Loss: 2.550 Accuracy: 0.590\n",
      "Epoch 268, CIFAR-10 Batch 4:  Loss: 2.478 Accuracy: 0.591\n",
      "Epoch 268, CIFAR-10 Batch 5:  Loss: 2.385 Accuracy: 0.602\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss: 2.699 Accuracy: 0.588\n",
      "Epoch 269, CIFAR-10 Batch 2:  Loss: 2.733 Accuracy: 0.588\n",
      "Epoch 269, CIFAR-10 Batch 3:  Loss: 2.626 Accuracy: 0.599\n",
      "Epoch 269, CIFAR-10 Batch 4:  Loss: 2.413 Accuracy: 0.596\n",
      "Epoch 269, CIFAR-10 Batch 5:  Loss: 2.307 Accuracy: 0.602\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss: 2.653 Accuracy: 0.585\n",
      "Epoch 270, CIFAR-10 Batch 2:  Loss: 2.841 Accuracy: 0.578\n",
      "Epoch 270, CIFAR-10 Batch 3:  Loss: 2.560 Accuracy: 0.595\n",
      "Epoch 270, CIFAR-10 Batch 4:  Loss: 2.382 Accuracy: 0.592\n",
      "Epoch 270, CIFAR-10 Batch 5:  Loss: 2.493 Accuracy: 0.603\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss: 2.611 Accuracy: 0.594\n",
      "Epoch 271, CIFAR-10 Batch 2:  Loss: 2.816 Accuracy: 0.580\n",
      "Epoch 271, CIFAR-10 Batch 3:  Loss: 2.628 Accuracy: 0.597\n",
      "Epoch 271, CIFAR-10 Batch 4:  Loss: 2.544 Accuracy: 0.584\n",
      "Epoch 271, CIFAR-10 Batch 5:  Loss: 2.428 Accuracy: 0.601\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss: 2.612 Accuracy: 0.594\n",
      "Epoch 272, CIFAR-10 Batch 2:  Loss: 2.718 Accuracy: 0.585\n",
      "Epoch 272, CIFAR-10 Batch 3:  Loss: 2.658 Accuracy: 0.595\n",
      "Epoch 272, CIFAR-10 Batch 4:  Loss: 2.412 Accuracy: 0.595\n",
      "Epoch 272, CIFAR-10 Batch 5:  Loss: 2.456 Accuracy: 0.598\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss: 2.611 Accuracy: 0.591\n",
      "Epoch 273, CIFAR-10 Batch 2:  Loss: 2.819 Accuracy: 0.578\n",
      "Epoch 273, CIFAR-10 Batch 3:  Loss: 2.625 Accuracy: 0.593\n",
      "Epoch 273, CIFAR-10 Batch 4:  Loss: 2.458 Accuracy: 0.588\n",
      "Epoch 273, CIFAR-10 Batch 5:  Loss: 2.451 Accuracy: 0.601\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss: 2.747 Accuracy: 0.586\n",
      "Epoch 274, CIFAR-10 Batch 2:  Loss: 2.652 Accuracy: 0.591\n",
      "Epoch 274, CIFAR-10 Batch 3:  Loss: 2.526 Accuracy: 0.599\n",
      "Epoch 274, CIFAR-10 Batch 4:  Loss: 2.495 Accuracy: 0.585\n",
      "Epoch 274, CIFAR-10 Batch 5:  Loss: 2.420 Accuracy: 0.598\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss: 2.618 Accuracy: 0.597\n",
      "Epoch 275, CIFAR-10 Batch 2:  Loss: 2.778 Accuracy: 0.577\n",
      "Epoch 275, CIFAR-10 Batch 3:  Loss: 2.595 Accuracy: 0.595\n",
      "Epoch 275, CIFAR-10 Batch 4:  Loss: 2.421 Accuracy: 0.591\n",
      "Epoch 275, CIFAR-10 Batch 5:  Loss: 2.567 Accuracy: 0.597\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss: 2.579 Accuracy: 0.590\n",
      "Epoch 276, CIFAR-10 Batch 2:  Loss: 3.070 Accuracy: 0.581\n",
      "Epoch 276, CIFAR-10 Batch 3:  Loss: 2.604 Accuracy: 0.597\n",
      "Epoch 276, CIFAR-10 Batch 4:  Loss: 2.447 Accuracy: 0.586\n",
      "Epoch 276, CIFAR-10 Batch 5:  Loss: 2.537 Accuracy: 0.602\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss: 2.800 Accuracy: 0.585\n",
      "Epoch 277, CIFAR-10 Batch 2:  Loss: 2.846 Accuracy: 0.578\n",
      "Epoch 277, CIFAR-10 Batch 3:  Loss: 2.548 Accuracy: 0.596\n",
      "Epoch 277, CIFAR-10 Batch 4:  Loss: 2.430 Accuracy: 0.593\n",
      "Epoch 277, CIFAR-10 Batch 5:  Loss: 2.429 Accuracy: 0.603\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss: 2.739 Accuracy: 0.587\n",
      "Epoch 278, CIFAR-10 Batch 2:  Loss: 2.839 Accuracy: 0.594\n",
      "Epoch 278, CIFAR-10 Batch 3:  Loss: 2.562 Accuracy: 0.597\n",
      "Epoch 278, CIFAR-10 Batch 4:  Loss: 2.419 Accuracy: 0.590\n",
      "Epoch 278, CIFAR-10 Batch 5:  Loss: 2.522 Accuracy: 0.606\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss: 2.744 Accuracy: 0.591\n",
      "Epoch 279, CIFAR-10 Batch 2:  Loss: 2.911 Accuracy: 0.579\n",
      "Epoch 279, CIFAR-10 Batch 3:  Loss: 2.621 Accuracy: 0.601\n",
      "Epoch 279, CIFAR-10 Batch 4:  Loss: 2.381 Accuracy: 0.593\n",
      "Epoch 279, CIFAR-10 Batch 5:  Loss: 2.607 Accuracy: 0.598\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss: 2.735 Accuracy: 0.589\n",
      "Epoch 280, CIFAR-10 Batch 2:  Loss: 2.909 Accuracy: 0.585\n",
      "Epoch 280, CIFAR-10 Batch 3:  Loss: 2.597 Accuracy: 0.599\n",
      "Epoch 280, CIFAR-10 Batch 4:  Loss: 2.491 Accuracy: 0.595\n",
      "Epoch 280, CIFAR-10 Batch 5:  Loss: 2.580 Accuracy: 0.598\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss: 2.630 Accuracy: 0.588\n",
      "Epoch 281, CIFAR-10 Batch 2:  Loss: 2.853 Accuracy: 0.584\n",
      "Epoch 281, CIFAR-10 Batch 3:  Loss: 2.751 Accuracy: 0.595\n",
      "Epoch 281, CIFAR-10 Batch 4:  Loss: 2.470 Accuracy: 0.586\n",
      "Epoch 281, CIFAR-10 Batch 5:  Loss: 2.613 Accuracy: 0.597\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss: 2.650 Accuracy: 0.591\n",
      "Epoch 282, CIFAR-10 Batch 2:  Loss: 3.011 Accuracy: 0.580\n",
      "Epoch 282, CIFAR-10 Batch 3:  Loss: 2.637 Accuracy: 0.599\n",
      "Epoch 282, CIFAR-10 Batch 4:  Loss: 2.367 Accuracy: 0.597\n",
      "Epoch 282, CIFAR-10 Batch 5:  Loss: 2.625 Accuracy: 0.600\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss: 2.778 Accuracy: 0.578\n",
      "Epoch 283, CIFAR-10 Batch 2:  Loss: 2.914 Accuracy: 0.584\n",
      "Epoch 283, CIFAR-10 Batch 3:  Loss: 2.607 Accuracy: 0.598\n",
      "Epoch 283, CIFAR-10 Batch 4:  Loss: 2.519 Accuracy: 0.594\n",
      "Epoch 283, CIFAR-10 Batch 5:  Loss: 2.644 Accuracy: 0.593\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss: 2.858 Accuracy: 0.586\n",
      "Epoch 284, CIFAR-10 Batch 2:  Loss: 2.907 Accuracy: 0.585\n",
      "Epoch 284, CIFAR-10 Batch 3:  Loss: 2.661 Accuracy: 0.599\n",
      "Epoch 284, CIFAR-10 Batch 4:  Loss: 2.525 Accuracy: 0.591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284, CIFAR-10 Batch 5:  Loss: 2.652 Accuracy: 0.598\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss: 2.826 Accuracy: 0.577\n",
      "Epoch 285, CIFAR-10 Batch 2:  Loss: 2.978 Accuracy: 0.587\n",
      "Epoch 285, CIFAR-10 Batch 3:  Loss: 2.635 Accuracy: 0.602\n",
      "Epoch 285, CIFAR-10 Batch 4:  Loss: 2.532 Accuracy: 0.596\n",
      "Epoch 285, CIFAR-10 Batch 5:  Loss: 2.766 Accuracy: 0.591\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss: 2.859 Accuracy: 0.583\n",
      "Epoch 286, CIFAR-10 Batch 2:  Loss: 2.920 Accuracy: 0.584\n",
      "Epoch 286, CIFAR-10 Batch 3:  Loss: 2.654 Accuracy: 0.602\n",
      "Epoch 286, CIFAR-10 Batch 4:  Loss: 2.436 Accuracy: 0.601\n",
      "Epoch 286, CIFAR-10 Batch 5:  Loss: 2.653 Accuracy: 0.595\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss: 2.805 Accuracy: 0.587\n",
      "Epoch 287, CIFAR-10 Batch 2:  Loss: 2.767 Accuracy: 0.589\n",
      "Epoch 287, CIFAR-10 Batch 3:  Loss: 2.705 Accuracy: 0.597\n",
      "Epoch 287, CIFAR-10 Batch 4:  Loss: 2.519 Accuracy: 0.593\n",
      "Epoch 287, CIFAR-10 Batch 5:  Loss: 2.613 Accuracy: 0.603\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss: 2.762 Accuracy: 0.586\n",
      "Epoch 288, CIFAR-10 Batch 2:  Loss: 2.914 Accuracy: 0.584\n",
      "Epoch 288, CIFAR-10 Batch 3:  Loss: 2.605 Accuracy: 0.604\n",
      "Epoch 288, CIFAR-10 Batch 4:  Loss: 2.553 Accuracy: 0.593\n",
      "Epoch 288, CIFAR-10 Batch 5:  Loss: 2.608 Accuracy: 0.596\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss: 2.942 Accuracy: 0.578\n",
      "Epoch 289, CIFAR-10 Batch 2:  Loss: 2.763 Accuracy: 0.591\n",
      "Epoch 289, CIFAR-10 Batch 3:  Loss: 2.570 Accuracy: 0.601\n",
      "Epoch 289, CIFAR-10 Batch 4:  Loss: 2.540 Accuracy: 0.592\n",
      "Epoch 289, CIFAR-10 Batch 5:  Loss: 2.722 Accuracy: 0.595\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss: 2.867 Accuracy: 0.584\n",
      "Epoch 290, CIFAR-10 Batch 2:  Loss: 2.926 Accuracy: 0.588\n",
      "Epoch 290, CIFAR-10 Batch 3:  Loss: 2.608 Accuracy: 0.599\n",
      "Epoch 290, CIFAR-10 Batch 4:  Loss: 2.499 Accuracy: 0.594\n",
      "Epoch 290, CIFAR-10 Batch 5:  Loss: 2.673 Accuracy: 0.590\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss: 2.929 Accuracy: 0.587\n",
      "Epoch 291, CIFAR-10 Batch 2:  Loss: 2.962 Accuracy: 0.583\n",
      "Epoch 291, CIFAR-10 Batch 3:  Loss: 2.748 Accuracy: 0.597\n",
      "Epoch 291, CIFAR-10 Batch 4:  Loss: 2.451 Accuracy: 0.589\n",
      "Epoch 291, CIFAR-10 Batch 5:  Loss: 2.730 Accuracy: 0.597\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss: 2.951 Accuracy: 0.584\n",
      "Epoch 292, CIFAR-10 Batch 2:  Loss: 2.983 Accuracy: 0.584\n",
      "Epoch 292, CIFAR-10 Batch 3:  Loss: 2.620 Accuracy: 0.594\n",
      "Epoch 292, CIFAR-10 Batch 4:  Loss: 2.535 Accuracy: 0.593\n",
      "Epoch 292, CIFAR-10 Batch 5:  Loss: 2.729 Accuracy: 0.591\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss: 2.786 Accuracy: 0.591\n",
      "Epoch 293, CIFAR-10 Batch 2:  Loss: 2.871 Accuracy: 0.587\n",
      "Epoch 293, CIFAR-10 Batch 3:  Loss: 2.636 Accuracy: 0.605\n",
      "Epoch 293, CIFAR-10 Batch 4:  Loss: 2.548 Accuracy: 0.596\n",
      "Epoch 293, CIFAR-10 Batch 5:  Loss: 2.730 Accuracy: 0.595\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss: 2.960 Accuracy: 0.587\n",
      "Epoch 294, CIFAR-10 Batch 2:  Loss: 2.833 Accuracy: 0.586\n",
      "Epoch 294, CIFAR-10 Batch 3:  Loss: 2.591 Accuracy: 0.600\n",
      "Epoch 294, CIFAR-10 Batch 4:  Loss: 2.534 Accuracy: 0.601\n",
      "Epoch 294, CIFAR-10 Batch 5:  Loss: 2.719 Accuracy: 0.595\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss: 2.805 Accuracy: 0.588\n",
      "Epoch 295, CIFAR-10 Batch 2:  Loss: 2.903 Accuracy: 0.587\n",
      "Epoch 295, CIFAR-10 Batch 3:  Loss: 2.667 Accuracy: 0.595\n",
      "Epoch 295, CIFAR-10 Batch 4:  Loss: 2.473 Accuracy: 0.602\n",
      "Epoch 295, CIFAR-10 Batch 5:  Loss: 2.793 Accuracy: 0.590\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss: 2.773 Accuracy: 0.594\n",
      "Epoch 296, CIFAR-10 Batch 2:  Loss: 2.992 Accuracy: 0.586\n",
      "Epoch 296, CIFAR-10 Batch 3:  Loss: 2.703 Accuracy: 0.593\n",
      "Epoch 296, CIFAR-10 Batch 4:  Loss: 2.512 Accuracy: 0.597\n",
      "Epoch 296, CIFAR-10 Batch 5:  Loss: 2.706 Accuracy: 0.587\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss: 3.023 Accuracy: 0.587\n",
      "Epoch 297, CIFAR-10 Batch 2:  Loss: 2.929 Accuracy: 0.585\n",
      "Epoch 297, CIFAR-10 Batch 3:  Loss: 2.736 Accuracy: 0.592\n",
      "Epoch 297, CIFAR-10 Batch 4:  Loss: 2.570 Accuracy: 0.600\n",
      "Epoch 297, CIFAR-10 Batch 5:  Loss: 2.822 Accuracy: 0.594\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss: 2.781 Accuracy: 0.593\n",
      "Epoch 298, CIFAR-10 Batch 2:  Loss: 3.025 Accuracy: 0.587\n",
      "Epoch 298, CIFAR-10 Batch 3:  Loss: 2.714 Accuracy: 0.598\n",
      "Epoch 298, CIFAR-10 Batch 4:  Loss: 2.654 Accuracy: 0.597\n",
      "Epoch 298, CIFAR-10 Batch 5:  Loss: 2.805 Accuracy: 0.587\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss: 2.901 Accuracy: 0.589\n",
      "Epoch 299, CIFAR-10 Batch 2:  Loss: 2.989 Accuracy: 0.587\n",
      "Epoch 299, CIFAR-10 Batch 3:  Loss: 2.718 Accuracy: 0.597\n",
      "Epoch 299, CIFAR-10 Batch 4:  Loss: 2.617 Accuracy: 0.600\n",
      "Epoch 299, CIFAR-10 Batch 5:  Loss: 2.823 Accuracy: 0.587\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss: 2.974 Accuracy: 0.588\n",
      "Epoch 300, CIFAR-10 Batch 2:  Loss: 3.006 Accuracy: 0.587\n",
      "Epoch 300, CIFAR-10 Batch 3:  Loss: 2.796 Accuracy: 0.594\n",
      "Epoch 300, CIFAR-10 Batch 4:  Loss: 2.570 Accuracy: 0.603\n",
      "Epoch 300, CIFAR-10 Batch 5:  Loss: 2.841 Accuracy: 0.592\n",
      "Epoch 301, CIFAR-10 Batch 1:  Loss: 2.823 Accuracy: 0.595\n",
      "Epoch 301, CIFAR-10 Batch 2:  Loss: 2.979 Accuracy: 0.590\n",
      "Epoch 301, CIFAR-10 Batch 3:  Loss: 2.696 Accuracy: 0.599\n",
      "Epoch 301, CIFAR-10 Batch 4:  Loss: 2.495 Accuracy: 0.605\n",
      "Epoch 301, CIFAR-10 Batch 5:  Loss: 2.839 Accuracy: 0.589\n",
      "Epoch 302, CIFAR-10 Batch 1:  Loss: 2.939 Accuracy: 0.593\n",
      "Epoch 302, CIFAR-10 Batch 2:  Loss: 2.952 Accuracy: 0.588\n",
      "Epoch 302, CIFAR-10 Batch 3:  Loss: 2.814 Accuracy: 0.597\n",
      "Epoch 302, CIFAR-10 Batch 4:  Loss: 2.697 Accuracy: 0.589\n",
      "Epoch 302, CIFAR-10 Batch 5:  Loss: 2.908 Accuracy: 0.587\n",
      "Epoch 303, CIFAR-10 Batch 1:  Loss: 3.045 Accuracy: 0.580\n",
      "Epoch 303, CIFAR-10 Batch 2:  Loss: 2.909 Accuracy: 0.596\n",
      "Epoch 303, CIFAR-10 Batch 3:  Loss: 2.649 Accuracy: 0.599\n",
      "Epoch 303, CIFAR-10 Batch 4:  Loss: 2.639 Accuracy: 0.601\n",
      "Epoch 303, CIFAR-10 Batch 5:  Loss: 2.776 Accuracy: 0.592\n",
      "Epoch 304, CIFAR-10 Batch 1:  Loss: 2.887 Accuracy: 0.589\n",
      "Epoch 304, CIFAR-10 Batch 2:  Loss: 3.034 Accuracy: 0.592\n",
      "Epoch 304, CIFAR-10 Batch 3:  Loss: 2.780 Accuracy: 0.599\n",
      "Epoch 304, CIFAR-10 Batch 4:  Loss: 2.642 Accuracy: 0.601\n",
      "Epoch 304, CIFAR-10 Batch 5:  Loss: 2.910 Accuracy: 0.593\n",
      "Epoch 305, CIFAR-10 Batch 1:  Loss: 2.893 Accuracy: 0.589\n",
      "Epoch 305, CIFAR-10 Batch 2:  Loss: 3.124 Accuracy: 0.586\n",
      "Epoch 305, CIFAR-10 Batch 3:  Loss: 2.736 Accuracy: 0.601\n",
      "Epoch 305, CIFAR-10 Batch 4:  Loss: 2.500 Accuracy: 0.603\n",
      "Epoch 305, CIFAR-10 Batch 5:  Loss: 2.899 Accuracy: 0.594\n",
      "Epoch 306, CIFAR-10 Batch 1:  Loss: 2.903 Accuracy: 0.594\n",
      "Epoch 306, CIFAR-10 Batch 2:  Loss: 2.975 Accuracy: 0.590\n",
      "Epoch 306, CIFAR-10 Batch 3:  Loss: 2.656 Accuracy: 0.597\n",
      "Epoch 306, CIFAR-10 Batch 4:  Loss: 2.672 Accuracy: 0.599\n",
      "Epoch 306, CIFAR-10 Batch 5:  Loss: 2.826 Accuracy: 0.588\n",
      "Epoch 307, CIFAR-10 Batch 1:  Loss: 2.710 Accuracy: 0.598\n",
      "Epoch 307, CIFAR-10 Batch 2:  Loss: 3.084 Accuracy: 0.591\n",
      "Epoch 307, CIFAR-10 Batch 3:  Loss: 2.723 Accuracy: 0.599\n",
      "Epoch 307, CIFAR-10 Batch 4:  Loss: 2.581 Accuracy: 0.596\n",
      "Epoch 307, CIFAR-10 Batch 5:  Loss: 2.911 Accuracy: 0.585\n",
      "Epoch 308, CIFAR-10 Batch 1:  Loss: 2.891 Accuracy: 0.600\n",
      "Epoch 308, CIFAR-10 Batch 2:  Loss: 3.063 Accuracy: 0.587\n",
      "Epoch 308, CIFAR-10 Batch 3:  Loss: 2.750 Accuracy: 0.597\n",
      "Epoch 308, CIFAR-10 Batch 4:  Loss: 2.671 Accuracy: 0.602\n",
      "Epoch 308, CIFAR-10 Batch 5:  Loss: 2.887 Accuracy: 0.588\n",
      "Epoch 309, CIFAR-10 Batch 1:  Loss: 2.850 Accuracy: 0.598\n",
      "Epoch 309, CIFAR-10 Batch 2:  Loss: 2.959 Accuracy: 0.595\n",
      "Epoch 309, CIFAR-10 Batch 3:  Loss: 2.876 Accuracy: 0.598\n",
      "Epoch 309, CIFAR-10 Batch 4:  Loss: 2.659 Accuracy: 0.604\n",
      "Epoch 309, CIFAR-10 Batch 5:  Loss: 3.040 Accuracy: 0.590\n",
      "Epoch 310, CIFAR-10 Batch 1:  Loss: 2.900 Accuracy: 0.598\n",
      "Epoch 310, CIFAR-10 Batch 2:  Loss: 2.924 Accuracy: 0.596\n",
      "Epoch 310, CIFAR-10 Batch 3:  Loss: 2.895 Accuracy: 0.599\n",
      "Epoch 310, CIFAR-10 Batch 4:  Loss: 2.643 Accuracy: 0.596\n",
      "Epoch 310, CIFAR-10 Batch 5:  Loss: 3.021 Accuracy: 0.591\n",
      "Epoch 311, CIFAR-10 Batch 1:  Loss: 2.840 Accuracy: 0.600\n",
      "Epoch 311, CIFAR-10 Batch 2:  Loss: 3.137 Accuracy: 0.591\n",
      "Epoch 311, CIFAR-10 Batch 3:  Loss: 2.736 Accuracy: 0.597\n",
      "Epoch 311, CIFAR-10 Batch 4:  Loss: 2.735 Accuracy: 0.597\n",
      "Epoch 311, CIFAR-10 Batch 5:  Loss: 2.959 Accuracy: 0.591\n",
      "Epoch 312, CIFAR-10 Batch 1:  Loss: 2.959 Accuracy: 0.590\n",
      "Epoch 312, CIFAR-10 Batch 2:  Loss: 2.985 Accuracy: 0.597\n",
      "Epoch 312, CIFAR-10 Batch 3:  Loss: 2.873 Accuracy: 0.597\n",
      "Epoch 312, CIFAR-10 Batch 4:  Loss: 2.588 Accuracy: 0.596\n",
      "Epoch 312, CIFAR-10 Batch 5:  Loss: 2.892 Accuracy: 0.590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313, CIFAR-10 Batch 1:  Loss: 2.991 Accuracy: 0.592\n",
      "Epoch 313, CIFAR-10 Batch 2:  Loss: 2.960 Accuracy: 0.595\n",
      "Epoch 313, CIFAR-10 Batch 3:  Loss: 2.729 Accuracy: 0.598\n",
      "Epoch 313, CIFAR-10 Batch 4:  Loss: 2.668 Accuracy: 0.602\n",
      "Epoch 313, CIFAR-10 Batch 5:  Loss: 3.055 Accuracy: 0.585\n",
      "Epoch 314, CIFAR-10 Batch 1:  Loss: 2.870 Accuracy: 0.593\n",
      "Epoch 314, CIFAR-10 Batch 2:  Loss: 3.095 Accuracy: 0.589\n",
      "Epoch 314, CIFAR-10 Batch 3:  Loss: 2.858 Accuracy: 0.596\n",
      "Epoch 314, CIFAR-10 Batch 4:  Loss: 2.778 Accuracy: 0.599\n",
      "Epoch 314, CIFAR-10 Batch 5:  Loss: 3.001 Accuracy: 0.582\n",
      "Epoch 315, CIFAR-10 Batch 1:  Loss: 2.886 Accuracy: 0.594\n",
      "Epoch 315, CIFAR-10 Batch 2:  Loss: 3.042 Accuracy: 0.597\n",
      "Epoch 315, CIFAR-10 Batch 3:  Loss: 2.868 Accuracy: 0.599\n",
      "Epoch 315, CIFAR-10 Batch 4:  Loss: 2.626 Accuracy: 0.596\n",
      "Epoch 315, CIFAR-10 Batch 5:  Loss: 2.970 Accuracy: 0.584\n",
      "Epoch 316, CIFAR-10 Batch 1:  Loss: 2.810 Accuracy: 0.599\n",
      "Epoch 316, CIFAR-10 Batch 2:  Loss: 3.073 Accuracy: 0.595\n",
      "Epoch 316, CIFAR-10 Batch 3:  Loss: 2.821 Accuracy: 0.598\n",
      "Epoch 316, CIFAR-10 Batch 4:  Loss: 2.780 Accuracy: 0.603\n",
      "Epoch 316, CIFAR-10 Batch 5:  Loss: 3.095 Accuracy: 0.586\n",
      "Epoch 317, CIFAR-10 Batch 1:  Loss: 2.830 Accuracy: 0.597\n",
      "Epoch 317, CIFAR-10 Batch 2:  Loss: 3.083 Accuracy: 0.596\n",
      "Epoch 317, CIFAR-10 Batch 3:  Loss: 2.900 Accuracy: 0.597\n",
      "Epoch 317, CIFAR-10 Batch 4:  Loss: 2.816 Accuracy: 0.598\n",
      "Epoch 317, CIFAR-10 Batch 5:  Loss: 3.002 Accuracy: 0.588\n",
      "Epoch 318, CIFAR-10 Batch 1:  Loss: 2.926 Accuracy: 0.601\n",
      "Epoch 318, CIFAR-10 Batch 2:  Loss: 3.069 Accuracy: 0.596\n",
      "Epoch 318, CIFAR-10 Batch 3:  Loss: 2.836 Accuracy: 0.592\n",
      "Epoch 318, CIFAR-10 Batch 4:  Loss: 2.955 Accuracy: 0.593\n",
      "Epoch 318, CIFAR-10 Batch 5:  Loss: 3.163 Accuracy: 0.583\n",
      "Epoch 319, CIFAR-10 Batch 1:  Loss: 2.823 Accuracy: 0.601\n",
      "Epoch 319, CIFAR-10 Batch 2:  Loss: 3.037 Accuracy: 0.598\n",
      "Epoch 319, CIFAR-10 Batch 3:  Loss: 2.897 Accuracy: 0.603\n",
      "Epoch 319, CIFAR-10 Batch 4:  Loss: 2.671 Accuracy: 0.606\n",
      "Epoch 319, CIFAR-10 Batch 5:  Loss: 2.943 Accuracy: 0.586\n",
      "Epoch 320, CIFAR-10 Batch 1:  Loss: 2.785 Accuracy: 0.603\n",
      "Epoch 320, CIFAR-10 Batch 2:  Loss: 2.987 Accuracy: 0.599\n",
      "Epoch 320, CIFAR-10 Batch 3:  Loss: 2.943 Accuracy: 0.597\n",
      "Epoch 320, CIFAR-10 Batch 4:  Loss: 2.741 Accuracy: 0.604\n",
      "Epoch 320, CIFAR-10 Batch 5:  Loss: 3.017 Accuracy: 0.589\n",
      "Epoch 321, CIFAR-10 Batch 1:  Loss: 2.987 Accuracy: 0.600\n",
      "Epoch 321, CIFAR-10 Batch 2:  Loss: 3.014 Accuracy: 0.599\n",
      "Epoch 321, CIFAR-10 Batch 3:  Loss: 2.861 Accuracy: 0.598\n",
      "Epoch 321, CIFAR-10 Batch 4:  Loss: 2.861 Accuracy: 0.594\n",
      "Epoch 321, CIFAR-10 Batch 5:  Loss: 3.118 Accuracy: 0.591\n",
      "Epoch 322, CIFAR-10 Batch 1:  Loss: 2.913 Accuracy: 0.599\n",
      "Epoch 322, CIFAR-10 Batch 2:  Loss: 3.040 Accuracy: 0.595\n",
      "Epoch 322, CIFAR-10 Batch 3:  Loss: 2.907 Accuracy: 0.594\n",
      "Epoch 322, CIFAR-10 Batch 4:  Loss: 2.830 Accuracy: 0.604\n",
      "Epoch 322, CIFAR-10 Batch 5:  Loss: 3.107 Accuracy: 0.588\n",
      "Epoch 323, CIFAR-10 Batch 1:  Loss: 3.008 Accuracy: 0.598\n",
      "Epoch 323, CIFAR-10 Batch 2:  Loss: 3.243 Accuracy: 0.589\n",
      "Epoch 323, CIFAR-10 Batch 3:  Loss: 2.866 Accuracy: 0.600\n",
      "Epoch 323, CIFAR-10 Batch 4:  Loss: 2.908 Accuracy: 0.599\n",
      "Epoch 323, CIFAR-10 Batch 5:  Loss: 2.989 Accuracy: 0.591\n",
      "Epoch 324, CIFAR-10 Batch 1:  Loss: 2.865 Accuracy: 0.599\n",
      "Epoch 324, CIFAR-10 Batch 2:  Loss: 3.021 Accuracy: 0.600\n",
      "Epoch 324, CIFAR-10 Batch 3:  Loss: 2.913 Accuracy: 0.602\n",
      "Epoch 324, CIFAR-10 Batch 4:  Loss: 2.765 Accuracy: 0.599\n",
      "Epoch 324, CIFAR-10 Batch 5:  Loss: 3.129 Accuracy: 0.582\n",
      "Epoch 325, CIFAR-10 Batch 1:  Loss: 2.913 Accuracy: 0.595\n",
      "Epoch 325, CIFAR-10 Batch 2:  Loss: 3.033 Accuracy: 0.595\n",
      "Epoch 325, CIFAR-10 Batch 3:  Loss: 2.977 Accuracy: 0.595\n",
      "Epoch 325, CIFAR-10 Batch 4:  Loss: 2.899 Accuracy: 0.592\n",
      "Epoch 325, CIFAR-10 Batch 5:  Loss: 3.088 Accuracy: 0.588\n",
      "Epoch 326, CIFAR-10 Batch 1:  Loss: 2.897 Accuracy: 0.599\n",
      "Epoch 326, CIFAR-10 Batch 2:  Loss: 3.241 Accuracy: 0.590\n",
      "Epoch 326, CIFAR-10 Batch 3:  Loss: 2.934 Accuracy: 0.598\n",
      "Epoch 326, CIFAR-10 Batch 4:  Loss: 2.731 Accuracy: 0.604\n",
      "Epoch 326, CIFAR-10 Batch 5:  Loss: 3.046 Accuracy: 0.590\n",
      "Epoch 327, CIFAR-10 Batch 1:  Loss: 3.040 Accuracy: 0.595\n",
      "Epoch 327, CIFAR-10 Batch 2:  Loss: 3.099 Accuracy: 0.588\n",
      "Epoch 327, CIFAR-10 Batch 3:  Loss: 3.036 Accuracy: 0.597\n",
      "Epoch 327, CIFAR-10 Batch 4:  Loss: 2.858 Accuracy: 0.594\n",
      "Epoch 327, CIFAR-10 Batch 5:  Loss: 3.133 Accuracy: 0.586\n",
      "Epoch 328, CIFAR-10 Batch 1:  Loss: 2.992 Accuracy: 0.594\n",
      "Epoch 328, CIFAR-10 Batch 2:  Loss: 3.195 Accuracy: 0.587\n",
      "Epoch 328, CIFAR-10 Batch 3:  Loss: 2.936 Accuracy: 0.599\n",
      "Epoch 328, CIFAR-10 Batch 4:  Loss: 2.820 Accuracy: 0.600\n",
      "Epoch 328, CIFAR-10 Batch 5:  Loss: 2.943 Accuracy: 0.595\n",
      "Epoch 329, CIFAR-10 Batch 1:  Loss: 2.942 Accuracy: 0.600\n",
      "Epoch 329, CIFAR-10 Batch 2:  Loss: 3.189 Accuracy: 0.587\n",
      "Epoch 329, CIFAR-10 Batch 3:  Loss: 2.928 Accuracy: 0.599\n",
      "Epoch 329, CIFAR-10 Batch 4:  Loss: 2.895 Accuracy: 0.602\n",
      "Epoch 329, CIFAR-10 Batch 5:  Loss: 3.092 Accuracy: 0.593\n",
      "Epoch 330, CIFAR-10 Batch 1:  Loss: 3.083 Accuracy: 0.598\n",
      "Epoch 330, CIFAR-10 Batch 2:  Loss: 3.366 Accuracy: 0.584\n",
      "Epoch 330, CIFAR-10 Batch 3:  Loss: 2.989 Accuracy: 0.600\n",
      "Epoch 330, CIFAR-10 Batch 4:  Loss: 2.793 Accuracy: 0.599\n",
      "Epoch 330, CIFAR-10 Batch 5:  Loss: 3.105 Accuracy: 0.591\n",
      "Epoch 331, CIFAR-10 Batch 1:  Loss: 2.971 Accuracy: 0.603\n",
      "Epoch 331, CIFAR-10 Batch 2:  Loss: 3.181 Accuracy: 0.589\n",
      "Epoch 331, CIFAR-10 Batch 3:  Loss: 2.932 Accuracy: 0.602\n",
      "Epoch 331, CIFAR-10 Batch 4:  Loss: 2.924 Accuracy: 0.599\n",
      "Epoch 331, CIFAR-10 Batch 5:  Loss: 3.056 Accuracy: 0.594\n",
      "Epoch 332, CIFAR-10 Batch 1:  Loss: 3.053 Accuracy: 0.598\n",
      "Epoch 332, CIFAR-10 Batch 2:  Loss: 3.342 Accuracy: 0.589\n",
      "Epoch 332, CIFAR-10 Batch 3:  Loss: 2.976 Accuracy: 0.603\n",
      "Epoch 332, CIFAR-10 Batch 4:  Loss: 3.018 Accuracy: 0.592\n",
      "Epoch 332, CIFAR-10 Batch 5:  Loss: 3.040 Accuracy: 0.595\n",
      "Epoch 333, CIFAR-10 Batch 1:  Loss: 2.981 Accuracy: 0.602\n",
      "Epoch 333, CIFAR-10 Batch 2:  Loss: 3.132 Accuracy: 0.595\n",
      "Epoch 333, CIFAR-10 Batch 3:  Loss: 2.875 Accuracy: 0.597\n",
      "Epoch 333, CIFAR-10 Batch 4:  Loss: 2.879 Accuracy: 0.597\n",
      "Epoch 333, CIFAR-10 Batch 5:  Loss: 3.013 Accuracy: 0.594\n",
      "Epoch 334, CIFAR-10 Batch 1:  Loss: 3.000 Accuracy: 0.601\n",
      "Epoch 334, CIFAR-10 Batch 2:  Loss: 3.284 Accuracy: 0.591\n",
      "Epoch 334, CIFAR-10 Batch 3:  Loss: 3.067 Accuracy: 0.598\n",
      "Epoch 334, CIFAR-10 Batch 4:  Loss: 2.923 Accuracy: 0.593\n",
      "Epoch 334, CIFAR-10 Batch 5:  Loss: 3.333 Accuracy: 0.588\n",
      "Epoch 335, CIFAR-10 Batch 1:  Loss: 2.955 Accuracy: 0.600\n",
      "Epoch 335, CIFAR-10 Batch 2:  Loss: 3.254 Accuracy: 0.592\n",
      "Epoch 335, CIFAR-10 Batch 3:  Loss: 3.045 Accuracy: 0.595\n",
      "Epoch 335, CIFAR-10 Batch 4:  Loss: 2.980 Accuracy: 0.596\n",
      "Epoch 335, CIFAR-10 Batch 5:  Loss: 3.185 Accuracy: 0.591\n",
      "Epoch 336, CIFAR-10 Batch 1:  Loss: 2.992 Accuracy: 0.599\n",
      "Epoch 336, CIFAR-10 Batch 2:  Loss: 3.329 Accuracy: 0.590\n",
      "Epoch 336, CIFAR-10 Batch 3:  Loss: 2.937 Accuracy: 0.603\n",
      "Epoch 336, CIFAR-10 Batch 4:  Loss: 2.975 Accuracy: 0.592\n",
      "Epoch 336, CIFAR-10 Batch 5:  Loss: 3.190 Accuracy: 0.588\n",
      "Epoch 337, CIFAR-10 Batch 1:  Loss: 3.069 Accuracy: 0.600\n",
      "Epoch 337, CIFAR-10 Batch 2:  Loss: 3.229 Accuracy: 0.592\n",
      "Epoch 337, CIFAR-10 Batch 3:  Loss: 2.990 Accuracy: 0.605\n",
      "Epoch 337, CIFAR-10 Batch 4:  Loss: 2.891 Accuracy: 0.595\n",
      "Epoch 337, CIFAR-10 Batch 5:  Loss: 3.164 Accuracy: 0.593\n",
      "Epoch 338, CIFAR-10 Batch 1:  Loss: 2.989 Accuracy: 0.600\n",
      "Epoch 338, CIFAR-10 Batch 2:  Loss: 3.448 Accuracy: 0.588\n",
      "Epoch 338, CIFAR-10 Batch 3:  Loss: 3.019 Accuracy: 0.601\n",
      "Epoch 338, CIFAR-10 Batch 4:  Loss: 2.953 Accuracy: 0.589\n",
      "Epoch 338, CIFAR-10 Batch 5:  Loss: 3.119 Accuracy: 0.596\n",
      "Epoch 339, CIFAR-10 Batch 1:  Loss: 3.023 Accuracy: 0.597\n",
      "Epoch 339, CIFAR-10 Batch 2:  Loss: 3.347 Accuracy: 0.588\n",
      "Epoch 339, CIFAR-10 Batch 3:  Loss: 3.100 Accuracy: 0.598\n",
      "Epoch 339, CIFAR-10 Batch 4:  Loss: 3.039 Accuracy: 0.591\n",
      "Epoch 339, CIFAR-10 Batch 5:  Loss: 3.014 Accuracy: 0.597\n",
      "Epoch 340, CIFAR-10 Batch 1:  Loss: 3.132 Accuracy: 0.599\n",
      "Epoch 340, CIFAR-10 Batch 2:  Loss: 3.492 Accuracy: 0.593\n",
      "Epoch 340, CIFAR-10 Batch 3:  Loss: 3.070 Accuracy: 0.599\n",
      "Epoch 340, CIFAR-10 Batch 4:  Loss: 3.081 Accuracy: 0.590\n",
      "Epoch 340, CIFAR-10 Batch 5:  Loss: 3.076 Accuracy: 0.592\n",
      "Epoch 341, CIFAR-10 Batch 1:  Loss: 2.993 Accuracy: 0.600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341, CIFAR-10 Batch 2:  Loss: 3.397 Accuracy: 0.590\n",
      "Epoch 341, CIFAR-10 Batch 3:  Loss: 2.980 Accuracy: 0.597\n",
      "Epoch 341, CIFAR-10 Batch 4:  Loss: 3.015 Accuracy: 0.593\n",
      "Epoch 341, CIFAR-10 Batch 5:  Loss: 3.152 Accuracy: 0.592\n",
      "Epoch 342, CIFAR-10 Batch 1:  Loss: 3.104 Accuracy: 0.606\n",
      "Epoch 342, CIFAR-10 Batch 2:  Loss: 3.305 Accuracy: 0.591\n",
      "Epoch 342, CIFAR-10 Batch 3:  Loss: 3.083 Accuracy: 0.601\n",
      "Epoch 342, CIFAR-10 Batch 4:  Loss: 3.036 Accuracy: 0.595\n",
      "Epoch 342, CIFAR-10 Batch 5:  Loss: 3.292 Accuracy: 0.591\n",
      "Epoch 343, CIFAR-10 Batch 1:  Loss: 3.123 Accuracy: 0.601\n",
      "Epoch 343, CIFAR-10 Batch 2:  Loss: 3.449 Accuracy: 0.588\n",
      "Epoch 343, CIFAR-10 Batch 3:  Loss: 3.059 Accuracy: 0.602\n",
      "Epoch 343, CIFAR-10 Batch 4:  Loss: 2.911 Accuracy: 0.600\n",
      "Epoch 343, CIFAR-10 Batch 5:  Loss: 3.210 Accuracy: 0.600\n",
      "Epoch 344, CIFAR-10 Batch 1:  Loss: 3.169 Accuracy: 0.600\n",
      "Epoch 344, CIFAR-10 Batch 2:  Loss: 3.493 Accuracy: 0.582\n",
      "Epoch 344, CIFAR-10 Batch 3:  Loss: 3.070 Accuracy: 0.599\n",
      "Epoch 344, CIFAR-10 Batch 4:  Loss: 2.955 Accuracy: 0.596\n",
      "Epoch 344, CIFAR-10 Batch 5:  Loss: 3.234 Accuracy: 0.592\n",
      "Epoch 345, CIFAR-10 Batch 1:  Loss: 3.207 Accuracy: 0.595\n",
      "Epoch 345, CIFAR-10 Batch 2:  Loss: 3.559 Accuracy: 0.585\n",
      "Epoch 345, CIFAR-10 Batch 3:  Loss: 3.145 Accuracy: 0.600\n",
      "Epoch 345, CIFAR-10 Batch 4:  Loss: 3.106 Accuracy: 0.594\n",
      "Epoch 345, CIFAR-10 Batch 5:  Loss: 3.301 Accuracy: 0.589\n",
      "Epoch 346, CIFAR-10 Batch 1:  Loss: 3.284 Accuracy: 0.598\n",
      "Epoch 346, CIFAR-10 Batch 2:  Loss: 3.649 Accuracy: 0.583\n",
      "Epoch 346, CIFAR-10 Batch 3:  Loss: 2.993 Accuracy: 0.601\n",
      "Epoch 346, CIFAR-10 Batch 4:  Loss: 3.089 Accuracy: 0.594\n",
      "Epoch 346, CIFAR-10 Batch 5:  Loss: 3.254 Accuracy: 0.598\n",
      "Epoch 347, CIFAR-10 Batch 1:  Loss: 3.183 Accuracy: 0.596\n",
      "Epoch 347, CIFAR-10 Batch 2:  Loss: 3.686 Accuracy: 0.587\n",
      "Epoch 347, CIFAR-10 Batch 3:  Loss: 3.095 Accuracy: 0.597\n",
      "Epoch 347, CIFAR-10 Batch 4:  Loss: 3.023 Accuracy: 0.591\n",
      "Epoch 347, CIFAR-10 Batch 5:  Loss: 3.249 Accuracy: 0.597\n",
      "Epoch 348, CIFAR-10 Batch 1:  Loss: 3.171 Accuracy: 0.599\n",
      "Epoch 348, CIFAR-10 Batch 2:  Loss: 3.439 Accuracy: 0.596\n",
      "Epoch 348, CIFAR-10 Batch 3:  Loss: 3.096 Accuracy: 0.599\n",
      "Epoch 348, CIFAR-10 Batch 4:  Loss: 2.989 Accuracy: 0.597\n",
      "Epoch 348, CIFAR-10 Batch 5:  Loss: 3.205 Accuracy: 0.601\n",
      "Epoch 349, CIFAR-10 Batch 1:  Loss: 3.171 Accuracy: 0.596\n",
      "Epoch 349, CIFAR-10 Batch 2:  Loss: 3.625 Accuracy: 0.582\n",
      "Epoch 349, CIFAR-10 Batch 3:  Loss: 3.114 Accuracy: 0.598\n",
      "Epoch 349, CIFAR-10 Batch 4:  Loss: 3.120 Accuracy: 0.596\n",
      "Epoch 349, CIFAR-10 Batch 5:  Loss: 3.294 Accuracy: 0.593\n",
      "Epoch 350, CIFAR-10 Batch 1:  Loss: 3.288 Accuracy: 0.596\n",
      "Epoch 350, CIFAR-10 Batch 2:  Loss: 3.482 Accuracy: 0.586\n",
      "Epoch 350, CIFAR-10 Batch 3:  Loss: 3.198 Accuracy: 0.594\n",
      "Epoch 350, CIFAR-10 Batch 4:  Loss: 3.037 Accuracy: 0.599\n",
      "Epoch 350, CIFAR-10 Batch 5:  Loss: 3.263 Accuracy: 0.593\n",
      "Epoch 351, CIFAR-10 Batch 1:  Loss: 3.268 Accuracy: 0.595\n",
      "Epoch 351, CIFAR-10 Batch 2:  Loss: 3.549 Accuracy: 0.588\n",
      "Epoch 351, CIFAR-10 Batch 3:  Loss: 3.234 Accuracy: 0.597\n",
      "Epoch 351, CIFAR-10 Batch 4:  Loss: 3.106 Accuracy: 0.596\n",
      "Epoch 351, CIFAR-10 Batch 5:  Loss: 3.290 Accuracy: 0.593\n",
      "Epoch 352, CIFAR-10 Batch 1:  Loss: 3.322 Accuracy: 0.597\n",
      "Epoch 352, CIFAR-10 Batch 2:  Loss: 3.723 Accuracy: 0.582\n",
      "Epoch 352, CIFAR-10 Batch 3:  Loss: 3.257 Accuracy: 0.597\n",
      "Epoch 352, CIFAR-10 Batch 4:  Loss: 3.016 Accuracy: 0.598\n",
      "Epoch 352, CIFAR-10 Batch 5:  Loss: 3.299 Accuracy: 0.600\n",
      "Epoch 353, CIFAR-10 Batch 1:  Loss: 3.299 Accuracy: 0.595\n",
      "Epoch 353, CIFAR-10 Batch 2:  Loss: 3.575 Accuracy: 0.591\n",
      "Epoch 353, CIFAR-10 Batch 3:  Loss: 3.260 Accuracy: 0.599\n",
      "Epoch 353, CIFAR-10 Batch 4:  Loss: 3.176 Accuracy: 0.592\n",
      "Epoch 353, CIFAR-10 Batch 5:  Loss: 3.346 Accuracy: 0.593\n",
      "Epoch 354, CIFAR-10 Batch 1:  Loss: 3.424 Accuracy: 0.596\n",
      "Epoch 354, CIFAR-10 Batch 2:  Loss: 3.554 Accuracy: 0.587\n",
      "Epoch 354, CIFAR-10 Batch 3:  Loss: 3.163 Accuracy: 0.600\n",
      "Epoch 354, CIFAR-10 Batch 4:  Loss: 3.170 Accuracy: 0.594\n",
      "Epoch 354, CIFAR-10 Batch 5:  Loss: 3.216 Accuracy: 0.599\n",
      "Epoch 355, CIFAR-10 Batch 1:  Loss: 3.398 Accuracy: 0.594\n",
      "Epoch 355, CIFAR-10 Batch 2:  Loss: 3.722 Accuracy: 0.580\n",
      "Epoch 355, CIFAR-10 Batch 3:  Loss: 3.136 Accuracy: 0.599\n",
      "Epoch 355, CIFAR-10 Batch 4:  Loss: 3.181 Accuracy: 0.597\n",
      "Epoch 355, CIFAR-10 Batch 5:  Loss: 3.370 Accuracy: 0.597\n",
      "Epoch 356, CIFAR-10 Batch 1:  Loss: 3.306 Accuracy: 0.598\n",
      "Epoch 356, CIFAR-10 Batch 2:  Loss: 3.680 Accuracy: 0.582\n",
      "Epoch 356, CIFAR-10 Batch 3:  Loss: 3.230 Accuracy: 0.596\n",
      "Epoch 356, CIFAR-10 Batch 4:  Loss: 3.251 Accuracy: 0.590\n",
      "Epoch 356, CIFAR-10 Batch 5:  Loss: 3.271 Accuracy: 0.601\n",
      "Epoch 357, CIFAR-10 Batch 1:  Loss: 3.346 Accuracy: 0.592\n",
      "Epoch 357, CIFAR-10 Batch 2:  Loss: 3.792 Accuracy: 0.583\n",
      "Epoch 357, CIFAR-10 Batch 3:  Loss: 3.335 Accuracy: 0.599\n",
      "Epoch 357, CIFAR-10 Batch 4:  Loss: 3.261 Accuracy: 0.595\n",
      "Epoch 357, CIFAR-10 Batch 5:  Loss: 3.336 Accuracy: 0.590\n",
      "Epoch 358, CIFAR-10 Batch 1:  Loss: 3.284 Accuracy: 0.597\n",
      "Epoch 358, CIFAR-10 Batch 2:  Loss: 3.662 Accuracy: 0.578\n",
      "Epoch 358, CIFAR-10 Batch 3:  Loss: 3.187 Accuracy: 0.599\n",
      "Epoch 358, CIFAR-10 Batch 4:  Loss: 3.327 Accuracy: 0.589\n",
      "Epoch 358, CIFAR-10 Batch 5:  Loss: 3.447 Accuracy: 0.595\n",
      "Epoch 359, CIFAR-10 Batch 1:  Loss: 3.470 Accuracy: 0.589\n",
      "Epoch 359, CIFAR-10 Batch 2:  Loss: 3.781 Accuracy: 0.580\n",
      "Epoch 359, CIFAR-10 Batch 3:  Loss: 3.116 Accuracy: 0.602\n",
      "Epoch 359, CIFAR-10 Batch 4:  Loss: 3.232 Accuracy: 0.595\n",
      "Epoch 359, CIFAR-10 Batch 5:  Loss: 3.450 Accuracy: 0.593\n",
      "Epoch 360, CIFAR-10 Batch 1:  Loss: 3.556 Accuracy: 0.590\n",
      "Epoch 360, CIFAR-10 Batch 2:  Loss: 3.527 Accuracy: 0.585\n",
      "Epoch 360, CIFAR-10 Batch 3:  Loss: 3.186 Accuracy: 0.597\n",
      "Epoch 360, CIFAR-10 Batch 4:  Loss: 3.348 Accuracy: 0.590\n",
      "Epoch 360, CIFAR-10 Batch 5:  Loss: 3.376 Accuracy: 0.597\n",
      "Epoch 361, CIFAR-10 Batch 1:  Loss: 3.422 Accuracy: 0.592\n",
      "Epoch 361, CIFAR-10 Batch 2:  Loss: 3.729 Accuracy: 0.588\n",
      "Epoch 361, CIFAR-10 Batch 3:  Loss: 3.244 Accuracy: 0.602\n",
      "Epoch 361, CIFAR-10 Batch 4:  Loss: 3.223 Accuracy: 0.592\n",
      "Epoch 361, CIFAR-10 Batch 5:  Loss: 3.446 Accuracy: 0.591\n",
      "Epoch 362, CIFAR-10 Batch 1:  Loss: 3.426 Accuracy: 0.588\n",
      "Epoch 362, CIFAR-10 Batch 2:  Loss: 3.832 Accuracy: 0.579\n",
      "Epoch 362, CIFAR-10 Batch 3:  Loss: 3.300 Accuracy: 0.591\n",
      "Epoch 362, CIFAR-10 Batch 4:  Loss: 3.293 Accuracy: 0.591\n",
      "Epoch 362, CIFAR-10 Batch 5:  Loss: 3.446 Accuracy: 0.591\n",
      "Epoch 363, CIFAR-10 Batch 1:  Loss: 3.447 Accuracy: 0.592\n",
      "Epoch 363, CIFAR-10 Batch 2:  Loss: 3.558 Accuracy: 0.586\n",
      "Epoch 363, CIFAR-10 Batch 3:  Loss: 3.331 Accuracy: 0.587\n",
      "Epoch 363, CIFAR-10 Batch 4:  Loss: 3.276 Accuracy: 0.597\n",
      "Epoch 363, CIFAR-10 Batch 5:  Loss: 3.488 Accuracy: 0.590\n",
      "Epoch 364, CIFAR-10 Batch 1:  Loss: 3.442 Accuracy: 0.589\n",
      "Epoch 364, CIFAR-10 Batch 2:  Loss: 3.799 Accuracy: 0.570\n",
      "Epoch 364, CIFAR-10 Batch 3:  Loss: 3.231 Accuracy: 0.592\n",
      "Epoch 364, CIFAR-10 Batch 4:  Loss: 3.375 Accuracy: 0.586\n",
      "Epoch 364, CIFAR-10 Batch 5:  Loss: 3.473 Accuracy: 0.595\n",
      "Epoch 365, CIFAR-10 Batch 1:  Loss: 3.466 Accuracy: 0.593\n",
      "Epoch 365, CIFAR-10 Batch 2:  Loss: 3.782 Accuracy: 0.577\n",
      "Epoch 365, CIFAR-10 Batch 3:  Loss: 3.237 Accuracy: 0.594\n",
      "Epoch 365, CIFAR-10 Batch 4:  Loss: 3.413 Accuracy: 0.588\n",
      "Epoch 365, CIFAR-10 Batch 5:  Loss: 3.420 Accuracy: 0.591\n",
      "Epoch 366, CIFAR-10 Batch 1:  Loss: 3.454 Accuracy: 0.592\n",
      "Epoch 366, CIFAR-10 Batch 2:  Loss: 3.776 Accuracy: 0.578\n",
      "Epoch 366, CIFAR-10 Batch 3:  Loss: 3.214 Accuracy: 0.593\n",
      "Epoch 366, CIFAR-10 Batch 4:  Loss: 3.447 Accuracy: 0.584\n",
      "Epoch 366, CIFAR-10 Batch 5:  Loss: 3.470 Accuracy: 0.594\n",
      "Epoch 367, CIFAR-10 Batch 1:  Loss: 3.520 Accuracy: 0.591\n",
      "Epoch 367, CIFAR-10 Batch 2:  Loss: 3.900 Accuracy: 0.577\n",
      "Epoch 367, CIFAR-10 Batch 3:  Loss: 3.244 Accuracy: 0.603\n",
      "Epoch 367, CIFAR-10 Batch 4:  Loss: 3.386 Accuracy: 0.591\n",
      "Epoch 367, CIFAR-10 Batch 5:  Loss: 3.539 Accuracy: 0.595\n",
      "Epoch 368, CIFAR-10 Batch 1:  Loss: 3.416 Accuracy: 0.592\n",
      "Epoch 368, CIFAR-10 Batch 2:  Loss: 3.704 Accuracy: 0.585\n",
      "Epoch 368, CIFAR-10 Batch 3:  Loss: 3.308 Accuracy: 0.598\n",
      "Epoch 368, CIFAR-10 Batch 4:  Loss: 3.397 Accuracy: 0.587\n",
      "Epoch 368, CIFAR-10 Batch 5:  Loss: 3.555 Accuracy: 0.593\n",
      "Epoch 369, CIFAR-10 Batch 1:  Loss: 3.469 Accuracy: 0.596\n",
      "Epoch 369, CIFAR-10 Batch 2:  Loss: 3.748 Accuracy: 0.580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 369, CIFAR-10 Batch 3:  Loss: 3.317 Accuracy: 0.598\n",
      "Epoch 369, CIFAR-10 Batch 4:  Loss: 3.476 Accuracy: 0.577\n",
      "Epoch 369, CIFAR-10 Batch 5:  Loss: 3.442 Accuracy: 0.600\n",
      "Epoch 370, CIFAR-10 Batch 1:  Loss: 3.380 Accuracy: 0.587\n",
      "Epoch 370, CIFAR-10 Batch 2:  Loss: 3.851 Accuracy: 0.582\n",
      "Epoch 370, CIFAR-10 Batch 3:  Loss: 3.283 Accuracy: 0.596\n",
      "Epoch 370, CIFAR-10 Batch 4:  Loss: 3.506 Accuracy: 0.582\n",
      "Epoch 370, CIFAR-10 Batch 5:  Loss: 3.718 Accuracy: 0.589\n",
      "Epoch 371, CIFAR-10 Batch 1:  Loss: 3.534 Accuracy: 0.592\n",
      "Epoch 371, CIFAR-10 Batch 2:  Loss: 3.888 Accuracy: 0.582\n",
      "Epoch 371, CIFAR-10 Batch 3:  Loss: 3.181 Accuracy: 0.594\n",
      "Epoch 371, CIFAR-10 Batch 4:  Loss: 3.485 Accuracy: 0.586\n",
      "Epoch 371, CIFAR-10 Batch 5:  Loss: 3.437 Accuracy: 0.595\n",
      "Epoch 372, CIFAR-10 Batch 1:  Loss: 3.492 Accuracy: 0.590\n",
      "Epoch 372, CIFAR-10 Batch 2:  Loss: 3.839 Accuracy: 0.581\n",
      "Epoch 372, CIFAR-10 Batch 3:  Loss: 3.251 Accuracy: 0.598\n",
      "Epoch 372, CIFAR-10 Batch 4:  Loss: 3.509 Accuracy: 0.590\n",
      "Epoch 372, CIFAR-10 Batch 5:  Loss: 3.465 Accuracy: 0.600\n",
      "Epoch 373, CIFAR-10 Batch 1:  Loss: 3.759 Accuracy: 0.588\n",
      "Epoch 373, CIFAR-10 Batch 2:  Loss: 3.888 Accuracy: 0.576\n",
      "Epoch 373, CIFAR-10 Batch 3:  Loss: 3.372 Accuracy: 0.595\n",
      "Epoch 373, CIFAR-10 Batch 4:  Loss: 3.366 Accuracy: 0.589\n",
      "Epoch 373, CIFAR-10 Batch 5:  Loss: 3.377 Accuracy: 0.597\n",
      "Epoch 374, CIFAR-10 Batch 1:  Loss: 3.618 Accuracy: 0.584\n",
      "Epoch 374, CIFAR-10 Batch 2:  Loss: 3.739 Accuracy: 0.579\n",
      "Epoch 374, CIFAR-10 Batch 3:  Loss: 3.398 Accuracy: 0.599\n",
      "Epoch 374, CIFAR-10 Batch 4:  Loss: 3.508 Accuracy: 0.582\n",
      "Epoch 374, CIFAR-10 Batch 5:  Loss: 3.516 Accuracy: 0.593\n",
      "Epoch 375, CIFAR-10 Batch 1:  Loss: 3.627 Accuracy: 0.586\n",
      "Epoch 375, CIFAR-10 Batch 2:  Loss: 3.900 Accuracy: 0.578\n",
      "Epoch 375, CIFAR-10 Batch 3:  Loss: 3.354 Accuracy: 0.598\n",
      "Epoch 375, CIFAR-10 Batch 4:  Loss: 3.550 Accuracy: 0.582\n",
      "Epoch 375, CIFAR-10 Batch 5:  Loss: 3.558 Accuracy: 0.590\n",
      "Epoch 376, CIFAR-10 Batch 1:  Loss: 3.511 Accuracy: 0.589\n",
      "Epoch 376, CIFAR-10 Batch 2:  Loss: 3.817 Accuracy: 0.588\n",
      "Epoch 376, CIFAR-10 Batch 3:  Loss: 3.410 Accuracy: 0.594\n",
      "Epoch 376, CIFAR-10 Batch 4:  Loss: 3.616 Accuracy: 0.585\n",
      "Epoch 376, CIFAR-10 Batch 5:  Loss: 3.485 Accuracy: 0.599\n",
      "Epoch 377, CIFAR-10 Batch 1:  Loss: 3.496 Accuracy: 0.597\n",
      "Epoch 377, CIFAR-10 Batch 2:  Loss: 3.742 Accuracy: 0.584\n",
      "Epoch 377, CIFAR-10 Batch 3:  Loss: 3.430 Accuracy: 0.597\n",
      "Epoch 377, CIFAR-10 Batch 4:  Loss: 3.588 Accuracy: 0.579\n",
      "Epoch 377, CIFAR-10 Batch 5:  Loss: 3.558 Accuracy: 0.593\n",
      "Epoch 378, CIFAR-10 Batch 1:  Loss: 3.618 Accuracy: 0.591\n",
      "Epoch 378, CIFAR-10 Batch 2:  Loss: 3.996 Accuracy: 0.580\n",
      "Epoch 378, CIFAR-10 Batch 3:  Loss: 3.518 Accuracy: 0.595\n",
      "Epoch 378, CIFAR-10 Batch 4:  Loss: 3.569 Accuracy: 0.583\n",
      "Epoch 378, CIFAR-10 Batch 5:  Loss: 3.632 Accuracy: 0.587\n",
      "Epoch 379, CIFAR-10 Batch 1:  Loss: 3.806 Accuracy: 0.584\n",
      "Epoch 379, CIFAR-10 Batch 2:  Loss: 3.919 Accuracy: 0.588\n",
      "Epoch 379, CIFAR-10 Batch 3:  Loss: 3.282 Accuracy: 0.597\n",
      "Epoch 379, CIFAR-10 Batch 4:  Loss: 3.644 Accuracy: 0.583\n",
      "Epoch 379, CIFAR-10 Batch 5:  Loss: 3.640 Accuracy: 0.590\n",
      "Epoch 380, CIFAR-10 Batch 1:  Loss: 3.701 Accuracy: 0.590\n",
      "Epoch 380, CIFAR-10 Batch 2:  Loss: 3.974 Accuracy: 0.577\n",
      "Epoch 380, CIFAR-10 Batch 3:  Loss: 3.510 Accuracy: 0.599\n",
      "Epoch 380, CIFAR-10 Batch 4:  Loss: 3.490 Accuracy: 0.594\n",
      "Epoch 380, CIFAR-10 Batch 5:  Loss: 3.545 Accuracy: 0.591\n",
      "Epoch 381, CIFAR-10 Batch 1:  Loss: 3.723 Accuracy: 0.591\n",
      "Epoch 381, CIFAR-10 Batch 2:  Loss: 3.818 Accuracy: 0.590\n",
      "Epoch 381, CIFAR-10 Batch 3:  Loss: 3.426 Accuracy: 0.597\n",
      "Epoch 381, CIFAR-10 Batch 4:  Loss: 3.582 Accuracy: 0.583\n",
      "Epoch 381, CIFAR-10 Batch 5:  Loss: 3.789 Accuracy: 0.591\n",
      "Epoch 382, CIFAR-10 Batch 1:  Loss: 3.694 Accuracy: 0.590\n",
      "Epoch 382, CIFAR-10 Batch 2:  Loss: 3.891 Accuracy: 0.581\n",
      "Epoch 382, CIFAR-10 Batch 3:  Loss: 3.447 Accuracy: 0.592\n",
      "Epoch 382, CIFAR-10 Batch 4:  Loss: 3.538 Accuracy: 0.582\n",
      "Epoch 382, CIFAR-10 Batch 5:  Loss: 3.690 Accuracy: 0.587\n",
      "Epoch 383, CIFAR-10 Batch 1:  Loss: 3.643 Accuracy: 0.592\n",
      "Epoch 383, CIFAR-10 Batch 2:  Loss: 3.795 Accuracy: 0.592\n",
      "Epoch 383, CIFAR-10 Batch 3:  Loss: 3.716 Accuracy: 0.587\n",
      "Epoch 383, CIFAR-10 Batch 4:  Loss: 3.617 Accuracy: 0.586\n",
      "Epoch 383, CIFAR-10 Batch 5:  Loss: 3.737 Accuracy: 0.589\n",
      "Epoch 384, CIFAR-10 Batch 1:  Loss: 3.754 Accuracy: 0.590\n",
      "Epoch 384, CIFAR-10 Batch 2:  Loss: 3.748 Accuracy: 0.584\n",
      "Epoch 384, CIFAR-10 Batch 3:  Loss: 3.576 Accuracy: 0.594\n",
      "Epoch 384, CIFAR-10 Batch 4:  Loss: 3.584 Accuracy: 0.589\n",
      "Epoch 384, CIFAR-10 Batch 5:  Loss: 3.732 Accuracy: 0.591\n",
      "Epoch 385, CIFAR-10 Batch 1:  Loss: 3.850 Accuracy: 0.590\n",
      "Epoch 385, CIFAR-10 Batch 2:  Loss: 3.762 Accuracy: 0.587\n",
      "Epoch 385, CIFAR-10 Batch 3:  Loss: 3.517 Accuracy: 0.596\n",
      "Epoch 385, CIFAR-10 Batch 4:  Loss: 3.723 Accuracy: 0.585\n",
      "Epoch 385, CIFAR-10 Batch 5:  Loss: 3.543 Accuracy: 0.595\n",
      "Epoch 386, CIFAR-10 Batch 1:  Loss: 3.765 Accuracy: 0.589\n",
      "Epoch 386, CIFAR-10 Batch 2:  Loss: 3.791 Accuracy: 0.589\n",
      "Epoch 386, CIFAR-10 Batch 3:  Loss: 3.809 Accuracy: 0.593\n",
      "Epoch 386, CIFAR-10 Batch 4:  Loss: 3.677 Accuracy: 0.587\n",
      "Epoch 386, CIFAR-10 Batch 5:  Loss: 3.587 Accuracy: 0.597\n",
      "Epoch 387, CIFAR-10 Batch 1:  Loss: 3.751 Accuracy: 0.592\n",
      "Epoch 387, CIFAR-10 Batch 2:  Loss: 3.834 Accuracy: 0.588\n",
      "Epoch 387, CIFAR-10 Batch 3:  Loss: 3.660 Accuracy: 0.591\n",
      "Epoch 387, CIFAR-10 Batch 4:  Loss: 3.618 Accuracy: 0.583\n",
      "Epoch 387, CIFAR-10 Batch 5:  Loss: 3.727 Accuracy: 0.590\n",
      "Epoch 388, CIFAR-10 Batch 1:  Loss: 3.830 Accuracy: 0.591\n",
      "Epoch 388, CIFAR-10 Batch 2:  Loss: 4.172 Accuracy: 0.582\n",
      "Epoch 388, CIFAR-10 Batch 3:  Loss: 3.577 Accuracy: 0.600\n",
      "Epoch 388, CIFAR-10 Batch 4:  Loss: 3.706 Accuracy: 0.579\n",
      "Epoch 388, CIFAR-10 Batch 5:  Loss: 3.736 Accuracy: 0.587\n",
      "Epoch 389, CIFAR-10 Batch 1:  Loss: 3.759 Accuracy: 0.588\n",
      "Epoch 389, CIFAR-10 Batch 2:  Loss: 3.864 Accuracy: 0.587\n",
      "Epoch 389, CIFAR-10 Batch 3:  Loss: 3.559 Accuracy: 0.593\n",
      "Epoch 389, CIFAR-10 Batch 4:  Loss: 3.705 Accuracy: 0.582\n",
      "Epoch 389, CIFAR-10 Batch 5:  Loss: 3.551 Accuracy: 0.596\n",
      "Epoch 390, CIFAR-10 Batch 1:  Loss: 3.871 Accuracy: 0.587\n",
      "Epoch 390, CIFAR-10 Batch 2:  Loss: 4.075 Accuracy: 0.588\n",
      "Epoch 390, CIFAR-10 Batch 3:  Loss: 3.599 Accuracy: 0.598\n",
      "Epoch 390, CIFAR-10 Batch 4:  Loss: 3.551 Accuracy: 0.583\n",
      "Epoch 390, CIFAR-10 Batch 5:  Loss: 3.774 Accuracy: 0.589\n",
      "Epoch 391, CIFAR-10 Batch 1:  Loss: 3.908 Accuracy: 0.582\n",
      "Epoch 391, CIFAR-10 Batch 2:  Loss: 3.896 Accuracy: 0.589\n",
      "Epoch 391, CIFAR-10 Batch 3:  Loss: 3.590 Accuracy: 0.588\n",
      "Epoch 391, CIFAR-10 Batch 4:  Loss: 3.708 Accuracy: 0.582\n",
      "Epoch 391, CIFAR-10 Batch 5:  Loss: 3.428 Accuracy: 0.596\n",
      "Epoch 392, CIFAR-10 Batch 1:  Loss: 3.811 Accuracy: 0.590\n",
      "Epoch 392, CIFAR-10 Batch 2:  Loss: 3.950 Accuracy: 0.590\n",
      "Epoch 392, CIFAR-10 Batch 3:  Loss: 3.580 Accuracy: 0.595\n",
      "Epoch 392, CIFAR-10 Batch 4:  Loss: 3.708 Accuracy: 0.588\n",
      "Epoch 392, CIFAR-10 Batch 5:  Loss: 3.672 Accuracy: 0.593\n",
      "Epoch 393, CIFAR-10 Batch 1:  Loss: 3.925 Accuracy: 0.591\n",
      "Epoch 393, CIFAR-10 Batch 2:  Loss: 3.925 Accuracy: 0.591\n",
      "Epoch 393, CIFAR-10 Batch 3:  Loss: 3.686 Accuracy: 0.592\n",
      "Epoch 393, CIFAR-10 Batch 4:  Loss: 3.624 Accuracy: 0.584\n",
      "Epoch 393, CIFAR-10 Batch 5:  Loss: 3.679 Accuracy: 0.594\n",
      "Epoch 394, CIFAR-10 Batch 1:  Loss: 3.967 Accuracy: 0.585\n",
      "Epoch 394, CIFAR-10 Batch 2:  Loss: 4.107 Accuracy: 0.586\n",
      "Epoch 394, CIFAR-10 Batch 3:  Loss: 3.613 Accuracy: 0.598\n",
      "Epoch 394, CIFAR-10 Batch 4:  Loss: 3.603 Accuracy: 0.586\n",
      "Epoch 394, CIFAR-10 Batch 5:  Loss: 3.682 Accuracy: 0.596\n",
      "Epoch 395, CIFAR-10 Batch 1:  Loss: 3.850 Accuracy: 0.593\n",
      "Epoch 395, CIFAR-10 Batch 2:  Loss: 4.045 Accuracy: 0.586\n",
      "Epoch 395, CIFAR-10 Batch 3:  Loss: 3.581 Accuracy: 0.591\n",
      "Epoch 395, CIFAR-10 Batch 4:  Loss: 3.808 Accuracy: 0.585\n",
      "Epoch 395, CIFAR-10 Batch 5:  Loss: 3.716 Accuracy: 0.596\n",
      "Epoch 396, CIFAR-10 Batch 1:  Loss: 3.835 Accuracy: 0.587\n",
      "Epoch 396, CIFAR-10 Batch 2:  Loss: 4.022 Accuracy: 0.583\n",
      "Epoch 396, CIFAR-10 Batch 3:  Loss: 3.910 Accuracy: 0.588\n",
      "Epoch 396, CIFAR-10 Batch 4:  Loss: 3.781 Accuracy: 0.583\n",
      "Epoch 396, CIFAR-10 Batch 5:  Loss: 3.780 Accuracy: 0.596\n",
      "Epoch 397, CIFAR-10 Batch 1:  Loss: 3.979 Accuracy: 0.590\n",
      "Epoch 397, CIFAR-10 Batch 2:  Loss: 3.987 Accuracy: 0.580\n",
      "Epoch 397, CIFAR-10 Batch 3:  Loss: 3.722 Accuracy: 0.594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 397, CIFAR-10 Batch 4:  Loss: 3.698 Accuracy: 0.584\n",
      "Epoch 397, CIFAR-10 Batch 5:  Loss: 3.785 Accuracy: 0.587\n",
      "Epoch 398, CIFAR-10 Batch 1:  Loss: 3.768 Accuracy: 0.593\n",
      "Epoch 398, CIFAR-10 Batch 2:  Loss: 4.053 Accuracy: 0.587\n",
      "Epoch 398, CIFAR-10 Batch 3:  Loss: 3.789 Accuracy: 0.586\n",
      "Epoch 398, CIFAR-10 Batch 4:  Loss: 3.844 Accuracy: 0.583\n",
      "Epoch 398, CIFAR-10 Batch 5:  Loss: 3.818 Accuracy: 0.593\n",
      "Epoch 399, CIFAR-10 Batch 1:  Loss: 3.964 Accuracy: 0.588\n",
      "Epoch 399, CIFAR-10 Batch 2:  Loss: 3.950 Accuracy: 0.588\n",
      "Epoch 399, CIFAR-10 Batch 3:  Loss: 3.916 Accuracy: 0.589\n",
      "Epoch 399, CIFAR-10 Batch 4:  Loss: 3.695 Accuracy: 0.587\n",
      "Epoch 399, CIFAR-10 Batch 5:  Loss: 3.723 Accuracy: 0.591\n",
      "Epoch 400, CIFAR-10 Batch 1:  Loss: 3.810 Accuracy: 0.593\n",
      "Epoch 400, CIFAR-10 Batch 2:  Loss: 3.982 Accuracy: 0.588\n",
      "Epoch 400, CIFAR-10 Batch 3:  Loss: 3.921 Accuracy: 0.586\n",
      "Epoch 400, CIFAR-10 Batch 4:  Loss: 3.635 Accuracy: 0.595\n",
      "Epoch 400, CIFAR-10 Batch 5:  Loss: 3.800 Accuracy: 0.594\n",
      "Epoch 401, CIFAR-10 Batch 1:  Loss: 3.698 Accuracy: 0.593\n",
      "Epoch 401, CIFAR-10 Batch 2:  Loss: 3.779 Accuracy: 0.591\n",
      "Epoch 401, CIFAR-10 Batch 3:  Loss: 3.786 Accuracy: 0.585\n",
      "Epoch 401, CIFAR-10 Batch 4:  Loss: 3.649 Accuracy: 0.598\n",
      "Epoch 401, CIFAR-10 Batch 5:  Loss: 3.906 Accuracy: 0.595\n",
      "Epoch 402, CIFAR-10 Batch 1:  Loss: 3.939 Accuracy: 0.586\n",
      "Epoch 402, CIFAR-10 Batch 2:  Loss: 4.023 Accuracy: 0.584\n",
      "Epoch 402, CIFAR-10 Batch 3:  Loss: 3.928 Accuracy: 0.581\n",
      "Epoch 402, CIFAR-10 Batch 4:  Loss: 3.833 Accuracy: 0.585\n",
      "Epoch 402, CIFAR-10 Batch 5:  Loss: 3.907 Accuracy: 0.589\n",
      "Epoch 403, CIFAR-10 Batch 1:  Loss: 3.852 Accuracy: 0.592\n",
      "Epoch 403, CIFAR-10 Batch 2:  Loss: 4.062 Accuracy: 0.589\n",
      "Epoch 403, CIFAR-10 Batch 3:  Loss: 3.894 Accuracy: 0.589\n",
      "Epoch 403, CIFAR-10 Batch 4:  Loss: 3.889 Accuracy: 0.588\n",
      "Epoch 403, CIFAR-10 Batch 5:  Loss: 3.863 Accuracy: 0.592\n",
      "Epoch 404, CIFAR-10 Batch 1:  Loss: 3.929 Accuracy: 0.587\n",
      "Epoch 404, CIFAR-10 Batch 2:  Loss: 4.088 Accuracy: 0.586\n",
      "Epoch 404, CIFAR-10 Batch 3:  Loss: 3.782 Accuracy: 0.588\n",
      "Epoch 404, CIFAR-10 Batch 4:  Loss: 3.689 Accuracy: 0.594\n",
      "Epoch 404, CIFAR-10 Batch 5:  Loss: 3.952 Accuracy: 0.594\n",
      "Epoch 405, CIFAR-10 Batch 1:  Loss: 3.941 Accuracy: 0.590\n",
      "Epoch 405, CIFAR-10 Batch 2:  Loss: 4.161 Accuracy: 0.587\n",
      "Epoch 405, CIFAR-10 Batch 3:  Loss: 4.039 Accuracy: 0.586\n",
      "Epoch 405, CIFAR-10 Batch 4:  Loss: 3.782 Accuracy: 0.583\n",
      "Epoch 405, CIFAR-10 Batch 5:  Loss: 3.791 Accuracy: 0.594\n",
      "Epoch 406, CIFAR-10 Batch 1:  Loss: 3.944 Accuracy: 0.589\n",
      "Epoch 406, CIFAR-10 Batch 2:  Loss: 4.106 Accuracy: 0.589\n",
      "Epoch 406, CIFAR-10 Batch 3:  Loss: 3.969 Accuracy: 0.589\n",
      "Epoch 406, CIFAR-10 Batch 4:  Loss: 3.692 Accuracy: 0.593\n",
      "Epoch 406, CIFAR-10 Batch 5:  Loss: 3.997 Accuracy: 0.594\n",
      "Epoch 407, CIFAR-10 Batch 1:  Loss: 3.897 Accuracy: 0.586\n",
      "Epoch 407, CIFAR-10 Batch 2:  Loss: 4.153 Accuracy: 0.588\n",
      "Epoch 407, CIFAR-10 Batch 3:  Loss: 3.948 Accuracy: 0.589\n",
      "Epoch 407, CIFAR-10 Batch 4:  Loss: 3.877 Accuracy: 0.588\n",
      "Epoch 407, CIFAR-10 Batch 5:  Loss: 3.950 Accuracy: 0.592\n",
      "Epoch 408, CIFAR-10 Batch 1:  Loss: 4.071 Accuracy: 0.593\n",
      "Epoch 408, CIFAR-10 Batch 2:  Loss: 4.091 Accuracy: 0.593\n",
      "Epoch 408, CIFAR-10 Batch 3:  Loss: 3.929 Accuracy: 0.591\n",
      "Epoch 408, CIFAR-10 Batch 4:  Loss: 3.825 Accuracy: 0.593\n",
      "Epoch 408, CIFAR-10 Batch 5:  Loss: 3.972 Accuracy: 0.594\n",
      "Epoch 409, CIFAR-10 Batch 1:  Loss: 3.998 Accuracy: 0.590\n",
      "Epoch 409, CIFAR-10 Batch 2:  Loss: 4.018 Accuracy: 0.588\n",
      "Epoch 409, CIFAR-10 Batch 3:  Loss: 4.157 Accuracy: 0.586\n",
      "Epoch 409, CIFAR-10 Batch 4:  Loss: 3.777 Accuracy: 0.596\n",
      "Epoch 409, CIFAR-10 Batch 5:  Loss: 3.899 Accuracy: 0.595\n",
      "Epoch 410, CIFAR-10 Batch 1:  Loss: 3.983 Accuracy: 0.596\n",
      "Epoch 410, CIFAR-10 Batch 2:  Loss: 4.150 Accuracy: 0.587\n",
      "Epoch 410, CIFAR-10 Batch 3:  Loss: 3.899 Accuracy: 0.589\n",
      "Epoch 410, CIFAR-10 Batch 4:  Loss: 3.603 Accuracy: 0.595\n",
      "Epoch 410, CIFAR-10 Batch 5:  Loss: 3.896 Accuracy: 0.591\n",
      "Epoch 411, CIFAR-10 Batch 1:  Loss: 4.012 Accuracy: 0.590\n",
      "Epoch 411, CIFAR-10 Batch 2:  Loss: 4.308 Accuracy: 0.591\n",
      "Epoch 411, CIFAR-10 Batch 3:  Loss: 3.876 Accuracy: 0.595\n",
      "Epoch 411, CIFAR-10 Batch 4:  Loss: 3.957 Accuracy: 0.592\n",
      "Epoch 411, CIFAR-10 Batch 5:  Loss: 4.128 Accuracy: 0.590\n",
      "Epoch 412, CIFAR-10 Batch 1:  Loss: 3.913 Accuracy: 0.594\n",
      "Epoch 412, CIFAR-10 Batch 2:  Loss: 4.127 Accuracy: 0.596\n",
      "Epoch 412, CIFAR-10 Batch 3:  Loss: 4.044 Accuracy: 0.587\n",
      "Epoch 412, CIFAR-10 Batch 4:  Loss: 3.873 Accuracy: 0.591\n",
      "Epoch 412, CIFAR-10 Batch 5:  Loss: 4.077 Accuracy: 0.589\n",
      "Epoch 413, CIFAR-10 Batch 1:  Loss: 3.969 Accuracy: 0.586\n",
      "Epoch 413, CIFAR-10 Batch 2:  Loss: 3.954 Accuracy: 0.595\n",
      "Epoch 413, CIFAR-10 Batch 3:  Loss: 4.008 Accuracy: 0.590\n",
      "Epoch 413, CIFAR-10 Batch 4:  Loss: 3.813 Accuracy: 0.596\n",
      "Epoch 413, CIFAR-10 Batch 5:  Loss: 4.013 Accuracy: 0.592\n",
      "Epoch 414, CIFAR-10 Batch 1:  Loss: 3.970 Accuracy: 0.588\n",
      "Epoch 414, CIFAR-10 Batch 2:  Loss: 4.130 Accuracy: 0.587\n",
      "Epoch 414, CIFAR-10 Batch 3:  Loss: 4.003 Accuracy: 0.587\n",
      "Epoch 414, CIFAR-10 Batch 4:  Loss: 3.873 Accuracy: 0.594\n",
      "Epoch 414, CIFAR-10 Batch 5:  Loss: 3.980 Accuracy: 0.588\n",
      "Epoch 415, CIFAR-10 Batch 1:  Loss: 3.805 Accuracy: 0.594\n",
      "Epoch 415, CIFAR-10 Batch 2:  Loss: 4.070 Accuracy: 0.591\n",
      "Epoch 415, CIFAR-10 Batch 3:  Loss: 4.048 Accuracy: 0.586\n",
      "Epoch 415, CIFAR-10 Batch 4:  Loss: 4.111 Accuracy: 0.590\n",
      "Epoch 415, CIFAR-10 Batch 5:  Loss: 3.847 Accuracy: 0.595\n",
      "Epoch 416, CIFAR-10 Batch 1:  Loss: 3.889 Accuracy: 0.594\n",
      "Epoch 416, CIFAR-10 Batch 2:  Loss: 4.027 Accuracy: 0.594\n",
      "Epoch 416, CIFAR-10 Batch 3:  Loss: 4.096 Accuracy: 0.587\n",
      "Epoch 416, CIFAR-10 Batch 4:  Loss: 3.939 Accuracy: 0.591\n",
      "Epoch 416, CIFAR-10 Batch 5:  Loss: 3.916 Accuracy: 0.591\n",
      "Epoch 417, CIFAR-10 Batch 1:  Loss: 3.904 Accuracy: 0.592\n",
      "Epoch 417, CIFAR-10 Batch 2:  Loss: 4.131 Accuracy: 0.590\n",
      "Epoch 417, CIFAR-10 Batch 3:  Loss: 4.044 Accuracy: 0.592\n",
      "Epoch 417, CIFAR-10 Batch 4:  Loss: 3.737 Accuracy: 0.594\n",
      "Epoch 417, CIFAR-10 Batch 5:  Loss: 4.086 Accuracy: 0.592\n",
      "Epoch 418, CIFAR-10 Batch 1:  Loss: 3.811 Accuracy: 0.597\n",
      "Epoch 418, CIFAR-10 Batch 2:  Loss: 4.137 Accuracy: 0.590\n",
      "Epoch 418, CIFAR-10 Batch 3:  Loss: 4.074 Accuracy: 0.586\n",
      "Epoch 418, CIFAR-10 Batch 4:  Loss: 4.042 Accuracy: 0.594\n",
      "Epoch 418, CIFAR-10 Batch 5:  Loss: 4.082 Accuracy: 0.591\n",
      "Epoch 419, CIFAR-10 Batch 1:  Loss: 3.882 Accuracy: 0.594\n",
      "Epoch 419, CIFAR-10 Batch 2:  Loss: 3.910 Accuracy: 0.598\n",
      "Epoch 419, CIFAR-10 Batch 3:  Loss: 4.018 Accuracy: 0.593\n",
      "Epoch 419, CIFAR-10 Batch 4:  Loss: 4.051 Accuracy: 0.596\n",
      "Epoch 419, CIFAR-10 Batch 5:  Loss: 4.016 Accuracy: 0.594\n",
      "Epoch 420, CIFAR-10 Batch 1:  Loss: 3.800 Accuracy: 0.595\n",
      "Epoch 420, CIFAR-10 Batch 2:  Loss: 4.229 Accuracy: 0.587\n",
      "Epoch 420, CIFAR-10 Batch 3:  Loss: 4.129 Accuracy: 0.590\n",
      "Epoch 420, CIFAR-10 Batch 4:  Loss: 4.004 Accuracy: 0.595\n",
      "Epoch 420, CIFAR-10 Batch 5:  Loss: 4.041 Accuracy: 0.593\n",
      "Epoch 421, CIFAR-10 Batch 1:  Loss: 3.877 Accuracy: 0.597\n",
      "Epoch 421, CIFAR-10 Batch 2:  Loss: 4.191 Accuracy: 0.595\n",
      "Epoch 421, CIFAR-10 Batch 3:  Loss: 4.127 Accuracy: 0.593\n",
      "Epoch 421, CIFAR-10 Batch 4:  Loss: 3.978 Accuracy: 0.595\n",
      "Epoch 421, CIFAR-10 Batch 5:  Loss: 3.957 Accuracy: 0.595\n",
      "Epoch 422, CIFAR-10 Batch 1:  Loss: 3.929 Accuracy: 0.594\n",
      "Epoch 422, CIFAR-10 Batch 2:  Loss: 4.203 Accuracy: 0.592\n",
      "Epoch 422, CIFAR-10 Batch 3:  Loss: 4.229 Accuracy: 0.583\n",
      "Epoch 422, CIFAR-10 Batch 4:  Loss: 4.057 Accuracy: 0.591\n",
      "Epoch 422, CIFAR-10 Batch 5:  Loss: 4.224 Accuracy: 0.587\n",
      "Epoch 423, CIFAR-10 Batch 1:  Loss: 3.943 Accuracy: 0.592\n",
      "Epoch 423, CIFAR-10 Batch 2:  Loss: 4.427 Accuracy: 0.585\n",
      "Epoch 423, CIFAR-10 Batch 3:  Loss: 4.012 Accuracy: 0.594\n",
      "Epoch 423, CIFAR-10 Batch 4:  Loss: 4.060 Accuracy: 0.592\n",
      "Epoch 423, CIFAR-10 Batch 5:  Loss: 3.988 Accuracy: 0.592\n",
      "Epoch 424, CIFAR-10 Batch 1:  Loss: 4.019 Accuracy: 0.590\n",
      "Epoch 424, CIFAR-10 Batch 2:  Loss: 4.519 Accuracy: 0.587\n",
      "Epoch 424, CIFAR-10 Batch 3:  Loss: 3.969 Accuracy: 0.597\n",
      "Epoch 424, CIFAR-10 Batch 4:  Loss: 4.062 Accuracy: 0.593\n",
      "Epoch 424, CIFAR-10 Batch 5:  Loss: 4.056 Accuracy: 0.587\n",
      "Epoch 425, CIFAR-10 Batch 1:  Loss: 4.065 Accuracy: 0.591\n",
      "Epoch 425, CIFAR-10 Batch 2:  Loss: 4.276 Accuracy: 0.587\n",
      "Epoch 425, CIFAR-10 Batch 3:  Loss: 3.894 Accuracy: 0.595\n",
      "Epoch 425, CIFAR-10 Batch 4:  Loss: 3.960 Accuracy: 0.592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425, CIFAR-10 Batch 5:  Loss: 4.278 Accuracy: 0.591\n",
      "Epoch 426, CIFAR-10 Batch 1:  Loss: 3.973 Accuracy: 0.595\n",
      "Epoch 426, CIFAR-10 Batch 2:  Loss: 4.321 Accuracy: 0.590\n",
      "Epoch 426, CIFAR-10 Batch 3:  Loss: 4.315 Accuracy: 0.588\n",
      "Epoch 426, CIFAR-10 Batch 4:  Loss: 3.996 Accuracy: 0.592\n",
      "Epoch 426, CIFAR-10 Batch 5:  Loss: 4.384 Accuracy: 0.585\n",
      "Epoch 427, CIFAR-10 Batch 1:  Loss: 4.075 Accuracy: 0.590\n",
      "Epoch 427, CIFAR-10 Batch 2:  Loss: 4.252 Accuracy: 0.581\n",
      "Epoch 427, CIFAR-10 Batch 3:  Loss: 4.015 Accuracy: 0.592\n",
      "Epoch 427, CIFAR-10 Batch 4:  Loss: 4.242 Accuracy: 0.586\n",
      "Epoch 427, CIFAR-10 Batch 5:  Loss: 4.294 Accuracy: 0.589\n",
      "Epoch 428, CIFAR-10 Batch 1:  Loss: 3.883 Accuracy: 0.594\n",
      "Epoch 428, CIFAR-10 Batch 2:  Loss: 4.166 Accuracy: 0.593\n",
      "Epoch 428, CIFAR-10 Batch 3:  Loss: 4.136 Accuracy: 0.587\n",
      "Epoch 428, CIFAR-10 Batch 4:  Loss: 3.971 Accuracy: 0.598\n",
      "Epoch 428, CIFAR-10 Batch 5:  Loss: 4.240 Accuracy: 0.592\n",
      "Epoch 429, CIFAR-10 Batch 1:  Loss: 3.955 Accuracy: 0.591\n",
      "Epoch 429, CIFAR-10 Batch 2:  Loss: 4.168 Accuracy: 0.585\n",
      "Epoch 429, CIFAR-10 Batch 3:  Loss: 4.191 Accuracy: 0.587\n",
      "Epoch 429, CIFAR-10 Batch 4:  Loss: 4.095 Accuracy: 0.596\n",
      "Epoch 429, CIFAR-10 Batch 5:  Loss: 4.090 Accuracy: 0.591\n",
      "Epoch 430, CIFAR-10 Batch 1:  Loss: 3.883 Accuracy: 0.598\n",
      "Epoch 430, CIFAR-10 Batch 2:  Loss: 4.372 Accuracy: 0.592\n",
      "Epoch 430, CIFAR-10 Batch 3:  Loss: 4.385 Accuracy: 0.584\n",
      "Epoch 430, CIFAR-10 Batch 4:  Loss: 4.061 Accuracy: 0.593\n",
      "Epoch 430, CIFAR-10 Batch 5:  Loss: 4.409 Accuracy: 0.588\n",
      "Epoch 431, CIFAR-10 Batch 1:  Loss: 4.179 Accuracy: 0.590\n",
      "Epoch 431, CIFAR-10 Batch 2:  Loss: 4.491 Accuracy: 0.586\n",
      "Epoch 431, CIFAR-10 Batch 3:  Loss: 4.259 Accuracy: 0.588\n",
      "Epoch 431, CIFAR-10 Batch 4:  Loss: 4.049 Accuracy: 0.595\n",
      "Epoch 431, CIFAR-10 Batch 5:  Loss: 4.324 Accuracy: 0.590\n",
      "Epoch 432, CIFAR-10 Batch 1:  Loss: 3.931 Accuracy: 0.601\n",
      "Epoch 432, CIFAR-10 Batch 2:  Loss: 4.448 Accuracy: 0.585\n",
      "Epoch 432, CIFAR-10 Batch 3:  Loss: 4.163 Accuracy: 0.594\n",
      "Epoch 432, CIFAR-10 Batch 4:  Loss: 4.233 Accuracy: 0.595\n",
      "Epoch 432, CIFAR-10 Batch 5:  Loss: 4.489 Accuracy: 0.588\n",
      "Epoch 433, CIFAR-10 Batch 1:  Loss: 4.000 Accuracy: 0.596\n",
      "Epoch 433, CIFAR-10 Batch 2:  Loss: 4.365 Accuracy: 0.587\n",
      "Epoch 433, CIFAR-10 Batch 3:  Loss: 4.129 Accuracy: 0.598\n",
      "Epoch 433, CIFAR-10 Batch 4:  Loss: 4.229 Accuracy: 0.590\n",
      "Epoch 433, CIFAR-10 Batch 5:  Loss: 4.268 Accuracy: 0.594\n",
      "Epoch 434, CIFAR-10 Batch 1:  Loss: 4.156 Accuracy: 0.601\n",
      "Epoch 434, CIFAR-10 Batch 2:  Loss: 4.278 Accuracy: 0.595\n",
      "Epoch 434, CIFAR-10 Batch 3:  Loss: 4.292 Accuracy: 0.591\n",
      "Epoch 434, CIFAR-10 Batch 4:  Loss: 4.029 Accuracy: 0.596\n",
      "Epoch 434, CIFAR-10 Batch 5:  Loss: 4.288 Accuracy: 0.587\n",
      "Epoch 435, CIFAR-10 Batch 1:  Loss: 4.003 Accuracy: 0.595\n",
      "Epoch 435, CIFAR-10 Batch 2:  Loss: 4.456 Accuracy: 0.591\n",
      "Epoch 435, CIFAR-10 Batch 3:  Loss: 4.248 Accuracy: 0.596\n",
      "Epoch 435, CIFAR-10 Batch 4:  Loss: 4.146 Accuracy: 0.598\n",
      "Epoch 435, CIFAR-10 Batch 5:  Loss: 4.535 Accuracy: 0.592\n",
      "Epoch 436, CIFAR-10 Batch 1:  Loss: 4.171 Accuracy: 0.590\n",
      "Epoch 436, CIFAR-10 Batch 2:  Loss: 4.502 Accuracy: 0.591\n",
      "Epoch 436, CIFAR-10 Batch 3:  Loss: 4.348 Accuracy: 0.593\n",
      "Epoch 436, CIFAR-10 Batch 4:  Loss: 4.279 Accuracy: 0.592\n",
      "Epoch 436, CIFAR-10 Batch 5:  Loss: 4.489 Accuracy: 0.588\n",
      "Epoch 437, CIFAR-10 Batch 1:  Loss: 4.057 Accuracy: 0.594\n",
      "Epoch 437, CIFAR-10 Batch 2:  Loss: 4.332 Accuracy: 0.593\n",
      "Epoch 437, CIFAR-10 Batch 3:  Loss: 4.199 Accuracy: 0.600\n",
      "Epoch 437, CIFAR-10 Batch 4:  Loss: 4.065 Accuracy: 0.594\n",
      "Epoch 437, CIFAR-10 Batch 5:  Loss: 4.426 Accuracy: 0.588\n",
      "Epoch 438, CIFAR-10 Batch 1:  Loss: 4.043 Accuracy: 0.596\n",
      "Epoch 438, CIFAR-10 Batch 2:  Loss: 4.545 Accuracy: 0.593\n",
      "Epoch 438, CIFAR-10 Batch 3:  Loss: 4.380 Accuracy: 0.588\n",
      "Epoch 438, CIFAR-10 Batch 4:  Loss: 4.205 Accuracy: 0.588\n",
      "Epoch 438, CIFAR-10 Batch 5:  Loss: 4.468 Accuracy: 0.588\n",
      "Epoch 439, CIFAR-10 Batch 1:  Loss: 4.103 Accuracy: 0.595\n",
      "Epoch 439, CIFAR-10 Batch 2:  Loss: 4.281 Accuracy: 0.592\n",
      "Epoch 439, CIFAR-10 Batch 3:  Loss: 4.094 Accuracy: 0.595\n",
      "Epoch 439, CIFAR-10 Batch 4:  Loss: 4.114 Accuracy: 0.594\n",
      "Epoch 439, CIFAR-10 Batch 5:  Loss: 4.511 Accuracy: 0.588\n",
      "Epoch 440, CIFAR-10 Batch 1:  Loss: 4.051 Accuracy: 0.594\n",
      "Epoch 440, CIFAR-10 Batch 2:  Loss: 4.271 Accuracy: 0.597\n",
      "Epoch 440, CIFAR-10 Batch 3:  Loss: 4.294 Accuracy: 0.592\n",
      "Epoch 440, CIFAR-10 Batch 4:  Loss: 4.249 Accuracy: 0.599\n",
      "Epoch 440, CIFAR-10 Batch 5:  Loss: 4.450 Accuracy: 0.583\n",
      "Epoch 441, CIFAR-10 Batch 1:  Loss: 4.199 Accuracy: 0.595\n",
      "Epoch 441, CIFAR-10 Batch 2:  Loss: 4.325 Accuracy: 0.589\n",
      "Epoch 441, CIFAR-10 Batch 3:  Loss: 4.079 Accuracy: 0.596\n",
      "Epoch 441, CIFAR-10 Batch 4:  Loss: 4.229 Accuracy: 0.595\n",
      "Epoch 441, CIFAR-10 Batch 5:  Loss: 4.479 Accuracy: 0.588\n",
      "Epoch 442, CIFAR-10 Batch 1:  Loss: 4.304 Accuracy: 0.593\n",
      "Epoch 442, CIFAR-10 Batch 2:  Loss: 4.294 Accuracy: 0.593\n",
      "Epoch 442, CIFAR-10 Batch 3:  Loss: 4.207 Accuracy: 0.594\n",
      "Epoch 442, CIFAR-10 Batch 4:  Loss: 4.186 Accuracy: 0.600\n",
      "Epoch 442, CIFAR-10 Batch 5:  Loss: 4.577 Accuracy: 0.584\n",
      "Epoch 443, CIFAR-10 Batch 1:  Loss: 4.205 Accuracy: 0.596\n",
      "Epoch 443, CIFAR-10 Batch 2:  Loss: 4.455 Accuracy: 0.590\n",
      "Epoch 443, CIFAR-10 Batch 3:  Loss: 4.396 Accuracy: 0.588\n",
      "Epoch 443, CIFAR-10 Batch 4:  Loss: 4.417 Accuracy: 0.588\n",
      "Epoch 443, CIFAR-10 Batch 5:  Loss: 4.525 Accuracy: 0.584\n",
      "Epoch 444, CIFAR-10 Batch 1:  Loss: 4.133 Accuracy: 0.594\n",
      "Epoch 444, CIFAR-10 Batch 2:  Loss: 4.418 Accuracy: 0.585\n",
      "Epoch 444, CIFAR-10 Batch 3:  Loss: 4.282 Accuracy: 0.582\n",
      "Epoch 444, CIFAR-10 Batch 4:  Loss: 4.297 Accuracy: 0.595\n",
      "Epoch 444, CIFAR-10 Batch 5:  Loss: 4.515 Accuracy: 0.583\n",
      "Epoch 445, CIFAR-10 Batch 1:  Loss: 4.074 Accuracy: 0.590\n",
      "Epoch 445, CIFAR-10 Batch 2:  Loss: 4.593 Accuracy: 0.586\n",
      "Epoch 445, CIFAR-10 Batch 3:  Loss: 4.368 Accuracy: 0.586\n",
      "Epoch 445, CIFAR-10 Batch 4:  Loss: 4.245 Accuracy: 0.594\n",
      "Epoch 445, CIFAR-10 Batch 5:  Loss: 4.562 Accuracy: 0.585\n",
      "Epoch 446, CIFAR-10 Batch 1:  Loss: 4.195 Accuracy: 0.591\n",
      "Epoch 446, CIFAR-10 Batch 2:  Loss: 4.513 Accuracy: 0.590\n",
      "Epoch 446, CIFAR-10 Batch 3:  Loss: 4.316 Accuracy: 0.587\n",
      "Epoch 446, CIFAR-10 Batch 4:  Loss: 4.040 Accuracy: 0.595\n",
      "Epoch 446, CIFAR-10 Batch 5:  Loss: 4.535 Accuracy: 0.592\n",
      "Epoch 447, CIFAR-10 Batch 1:  Loss: 4.100 Accuracy: 0.594\n",
      "Epoch 447, CIFAR-10 Batch 2:  Loss: 4.489 Accuracy: 0.591\n",
      "Epoch 447, CIFAR-10 Batch 3:  Loss: 4.268 Accuracy: 0.590\n",
      "Epoch 447, CIFAR-10 Batch 4:  Loss: 4.494 Accuracy: 0.588\n",
      "Epoch 447, CIFAR-10 Batch 5:  Loss: 4.527 Accuracy: 0.585\n",
      "Epoch 448, CIFAR-10 Batch 1:  Loss: 4.063 Accuracy: 0.595\n",
      "Epoch 448, CIFAR-10 Batch 2:  Loss: 4.586 Accuracy: 0.586\n",
      "Epoch 448, CIFAR-10 Batch 3:  Loss: 4.466 Accuracy: 0.590\n",
      "Epoch 448, CIFAR-10 Batch 4:  Loss: 4.604 Accuracy: 0.588\n",
      "Epoch 448, CIFAR-10 Batch 5:  Loss: 4.497 Accuracy: 0.590\n",
      "Epoch 449, CIFAR-10 Batch 1:  Loss: 4.099 Accuracy: 0.603\n",
      "Epoch 449, CIFAR-10 Batch 2:  Loss: 4.519 Accuracy: 0.596\n",
      "Epoch 449, CIFAR-10 Batch 3:  Loss: 4.347 Accuracy: 0.592\n",
      "Epoch 449, CIFAR-10 Batch 4:  Loss: 4.399 Accuracy: 0.595\n",
      "Epoch 449, CIFAR-10 Batch 5:  Loss: 4.344 Accuracy: 0.589\n",
      "Epoch 450, CIFAR-10 Batch 1:  Loss: 4.205 Accuracy: 0.599\n",
      "Epoch 450, CIFAR-10 Batch 2:  Loss: 4.600 Accuracy: 0.585\n",
      "Epoch 450, CIFAR-10 Batch 3:  Loss: 4.265 Accuracy: 0.591\n",
      "Epoch 450, CIFAR-10 Batch 4:  Loss: 4.264 Accuracy: 0.595\n",
      "Epoch 450, CIFAR-10 Batch 5:  Loss: 4.446 Accuracy: 0.583\n",
      "Epoch 451, CIFAR-10 Batch 1:  Loss: 4.207 Accuracy: 0.595\n",
      "Epoch 451, CIFAR-10 Batch 2:  Loss: 4.600 Accuracy: 0.588\n",
      "Epoch 451, CIFAR-10 Batch 3:  Loss: 4.317 Accuracy: 0.596\n",
      "Epoch 451, CIFAR-10 Batch 4:  Loss: 4.450 Accuracy: 0.594\n",
      "Epoch 451, CIFAR-10 Batch 5:  Loss: 4.501 Accuracy: 0.589\n",
      "Epoch 452, CIFAR-10 Batch 1:  Loss: 4.163 Accuracy: 0.591\n",
      "Epoch 452, CIFAR-10 Batch 2:  Loss: 4.647 Accuracy: 0.589\n",
      "Epoch 452, CIFAR-10 Batch 3:  Loss: 4.365 Accuracy: 0.593\n",
      "Epoch 452, CIFAR-10 Batch 4:  Loss: 4.413 Accuracy: 0.592\n",
      "Epoch 452, CIFAR-10 Batch 5:  Loss: 4.440 Accuracy: 0.593\n",
      "Epoch 453, CIFAR-10 Batch 1:  Loss: 4.186 Accuracy: 0.595\n",
      "Epoch 453, CIFAR-10 Batch 2:  Loss: 4.675 Accuracy: 0.584\n",
      "Epoch 453, CIFAR-10 Batch 3:  Loss: 4.280 Accuracy: 0.592\n",
      "Epoch 453, CIFAR-10 Batch 4:  Loss: 4.640 Accuracy: 0.589\n",
      "Epoch 453, CIFAR-10 Batch 5:  Loss: 4.440 Accuracy: 0.591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 454, CIFAR-10 Batch 1:  Loss: 4.082 Accuracy: 0.599\n",
      "Epoch 454, CIFAR-10 Batch 2:  Loss: 4.442 Accuracy: 0.591\n",
      "Epoch 454, CIFAR-10 Batch 3:  Loss: 4.347 Accuracy: 0.596\n",
      "Epoch 454, CIFAR-10 Batch 4:  Loss: 4.371 Accuracy: 0.594\n",
      "Epoch 454, CIFAR-10 Batch 5:  Loss: 4.660 Accuracy: 0.585\n",
      "Epoch 455, CIFAR-10 Batch 1:  Loss: 4.200 Accuracy: 0.597\n",
      "Epoch 455, CIFAR-10 Batch 2:  Loss: 4.584 Accuracy: 0.586\n",
      "Epoch 455, CIFAR-10 Batch 3:  Loss: 4.397 Accuracy: 0.592\n",
      "Epoch 455, CIFAR-10 Batch 4:  Loss: 4.657 Accuracy: 0.588\n",
      "Epoch 455, CIFAR-10 Batch 5:  Loss: 4.366 Accuracy: 0.591\n",
      "Epoch 456, CIFAR-10 Batch 1:  Loss: 4.205 Accuracy: 0.592\n",
      "Epoch 456, CIFAR-10 Batch 2:  Loss: 4.379 Accuracy: 0.587\n",
      "Epoch 456, CIFAR-10 Batch 3:  Loss: 4.435 Accuracy: 0.591\n",
      "Epoch 456, CIFAR-10 Batch 4:  Loss: 4.337 Accuracy: 0.597\n",
      "Epoch 456, CIFAR-10 Batch 5:  Loss: 4.433 Accuracy: 0.587\n",
      "Epoch 457, CIFAR-10 Batch 1:  Loss: 4.194 Accuracy: 0.599\n",
      "Epoch 457, CIFAR-10 Batch 2:  Loss: 4.385 Accuracy: 0.590\n",
      "Epoch 457, CIFAR-10 Batch 3:  Loss: 4.482 Accuracy: 0.591\n",
      "Epoch 457, CIFAR-10 Batch 4:  Loss: 4.625 Accuracy: 0.592\n",
      "Epoch 457, CIFAR-10 Batch 5:  Loss: 4.443 Accuracy: 0.591\n",
      "Epoch 458, CIFAR-10 Batch 1:  Loss: 4.390 Accuracy: 0.597\n",
      "Epoch 458, CIFAR-10 Batch 2:  Loss: 4.655 Accuracy: 0.587\n",
      "Epoch 458, CIFAR-10 Batch 3:  Loss: 4.572 Accuracy: 0.587\n",
      "Epoch 458, CIFAR-10 Batch 4:  Loss: 4.585 Accuracy: 0.587\n",
      "Epoch 458, CIFAR-10 Batch 5:  Loss: 4.347 Accuracy: 0.590\n",
      "Epoch 459, CIFAR-10 Batch 1:  Loss: 4.366 Accuracy: 0.592\n",
      "Epoch 459, CIFAR-10 Batch 2:  Loss: 4.497 Accuracy: 0.590\n",
      "Epoch 459, CIFAR-10 Batch 3:  Loss: 4.430 Accuracy: 0.592\n",
      "Epoch 459, CIFAR-10 Batch 4:  Loss: 4.583 Accuracy: 0.589\n",
      "Epoch 459, CIFAR-10 Batch 5:  Loss: 4.463 Accuracy: 0.590\n",
      "Epoch 460, CIFAR-10 Batch 1:  Loss: 4.395 Accuracy: 0.593\n",
      "Epoch 460, CIFAR-10 Batch 2:  Loss: 4.597 Accuracy: 0.589\n",
      "Epoch 460, CIFAR-10 Batch 3:  Loss: 4.397 Accuracy: 0.592\n",
      "Epoch 460, CIFAR-10 Batch 4:  Loss: 4.431 Accuracy: 0.595\n",
      "Epoch 460, CIFAR-10 Batch 5:  Loss: 4.440 Accuracy: 0.592\n",
      "Epoch 461, CIFAR-10 Batch 1:  Loss: 4.400 Accuracy: 0.590\n",
      "Epoch 461, CIFAR-10 Batch 2:  Loss: 4.689 Accuracy: 0.590\n",
      "Epoch 461, CIFAR-10 Batch 3:  Loss: 4.279 Accuracy: 0.592\n",
      "Epoch 461, CIFAR-10 Batch 4:  Loss: 4.861 Accuracy: 0.584\n",
      "Epoch 461, CIFAR-10 Batch 5:  Loss: 4.469 Accuracy: 0.595\n",
      "Epoch 462, CIFAR-10 Batch 1:  Loss: 4.352 Accuracy: 0.590\n",
      "Epoch 462, CIFAR-10 Batch 2:  Loss: 4.895 Accuracy: 0.581\n",
      "Epoch 462, CIFAR-10 Batch 3:  Loss: 4.440 Accuracy: 0.590\n",
      "Epoch 462, CIFAR-10 Batch 4:  Loss: 4.612 Accuracy: 0.593\n",
      "Epoch 462, CIFAR-10 Batch 5:  Loss: 4.606 Accuracy: 0.586\n",
      "Epoch 463, CIFAR-10 Batch 1:  Loss: 4.401 Accuracy: 0.591\n",
      "Epoch 463, CIFAR-10 Batch 2:  Loss: 4.672 Accuracy: 0.587\n",
      "Epoch 463, CIFAR-10 Batch 3:  Loss: 4.450 Accuracy: 0.593\n",
      "Epoch 463, CIFAR-10 Batch 4:  Loss: 4.496 Accuracy: 0.593\n",
      "Epoch 463, CIFAR-10 Batch 5:  Loss: 4.360 Accuracy: 0.594\n",
      "Epoch 464, CIFAR-10 Batch 1:  Loss: 4.430 Accuracy: 0.594\n",
      "Epoch 464, CIFAR-10 Batch 2:  Loss: 4.746 Accuracy: 0.584\n",
      "Epoch 464, CIFAR-10 Batch 3:  Loss: 4.505 Accuracy: 0.593\n",
      "Epoch 464, CIFAR-10 Batch 4:  Loss: 4.772 Accuracy: 0.589\n",
      "Epoch 464, CIFAR-10 Batch 5:  Loss: 4.592 Accuracy: 0.589\n",
      "Epoch 465, CIFAR-10 Batch 1:  Loss: 4.184 Accuracy: 0.601\n",
      "Epoch 465, CIFAR-10 Batch 2:  Loss: 4.705 Accuracy: 0.589\n",
      "Epoch 465, CIFAR-10 Batch 3:  Loss: 4.467 Accuracy: 0.594\n",
      "Epoch 465, CIFAR-10 Batch 4:  Loss: 4.485 Accuracy: 0.594\n",
      "Epoch 465, CIFAR-10 Batch 5:  Loss: 4.576 Accuracy: 0.586\n",
      "Epoch 466, CIFAR-10 Batch 1:  Loss: 4.347 Accuracy: 0.593\n",
      "Epoch 466, CIFAR-10 Batch 2:  Loss: 4.956 Accuracy: 0.585\n",
      "Epoch 466, CIFAR-10 Batch 3:  Loss: 4.650 Accuracy: 0.589\n",
      "Epoch 466, CIFAR-10 Batch 4:  Loss: 4.501 Accuracy: 0.588\n",
      "Epoch 466, CIFAR-10 Batch 5:  Loss: 4.515 Accuracy: 0.589\n",
      "Epoch 467, CIFAR-10 Batch 1:  Loss: 4.375 Accuracy: 0.595\n",
      "Epoch 467, CIFAR-10 Batch 2:  Loss: 4.636 Accuracy: 0.592\n",
      "Epoch 467, CIFAR-10 Batch 3:  Loss: 4.480 Accuracy: 0.589\n",
      "Epoch 467, CIFAR-10 Batch 4:  Loss: 4.743 Accuracy: 0.590\n",
      "Epoch 467, CIFAR-10 Batch 5:  Loss: 4.596 Accuracy: 0.591\n",
      "Epoch 468, CIFAR-10 Batch 1:  Loss: 4.440 Accuracy: 0.592\n",
      "Epoch 468, CIFAR-10 Batch 2:  Loss: 4.666 Accuracy: 0.588\n",
      "Epoch 468, CIFAR-10 Batch 3:  Loss: 4.514 Accuracy: 0.593\n",
      "Epoch 468, CIFAR-10 Batch 4:  Loss: 4.652 Accuracy: 0.586\n",
      "Epoch 468, CIFAR-10 Batch 5:  Loss: 4.611 Accuracy: 0.589\n",
      "Epoch 469, CIFAR-10 Batch 1:  Loss: 4.229 Accuracy: 0.599\n",
      "Epoch 469, CIFAR-10 Batch 2:  Loss: 4.839 Accuracy: 0.587\n",
      "Epoch 469, CIFAR-10 Batch 3:  Loss: 4.479 Accuracy: 0.590\n",
      "Epoch 469, CIFAR-10 Batch 4:  Loss: 4.516 Accuracy: 0.591\n",
      "Epoch 469, CIFAR-10 Batch 5:  Loss: 4.490 Accuracy: 0.593\n",
      "Epoch 470, CIFAR-10 Batch 1:  Loss: 4.480 Accuracy: 0.594\n",
      "Epoch 470, CIFAR-10 Batch 2:  Loss: 4.762 Accuracy: 0.584\n",
      "Epoch 470, CIFAR-10 Batch 3:  Loss: 4.527 Accuracy: 0.597\n",
      "Epoch 470, CIFAR-10 Batch 4:  Loss: 4.689 Accuracy: 0.592\n",
      "Epoch 470, CIFAR-10 Batch 5:  Loss: 4.571 Accuracy: 0.588\n",
      "Epoch 471, CIFAR-10 Batch 1:  Loss: 4.456 Accuracy: 0.591\n",
      "Epoch 471, CIFAR-10 Batch 2:  Loss: 4.900 Accuracy: 0.587\n",
      "Epoch 471, CIFAR-10 Batch 3:  Loss: 4.526 Accuracy: 0.596\n",
      "Epoch 471, CIFAR-10 Batch 4:  Loss: 4.492 Accuracy: 0.592\n",
      "Epoch 471, CIFAR-10 Batch 5:  Loss: 4.386 Accuracy: 0.595\n",
      "Epoch 472, CIFAR-10 Batch 1:  Loss: 4.605 Accuracy: 0.590\n",
      "Epoch 472, CIFAR-10 Batch 2:  Loss: 4.732 Accuracy: 0.589\n",
      "Epoch 472, CIFAR-10 Batch 3:  Loss: 4.648 Accuracy: 0.591\n",
      "Epoch 472, CIFAR-10 Batch 4:  Loss: 4.634 Accuracy: 0.588\n",
      "Epoch 472, CIFAR-10 Batch 5:  Loss: 4.543 Accuracy: 0.591\n",
      "Epoch 473, CIFAR-10 Batch 1:  Loss: 4.377 Accuracy: 0.592\n",
      "Epoch 473, CIFAR-10 Batch 2:  Loss: 4.842 Accuracy: 0.583\n",
      "Epoch 473, CIFAR-10 Batch 3:  Loss: 4.526 Accuracy: 0.593\n",
      "Epoch 473, CIFAR-10 Batch 4:  Loss: 4.804 Accuracy: 0.589\n",
      "Epoch 473, CIFAR-10 Batch 5:  Loss: 4.554 Accuracy: 0.592\n",
      "Epoch 474, CIFAR-10 Batch 1:  Loss: 4.571 Accuracy: 0.586\n",
      "Epoch 474, CIFAR-10 Batch 2:  Loss: 4.798 Accuracy: 0.586\n",
      "Epoch 474, CIFAR-10 Batch 3:  Loss: 4.653 Accuracy: 0.594\n",
      "Epoch 474, CIFAR-10 Batch 4:  Loss: 4.851 Accuracy: 0.591\n",
      "Epoch 474, CIFAR-10 Batch 5:  Loss: 4.768 Accuracy: 0.594\n",
      "Epoch 475, CIFAR-10 Batch 1:  Loss: 4.718 Accuracy: 0.589\n",
      "Epoch 475, CIFAR-10 Batch 2:  Loss: 4.971 Accuracy: 0.584\n",
      "Epoch 475, CIFAR-10 Batch 3:  Loss: 4.637 Accuracy: 0.593\n",
      "Epoch 475, CIFAR-10 Batch 4:  Loss: 4.653 Accuracy: 0.591\n",
      "Epoch 475, CIFAR-10 Batch 5:  Loss: 4.492 Accuracy: 0.589\n",
      "Epoch 476, CIFAR-10 Batch 1:  Loss: 4.586 Accuracy: 0.591\n",
      "Epoch 476, CIFAR-10 Batch 2:  Loss: 4.895 Accuracy: 0.589\n",
      "Epoch 476, CIFAR-10 Batch 3:  Loss: 4.544 Accuracy: 0.593\n",
      "Epoch 476, CIFAR-10 Batch 4:  Loss: 4.692 Accuracy: 0.589\n",
      "Epoch 476, CIFAR-10 Batch 5:  Loss: 4.557 Accuracy: 0.600\n",
      "Epoch 477, CIFAR-10 Batch 1:  Loss: 4.431 Accuracy: 0.592\n",
      "Epoch 477, CIFAR-10 Batch 2:  Loss: 4.745 Accuracy: 0.592\n",
      "Epoch 477, CIFAR-10 Batch 3:  Loss: 4.552 Accuracy: 0.592\n",
      "Epoch 477, CIFAR-10 Batch 4:  Loss: 4.793 Accuracy: 0.584\n",
      "Epoch 477, CIFAR-10 Batch 5:  Loss: 4.523 Accuracy: 0.594\n",
      "Epoch 478, CIFAR-10 Batch 1:  Loss: 4.513 Accuracy: 0.594\n",
      "Epoch 478, CIFAR-10 Batch 2:  Loss: 4.905 Accuracy: 0.586\n",
      "Epoch 478, CIFAR-10 Batch 3:  Loss: 4.625 Accuracy: 0.595\n",
      "Epoch 478, CIFAR-10 Batch 4:  Loss: 4.898 Accuracy: 0.589\n",
      "Epoch 478, CIFAR-10 Batch 5:  Loss: 4.863 Accuracy: 0.591\n",
      "Epoch 479, CIFAR-10 Batch 1:  Loss: 4.537 Accuracy: 0.592\n",
      "Epoch 479, CIFAR-10 Batch 2:  Loss: 4.822 Accuracy: 0.588\n",
      "Epoch 479, CIFAR-10 Batch 3:  Loss: 4.796 Accuracy: 0.587\n",
      "Epoch 479, CIFAR-10 Batch 4:  Loss: 4.706 Accuracy: 0.596\n",
      "Epoch 479, CIFAR-10 Batch 5:  Loss: 4.774 Accuracy: 0.592\n",
      "Epoch 480, CIFAR-10 Batch 1:  Loss: 4.559 Accuracy: 0.592\n",
      "Epoch 480, CIFAR-10 Batch 2:  Loss: 4.950 Accuracy: 0.585\n",
      "Epoch 480, CIFAR-10 Batch 3:  Loss: 4.707 Accuracy: 0.593\n",
      "Epoch 480, CIFAR-10 Batch 4:  Loss: 4.892 Accuracy: 0.586\n",
      "Epoch 480, CIFAR-10 Batch 5:  Loss: 4.822 Accuracy: 0.588\n",
      "Epoch 481, CIFAR-10 Batch 1:  Loss: 4.786 Accuracy: 0.590\n",
      "Epoch 481, CIFAR-10 Batch 2:  Loss: 4.725 Accuracy: 0.589\n",
      "Epoch 481, CIFAR-10 Batch 3:  Loss: 4.852 Accuracy: 0.589\n",
      "Epoch 481, CIFAR-10 Batch 4:  Loss: 5.061 Accuracy: 0.583\n",
      "Epoch 481, CIFAR-10 Batch 5:  Loss: 4.667 Accuracy: 0.591\n",
      "Epoch 482, CIFAR-10 Batch 1:  Loss: 4.692 Accuracy: 0.589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 482, CIFAR-10 Batch 2:  Loss: 4.870 Accuracy: 0.583\n",
      "Epoch 482, CIFAR-10 Batch 3:  Loss: 4.586 Accuracy: 0.591\n",
      "Epoch 482, CIFAR-10 Batch 4:  Loss: 4.678 Accuracy: 0.592\n",
      "Epoch 482, CIFAR-10 Batch 5:  Loss: 4.808 Accuracy: 0.586\n",
      "Epoch 483, CIFAR-10 Batch 1:  Loss: 4.671 Accuracy: 0.592\n",
      "Epoch 483, CIFAR-10 Batch 2:  Loss: 4.919 Accuracy: 0.588\n",
      "Epoch 483, CIFAR-10 Batch 3:  Loss: 4.742 Accuracy: 0.592\n",
      "Epoch 483, CIFAR-10 Batch 4:  Loss: 4.971 Accuracy: 0.591\n",
      "Epoch 483, CIFAR-10 Batch 5:  Loss: 4.798 Accuracy: 0.595\n",
      "Epoch 484, CIFAR-10 Batch 1:  Loss: 4.580 Accuracy: 0.588\n",
      "Epoch 484, CIFAR-10 Batch 2:  Loss: 4.800 Accuracy: 0.592\n",
      "Epoch 484, CIFAR-10 Batch 3:  Loss: 4.803 Accuracy: 0.590\n",
      "Epoch 484, CIFAR-10 Batch 4:  Loss: 4.946 Accuracy: 0.592\n",
      "Epoch 484, CIFAR-10 Batch 5:  Loss: 4.737 Accuracy: 0.592\n",
      "Epoch 485, CIFAR-10 Batch 1:  Loss: 4.603 Accuracy: 0.594\n",
      "Epoch 485, CIFAR-10 Batch 2:  Loss: 4.993 Accuracy: 0.591\n",
      "Epoch 485, CIFAR-10 Batch 3:  Loss: 4.788 Accuracy: 0.593\n",
      "Epoch 485, CIFAR-10 Batch 4:  Loss: 5.078 Accuracy: 0.593\n",
      "Epoch 485, CIFAR-10 Batch 5:  Loss: 4.717 Accuracy: 0.593\n",
      "Epoch 486, CIFAR-10 Batch 1:  Loss: 4.668 Accuracy: 0.590\n",
      "Epoch 486, CIFAR-10 Batch 2:  Loss: 5.086 Accuracy: 0.582\n",
      "Epoch 486, CIFAR-10 Batch 3:  Loss: 4.868 Accuracy: 0.594\n",
      "Epoch 486, CIFAR-10 Batch 4:  Loss: 4.962 Accuracy: 0.587\n",
      "Epoch 486, CIFAR-10 Batch 5:  Loss: 4.731 Accuracy: 0.593\n",
      "Epoch 487, CIFAR-10 Batch 1:  Loss: 4.861 Accuracy: 0.591\n",
      "Epoch 487, CIFAR-10 Batch 2:  Loss: 4.889 Accuracy: 0.591\n",
      "Epoch 487, CIFAR-10 Batch 3:  Loss: 4.677 Accuracy: 0.599\n",
      "Epoch 487, CIFAR-10 Batch 4:  Loss: 5.187 Accuracy: 0.589\n",
      "Epoch 487, CIFAR-10 Batch 5:  Loss: 4.673 Accuracy: 0.591\n",
      "Epoch 488, CIFAR-10 Batch 1:  Loss: 4.573 Accuracy: 0.594\n",
      "Epoch 488, CIFAR-10 Batch 2:  Loss: 4.847 Accuracy: 0.591\n",
      "Epoch 488, CIFAR-10 Batch 3:  Loss: 4.417 Accuracy: 0.599\n",
      "Epoch 488, CIFAR-10 Batch 4:  Loss: 4.904 Accuracy: 0.588\n",
      "Epoch 488, CIFAR-10 Batch 5:  Loss: 4.623 Accuracy: 0.595\n",
      "Epoch 489, CIFAR-10 Batch 1:  Loss: 4.878 Accuracy: 0.589\n",
      "Epoch 489, CIFAR-10 Batch 2:  Loss: 5.023 Accuracy: 0.587\n",
      "Epoch 489, CIFAR-10 Batch 3:  Loss: 4.765 Accuracy: 0.595\n",
      "Epoch 489, CIFAR-10 Batch 4:  Loss: 4.906 Accuracy: 0.593\n",
      "Epoch 489, CIFAR-10 Batch 5:  Loss: 4.523 Accuracy: 0.599\n",
      "Epoch 490, CIFAR-10 Batch 1:  Loss: 4.694 Accuracy: 0.589\n",
      "Epoch 490, CIFAR-10 Batch 2:  Loss: 4.931 Accuracy: 0.586\n",
      "Epoch 490, CIFAR-10 Batch 3:  Loss: 4.550 Accuracy: 0.601\n",
      "Epoch 490, CIFAR-10 Batch 4:  Loss: 5.072 Accuracy: 0.592\n",
      "Epoch 490, CIFAR-10 Batch 5:  Loss: 4.679 Accuracy: 0.591\n",
      "Epoch 491, CIFAR-10 Batch 1:  Loss: 4.653 Accuracy: 0.590\n",
      "Epoch 491, CIFAR-10 Batch 2:  Loss: 4.835 Accuracy: 0.589\n",
      "Epoch 491, CIFAR-10 Batch 3:  Loss: 4.574 Accuracy: 0.595\n",
      "Epoch 491, CIFAR-10 Batch 4:  Loss: 5.206 Accuracy: 0.590\n",
      "Epoch 491, CIFAR-10 Batch 5:  Loss: 4.558 Accuracy: 0.596\n",
      "Epoch 492, CIFAR-10 Batch 1:  Loss: 4.895 Accuracy: 0.594\n",
      "Epoch 492, CIFAR-10 Batch 2:  Loss: 4.744 Accuracy: 0.591\n",
      "Epoch 492, CIFAR-10 Batch 3:  Loss: 4.381 Accuracy: 0.600\n",
      "Epoch 492, CIFAR-10 Batch 4:  Loss: 5.163 Accuracy: 0.591\n",
      "Epoch 492, CIFAR-10 Batch 5:  Loss: 4.566 Accuracy: 0.600\n",
      "Epoch 493, CIFAR-10 Batch 1:  Loss: 4.598 Accuracy: 0.590\n",
      "Epoch 493, CIFAR-10 Batch 2:  Loss: 4.830 Accuracy: 0.592\n",
      "Epoch 493, CIFAR-10 Batch 3:  Loss: 4.655 Accuracy: 0.596\n",
      "Epoch 493, CIFAR-10 Batch 4:  Loss: 5.008 Accuracy: 0.593\n",
      "Epoch 493, CIFAR-10 Batch 5:  Loss: 4.694 Accuracy: 0.595\n",
      "Epoch 494, CIFAR-10 Batch 1:  Loss: 4.756 Accuracy: 0.599\n",
      "Epoch 494, CIFAR-10 Batch 2:  Loss: 5.066 Accuracy: 0.593\n",
      "Epoch 494, CIFAR-10 Batch 3:  Loss: 4.800 Accuracy: 0.591\n",
      "Epoch 494, CIFAR-10 Batch 4:  Loss: 4.914 Accuracy: 0.587\n",
      "Epoch 494, CIFAR-10 Batch 5:  Loss: 4.782 Accuracy: 0.597\n",
      "Epoch 495, CIFAR-10 Batch 1:  Loss: 4.887 Accuracy: 0.585\n",
      "Epoch 495, CIFAR-10 Batch 2:  Loss: 4.712 Accuracy: 0.590\n",
      "Epoch 495, CIFAR-10 Batch 3:  Loss: 4.764 Accuracy: 0.590\n",
      "Epoch 495, CIFAR-10 Batch 4:  Loss: 5.007 Accuracy: 0.590\n",
      "Epoch 495, CIFAR-10 Batch 5:  Loss: 4.621 Accuracy: 0.593\n",
      "Epoch 496, CIFAR-10 Batch 1:  Loss: 4.619 Accuracy: 0.591\n",
      "Epoch 496, CIFAR-10 Batch 2:  Loss: 4.836 Accuracy: 0.595\n",
      "Epoch 496, CIFAR-10 Batch 3:  Loss: 4.862 Accuracy: 0.594\n",
      "Epoch 496, CIFAR-10 Batch 4:  Loss: 5.135 Accuracy: 0.591\n",
      "Epoch 496, CIFAR-10 Batch 5:  Loss: 4.741 Accuracy: 0.596\n",
      "Epoch 497, CIFAR-10 Batch 1:  Loss: 4.738 Accuracy: 0.588\n",
      "Epoch 497, CIFAR-10 Batch 2:  Loss: 5.082 Accuracy: 0.586\n",
      "Epoch 497, CIFAR-10 Batch 3:  Loss: 4.870 Accuracy: 0.584\n",
      "Epoch 497, CIFAR-10 Batch 4:  Loss: 4.967 Accuracy: 0.589\n",
      "Epoch 497, CIFAR-10 Batch 5:  Loss: 4.665 Accuracy: 0.596\n",
      "Epoch 498, CIFAR-10 Batch 1:  Loss: 4.692 Accuracy: 0.595\n",
      "Epoch 498, CIFAR-10 Batch 2:  Loss: 4.948 Accuracy: 0.588\n",
      "Epoch 498, CIFAR-10 Batch 3:  Loss: 4.776 Accuracy: 0.592\n",
      "Epoch 498, CIFAR-10 Batch 4:  Loss: 4.903 Accuracy: 0.592\n",
      "Epoch 498, CIFAR-10 Batch 5:  Loss: 4.750 Accuracy: 0.599\n",
      "Epoch 499, CIFAR-10 Batch 1:  Loss: 4.682 Accuracy: 0.594\n",
      "Epoch 499, CIFAR-10 Batch 2:  Loss: 4.936 Accuracy: 0.589\n",
      "Epoch 499, CIFAR-10 Batch 3:  Loss: 4.698 Accuracy: 0.587\n",
      "Epoch 499, CIFAR-10 Batch 4:  Loss: 5.050 Accuracy: 0.590\n",
      "Epoch 499, CIFAR-10 Batch 5:  Loss: 4.746 Accuracy: 0.595\n",
      "Epoch 500, CIFAR-10 Batch 1:  Loss: 4.714 Accuracy: 0.583\n",
      "Epoch 500, CIFAR-10 Batch 2:  Loss: 5.030 Accuracy: 0.586\n",
      "Epoch 500, CIFAR-10 Batch 3:  Loss: 4.753 Accuracy: 0.594\n",
      "Epoch 500, CIFAR-10 Batch 4:  Loss: 5.129 Accuracy: 0.590\n",
      "Epoch 500, CIFAR-10 Batch 5:  Loss: 4.535 Accuracy: 0.597\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.5935546875\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c38f1d0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
